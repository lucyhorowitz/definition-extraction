The first chapter finished with a fair understanding of how Gauss's Method solves a linear system. It systematically takes linear combinations of the rows. Here we move to a general study of linear combinations. We need a setting. At times in the first chapter we've combined vectors from $ Re^2 $ , at other times vectors from $ Re^3 $ , and at other times vectors from higher - dimensional spaces. So our first impulse might be to work in $ Re^n $ , leaving $ n $ unspecified. This would have the advantage that any of the results would hold for $ Re^2 $ and for $ Re^3 $ and for many other spaces, simultaneously. But if having the results apply to many spaces at once is advantageous then sticking only to $ Re^n $ 's is restrictive. We'd like our results to apply to combinations of row vectors, as in the final section of the first chapter. We've even seen some spaces that are not simply a collection of all of the same - sized column vectors or row vectors. For instance, we've seen a homogeneous system's solution set that is a plane inside of $ Re^3 $ . This set is a closed system in that a linear combination of these solutions is also a solution. But it does not contain all of the three - tall column vectors, only some of them. We want the results about linear combinations to apply anywhere that linear combinations make sense. We shall call any such set a vector space. Our results, instead of being phrased as ``Whenever we have a collection in which we can sensibly take linear combinations ldots'', will be stated ``In any vector space ldots'' Such a statement describes at once what happens in many spaces. To understand the advantages of moving from studying a single space to studying a class of spaces, consider this analogy. Imagine that the government made laws one person at a time: ``Leslie Jones can't jay walk.'' That would be bad; statements have the virtue of economy when they apply to many cases at once. Or suppose that they said, ``Kim Ke must stop when passing an accident.'' Contrast that with, ``Any doctor must stop when passing an accident.'' More general statements, in some ways, are clearer. We shall study structures with two operations, an addition and a scalar multiplication, that are subject to some simple conditions. We will reflect more on the conditions later but on first reading notice how reasonable they are. For instance, surely any operation that can be called an addition (e.g., column vector addition, row vector addition, or real number addition) will satisfy conditions (1) through (5) below. vspace*{1ex} A vector space (over $ Re $ ) consists of a set $ V $ along with two operations `+' and ` $ cdot $ ' subject to the conditions that for all vectors $ vec{v}, vec{w}, vec{u}in V $ and all scalars $ r, sinRe $ : the set $ V $ is closed under vector addition, that is, $ vec{v}+vec{w}in V $ vector addition is commutative, $ vec{v}+vec{w}=vec{w}+vec{v} $ vector addition is associative, $ (vec{v}+vec{w})+vec{u}=vec{v}+(vec{w}+vec{u}) $ there is a zero vector $ zeroin V $ such that $ vec{v}+zero=vec{v}, $ for all $ vec{v}in V/ $ each $ vec{v}in V $ has an additive inverse $ vec{w}in V $ such that $ vec{w}+vec{v}=zero $ the set $ V $ is closed under scalar multiplication, that is, $ rcdotvec{v}in V $ scalar multiplication distributes over scalar addition, $ (r+s)cdotvec{v}=rcdotvec{v}+scdotvec{v} $ scalar multiplication distributes over vector addition, $ rcdot(vec{v}+vec{w})=rcdotvec{v}+rcdotvec{w} $ ordinary multiplication of scalars associates with scalar multiplication, $ (rs)cdotvec{v} =rcdot(scdotvec{v}) $ multiplication by the scalar  $ 1 $ is the identity operation, $ 1cdotvec{v}=vec{v} $ . vspace{ - (iv)5ex} why is the extra space there? Bug in shading code? The definition involves two kinds of addition and two kinds of multiplication, and so may at first seem confused. For instance, in condition (7) the ` $ + $ ' on the left is addition of two real numbers while the ` $ + $ ' on the right is addition of two vectors in $ V/ $ . These expressions aren't ambiguous because of context; for example, $ r $ and $ s $ are real numbers so ` $ r+s $ ' can only mean real number addition. In the same way, item (9)'s left side ` $ rs $ ' is ordinary real number multiplication, while its right side ` $ scdotvec{v} $ ' is the scalar multiplication defined for this vector space. vspace{ - 0.5ex} why is the extra space there? Bug in shading code? The best way to understand the definition is to go through the examples below and for each, check all ten conditions. The first example includes that check, written out at length. Use it as a model for the others. Especially important are the closure conditions, (1) and (6). They specify that the addition and scalar multiplication operations are always sensibleDash they are defined for every pair of vectors and every scalar and vector, and the result of the operation is a member of the set. This subset of $ Re^2 $ is a line through the origin. $ L=set{ colvec{x  y} suchthat y=3x} $ . We shall verify that it is a vector space under the usual meaning of `+' and ` $ cdot $ '. $ colvec{x_1  y_1} + colvec{x_2  y_2} = colvec{x_1+x_2  y_1+y_2} qquad rcdot colvec{x  y} = colvec{rx  ry} $ . These operations are just the ordinary ones, reused on its subset $ L $ . We say that $ L $ inherits/ these operations from $ Re^2 $ . We shall check all ten conditions. The paragraph having to do with addition has five conditions. For condition (1), closure under addition, suppose that we start with two vectors from the line  $ L $ , $ vec{v}_1=colvec{x_1  y_1} quad vec{v}_2=colvec{x_2  y_2} $ . so that they satisfy the restrictions that $ y_1=3x_1 $ and  $ y_2=3x_2 $ . Their sum $ vec{v}_1+vec{v}_2 =colvec{x_1+x_2  y_1+y_2} $ . is also a member of the line $ L $ because the fact that its second component is three times its first $ y_1+y_2=3(x_1+x_2) $ follows from the restrictions on $ vec{v}_1 $ and  $ vec{v}_2 $ . For (2), that addition of vectors commutes, just compare $ vec{v}_1+vec{v}_2 =colvec{x_1+x_2  y_1+y_2} quad vec{v}_2+vec{v}_1 =colvec{x_2+x_1  y_2+y_1} $ . and note that they are equal since their entries are real numbers and real numbers commute. (That the vectors satisfy the restriction of lying in the line is not relevant for this condition; they commute just because all vectors in the plane commute.) Condition (3), associativity of vector addition, is similar. $ (colvec{x_1  y_1} +colvec{x_2  y_2}) +colvec{x_3  y_3} &=colvec{(x_1+x_2)+x_3  (y_1+y_2)+y_3}  &=colvec{x_1+(x_2+x_3)  y_1+(y_2+y_3)}  &=colvec{x_1  y_1} +(colvec{x_2  y_2} +colvec{x_3  y_3}) $ . For the fourth condition we must produce a vector that acts as the zero element. The vector of zero entries will do. $ colvec{x  y} +colvec{0  0} =colvec{x  y} $ . Note that $ zeroin L $ as its second component is triple its first. For (5), that given any  $ vec{v}in L $ we can produce an additive inverse, we have $ colvec{ - x  - y} +colvec{x  y} =colvec{0  0} $ . and so the vector $ - vec{v} $ is the desired inverse. As with the prior condition, observe here that if $ vec{v}in L $ , so that $ y=3x $ , then $ - vec{v}in L $ also, since $ - y=3( - x) $ . The checks for the five conditions having to do with scalar multiplication are similar. For (6), closure under scalar multiplication, suppose that $ rinR $ and $ vec{v}in L $ , that is, $ vec{v}=colvec{x  y} $ . satisfies that  $ y=3x $ . Then $ rcdotvec{v}=rcdotcolvec{x  y} =colvec{rx  ry} $ . is also a member of $ L $ : the relation $ ry=3cdot rx $ holds because $ y=3x $ . Next, this checks (7). $ (r+s)cdotcolvec{x  y} =colvec{(r+s)x  (r+s)y} =colvec{rx+sx  ry+sy} =rcdotcolvec{x  y}+scdotcolvec{x  y} $ . For (8) we have this. $ rcdot(colvec{x_1  y_1}+colvec{x_2  y_2}) =colvec{r(x_1+x_2)  r(y_1+y_2)} =colvec{rx_1+rx_2  ry_1+ry_2} =rcdotcolvec{x_1  y_1}+rcdotcolvec{x_2  y_2} $ . The ninth $ (rs)cdotcolvec{x  y} =colvec{(rs)x  (rs)y} =colvec{r(sx)  r(sy)} =rcdot(scdotcolvec{x  y}) $ . and tenth conditions are also straightforward. $ 1cdotcolvec{x  y} =colvec{1x  1y} =colvec{x  y} $ . The whole plane, the set $ Re^2 $ , is a vector space where the operations ` $ + $ ' and ` $ cdot $ ' have their usual meaning. $ colvec{x_1  y_1} + colvec{x_2  y_2} = colvec{x_1+x_2  y_1+y_2} qquad rcdot colvec{x  y} = colvec{rx  ry} $ . We shall check just two of the conditions, the closure conditions. For (1) observe that the result of the vector sum $ colvec{x_1  y_1} +colvec{x_2  y_2} =colvec{x_1+x_2  y_1+y_2} $ . is a column array with two real entries, and so is a member of the plane  $ Re^2 $ . In contrast with the prior example, here there is no restriction on the first and second components of the vectors. Condition (6) is similar. The vector $ rcdotcolvec{x  y} =colvec{rx  ry} $ . has two real entries, and so is a member of  $ Re^2 $ . In a similar way, each $ Re^n $ is a vector space with the usual operations of vector addition and scalar multiplication. (In $ Re^1 $ , we usually do not write the members as column vectors, i.e., we usually do not write ` $ (pi) $ '. Instead we just write ` $ pi $ '.) $ X $ gives a subset of $ Re^2 $ that is a vector space. For contrast, consider the set of two - tall columns with entries that are integers, under the same operations of component - wise addition and scalar multiplication. This is a subset of $ Re^2 $ but it is not a vector space: it is not closed under scalar multiplication, that is, it does not satisfy condition (6). For instance, on the left below is a vector with integer entries, and a scalar. $ 0.5 cdot colvec[r]{4  3} = colvec[r]{2  (i)5} $ . On the right is a column vector that is not a member of the set, since its entries are not all integers. The one - element set $ set{ colvec[r]{0  0  0  0} } $ . is a vector space under the operations $ colvec[r]{0  0  0  0} + colvec[r]{0  0  0  0} = colvec[r]{0  0  0  0} qquad rcdot colvec[r]{0  0  0  0} = colvec[r]{0  0  0  0} $ . that it inherits from $ Re^4 $ . A vector space must have at least one element, its zero vector. Thus a one - element vector space is the smallest possible. A one - element vector space is a trivial space. The examples so far involve sets of column vectors with the usual operations. But vector spaces need not be collections of column vectors, or even of row vectors. Below are some other types of vector spaces. The term `vector space' does not mean `collection of columns of reals'. It means something more like `collection in which any linear combination is sensible'. Consider $ polyspace_3=set{a_0+a_1x+a_2x^2+a_3x^3suchthat a_0, ldots, a_3inRe} $ , the set of polynomials of degree three or less (in this book, we'll take constant polynomials, including the zero polynomial, to be of degree zero). It is a vector space under the operations (a_0+a_1x+a_2x^2+a_3x^3)+(b_0+b_1x+b_2x^2+b_3x^3)  =(a_0+b_0)+(a_1+b_1)x+(a_2+b_2)x^2+(a_3+b_3)x^3 and $ rcdot(a_0+a_1x+a_2x^2+a_3x^3)= (ra_0)+(ra_1)x+(ra_2)x^2+(ra_3)x^3 $ . (the verification is easy). This vector space is worthy of attention because these are the polynomial operations familiar from high school algebra. For instance, $ 3cdot(1 - 2x+3x^2 - 4x^3) - 2cdot(2 - 3x+x^2 - (1/2)x^3)= - 1+7x^2 - 11x^3 $ . Although this space is not a subset of any $ Re^n $ , there is a sense in which we can think of $ polyspace_3 $ as ``the same'' as $ Re^4 $ . If we identify these two space's elements in this way $ a_0+a_1x+a_2x^2+a_3x^3 quadtext{corresponds to}quad colvec{a_0  a_1  a_2  a_3} $ . then the operations also correspond. Here is an example of corresponding additions. $ mbox{{lr} & $ 1 - 2x+0x^2+1x^3 $  + & $ 2+3x+7x^2 - 4x^3 $  hline & $ 3+1x+7x^2 - 3x^3 $ } quadtext{corresponds to}quad colvec[r]{1  - 2  0  1} + colvec[r]{2  3  7  - 4} = colvec[r]{3  1  7  - 3} $ . Things we are thinking of as ``the same'' add to ``the same'' sum. Chapter Three makes precise this idea of vector space correspondence. For now we shall just leave it as an intuition. In general we write $ polyspace_n $ for the vector space of polynomials of degree  $ n $ or less $ set{a_0+a_1x+a_2x^2+cdots+a_nx^nsuchthat a_0, ldots, a_ninRe} $ , under the operations of the usual polynomial addition and scalar multiplication. We will often use these spaces as examples. The set $ matspace_{nbym{2}{2}} $ of $ nbym{2}{2} $ matrices with real number entries is a vector space under the natural entry - by - entry operations. $ a &b  c &d + w &x  y &z = a+w &b+x  c+y &d+z qquad rcdot a &b  c &d = ra &rb  rc &rd $ . As in the prior example, we can think of this space as ``the same'' as $ Re^4 $ . We write $ matspace_{nbym{n}{m}} $ for the vector space of $ nbym{n}{m} $ matrices under the natural operations of matrix addition and scalar multiplication. As with the polynomial spaces, we will often use these as examples. The set $ set{fsuchthat map{f}{N}{Re} } $ of all real - valued functions of one natural number variable is a vector space under the operations $ (f_1+f_2), (n)=f_1(n)+f_2(n) qquad (rcdot f), (n)=r, f(n) $ . so that if, for example, $ f_1(n)=n^2+2sin(n) $ and $ f_2(n)= - sin(n)+0.5 $ then $ (f_1+2f_2), (n)=n^2+1 $ . We can view this space as a generalization of $ X $ Dash instead of $ 2 $ - tall vectors, these functions are like infinitely - tall vectors. $ text{ {c|c} $ n $ & $ f(n)=n^2+1 $  hline $ 0 $ & $ 1 $  $ 1 $ & $ 2 $  $ 2 $ & $ 5 $  $ 3 $ & $ 10 $  $ vdots $ & $ vdots $ } quadtext{corresponds to}quad colvec[r]{ 1  2  5  10  vdotswithin{10} } $ . Addition and scalar multiplication are component - wise, as in $ X $ . (We can formalize ``infinitely - tall'' by saying that it means an infinite sequence, or that it means a function from $ N $ to $ Re $ .) The set of polynomials with real coefficients $ set{ a_0+a_1x+cdots+a_nx^nsuchthat ninN text{ and } a_0, ldots, a_ninRe} $ . makes a vector space when given the natural ` $ + $ ' (a_0+a_1x+cdots+a_nx^n)+(b_0+b_1x+cdots+b_nx^n)  =(a_0+b_0)+(a_1+b_1)x+cdots +(a_n+b_n)x^n and ` $ cdot $ '. $ rcdot (a_0+a_1x+ldots a_nx^n) = (ra_0)+(ra_1)x+ldots (ra_n)x^n $ . This space differs from the space $ polyspace_3 $ of $ X $ . This space contains not just degree three polynomials, but degree thirty polynomials and degree three hundred polynomials, too. Each individual polynomial of course is of a finite degree, but the set has no single bound on the degree of all of its members. We can think of this example, like the prior one, in terms of infinite - tuples. For instance, we can think of $ 1+3x+5x^2 $ as corresponding to $ (1, 3, 5, 0, 0, ldots) $ . However, this space differs from the one in $ X $ . Here, each member of the set has a finite degree, that is, under the correspondence there is no element from this space matching $ (1, 2, 5, 10, , ldots, ) $ . Vectors in this space correspond to infinite - tuples that end in zeroes. The set $ set{fsuchthat map{f}{Re}{Re} } $ of all real - valued functions of one real variable is a vector space under these. $ (f_1+f_2), (x)=f_1(x)+f_2(x) qquad (rcdot f), (x)=r, f(x) $ . The difference between this and $ X $ is the domain of the functions. The set $ F={ acostheta+bsintheta suchthat a, binRe } $ of real - valued functions of the real variable $ theta $ is a vector space under the operations $ (a_1costheta+b_1sintheta)+(a_2costheta+b_2sintheta) =(a_1+a_2)costheta+(b_1+b_2)sintheta $ . and $ rcdot (acostheta+bsintheta) =(ra)costheta+(rb)sintheta $ . inherited from the space in the prior example. (We can think of $ F $ as ``the same'' as $ Re^2 $ in that $ acostheta+bsintheta $ corresponds to the vector with components $ a $ and $ b $ .) The set $ set{map{f}{Re}{Re}suchthat dfrac{d^2f}{dx^2}+f=0} $ . is a vector space under the, by now natural, interpretation. $ (f+g), (x)=f(x)+g(x) qquad (rcdot f), (x)=r, f(x) $ . In particular, notice that basic Calculus gives $ frac{d^2(f+g)}{dx^2}+(f+g) =(frac{d^2f}{dx^2}+f)+(frac{d^2g}{dx^2}+g) $ . and $ frac{d^2(rf)}{dx^2}+(rf) =r(frac{d^2 f}{dx^2}+f) $ . and so the space is closed under addition and scalar multiplication. This turns out to equal the space from the prior exampleDash functions satisfying this differential equation have the form $ acostheta+bsintheta $ Dash but this description suggests an extension to solutions sets of other differential equations. The set of solutions of a homogeneous linear system in $ n $ variables is a vector space under the operations inherited from $ Re^n $ . For example, for closure under addition consider a typical equation in that system $ c_1x_1+cdots+c_nx_n=0 $ and suppose that both these vectors $ vec{v}=colvec{v_1  vdotswithin{v_1}  v_n} qquad vec{w}=colvec{w_1  vdotswithin{w_1}  w_n} $ . satisfy the equation. Then their sum $ vec{v}+vec{w}/ $ also satisfies that equation: $ c_1(v_1+w_1)+cdots+c_n(v_n+w_n) =(c_1v_1+cdots+c_nv_n)+(c_1w_1+cdots+c_nw_n) =0 $ . The checks of the other vector space conditions are just as routine. We often omit the multiplication symbol ` $ cdot $ ' between the scalar and the vector. We distinguish the multiplication in $ c_1v_1 $ from that in $ rvec{v}, $ by context, since if both multiplicands are real numbers then it must be real - real multiplication while if one is a vector then it must be scalar - vector multiplication. $ X $ has brought us full circle since it is one of our motivating examples. Now, with some feel for the kinds of structures that satisfy the definition of a vector space, we can reflect on that definition. For example, why specify in the definition the condition that $ 1cdotvec{v}=vec{v} $ but not a condition that $ 0cdotvec{v}=zero $ ? One answer is that this is just a definitionDash it gives the rules and you need to follow those rules to continue. Another answer is perhaps more satisfying. People in this area have worked to develop the right balance of power and generality. This definition is shaped so that it contains the conditions needed to prove all of the interesting and important properties of spaces of linear combinations. As we proceed, we shall derive all of the properties natural to collections of linear combinations from the conditions given in the definition. The next result is an example. We do not need to include these properties in the definition of vector space because they follow from the properties already listed there. In any vector space $ V $ , for any $ vec{v}in V $ and $ rinRe $ , we have (1)  $ 0cdotvec{v}=zero $ , (2)  $ ( - 1cdotvec{v})+vec{v}=zero $ , and (3)  $ rcdotzero=zero $ . For (1) note that $ vec{v}=(1+0)cdotvec{v}=vec{v}+(0cdotvec{v}) $ . Add to both sides the additive inverse of $ vec{v} $ , the vector $ vec{w} $ such that $ vec{w}+vec{v}=zero $ . $ vec{w}+vec{v} &=vec{w}+vec{v}+0cdotvec{v}  zero &=zero+0cdotvec{v}  zero &=0cdotvec{v} $ . Item (2) is easy: $ ( - 1cdotvec{v})+vec{v}=( - 1+1)cdotvec{v}=0cdotvec{v}=zero $ . For (3), $ rcdotzero = rcdot(0cdotzero) = (rcdot 0)cdotzero = zero $ will do. The second item shows that we can write the additive inverse of $ vec{v} $ as ` $ - vec{v}, $ ' without worrying about any confusion with $ ( - 1)cdotvec{v} $ . medskip A recap: our study in Chapter One of Gaussian reduction led us to consider collections of linear combinations. So in this chapter we have defined a vector space to be a structure in which we can form such combinations, subject to simple conditions on the addition and scalar multiplication operations. In a phrase: vector spaces are the right context in which to study linearity. From the fact that it forms a whole chapter, and especially because that chapter is the first one, a reader could suppose that our purpose in this book is the study of linear systems. The truth is that we will not so much use vector spaces in the study of linear systems as we instead have linear systems start us on the study of vector spaces. The wide variety of examples from this subsection shows that the study of vector spaces is interesting and important in its own right. Linear systems won't go away. But from now on our primary objects of study will be vector spaces. In $ X $ we saw a vector space that is a subset of $ Re^2 $ , a line through the origin. There, the vector space $ Re^2 $ contains inside it another vector space, the line. For any vector space, a subspace is a subset that is itself a vector space, under the inherited operations. This plane through the origin $ P=set{colvec{x  y  z}suchthat x+y+z=0} $ . is a subspace of $ Re^3 $ . As required by the definition the plane's operations are inherited from the larger space, that is, vectors add in $ P $ as they add in $ Re^3 $ $ colvec{x_1  y_1  z_1}+colvec{x_2  y_2  z_2} =colvec{x_1+x_2  y_1+y_2  z_1+z_2} $ . and scalar multiplication is also the same as in $ Re^3 $ . To show that $ P $ is a subspace we need only note that it is a subset and then verify that it is a space. We won't check all ten conditions, just the two closure ones. For closure under addition, note that if the summands satisfy that $ x_1+y_1+z_1=0 $ and $ x_2+y_2+z_2=0 $ then the sum satisfies that $ (x_1+x_2)+(y_1+y_2)+(z_1+z_2)=(x_1+y_1+z_1)+(x_2+y_2+z_2)=0 $ . For closure under scalar multiplication, if $ x+y+z=0 $ then the scalar multiple has $ rx+ry+rz=r(x+y+z)=0 $ . The $ x $ - axis in $ Re^2 $ is a subspace, where the addition and scalar multiplication operations are the inherited ones. $ colvec{x_1  0} + colvec{x_2  0} = colvec{x_1+x_2  0} qquad rcdotcolvec{x  0} =colvec{rx  0} $ . As in the prior example, to verify directly from the definition that this is a subspace we simply note that it is a subset and then check that it satisfies the conditions in definition of a vector space. For instance the two closure conditions are satisfied: adding two vectors with a second component of zero results in a vector with a second component of zero and multiplying a scalar times a vector with a second component of zero results in a vector with a second component of zero. Another subspace of $ Re^2 $ is its trivial subspace. $ set{colvec[r]{0  0}} $ . Any vector space has a trivial subspace $ set{zero, } $ . At the opposite extreme, any vector space has itself for a subspace. A subspace that is not the entire space is a proper subspace. Vector spaces that are not $ Re^n $ 's also have subspaces. The space of cubic polynomials $ set{a+bx+cx^2+dx^3suchthat a, b, c, dinRe} $ has a subspace comprised of all linear polynomials $ set{m+nxsuchthat m, ninRe} $ . Another example of a subspace that is not a subset of an $ Re^n $ followed the definition of a vector space. The space in $ X $ of all real - valued functions of one real variable $ set{fsuchthat map{f}{Re}{Re} } $ has the subspace in $ X $ of functions satisfying the restriction $ (d^2, f/dx^2)+f=0 $ . The definition requires that the addition and scalar multiplication operations must be the ones inherited from the larger space. The set $ S=set{1} $ is a subset of $ Re^1 $ . And, under the operations $ 1+1=1 $ and $ rcdot 1=1 $ the set $ S $ is a vector space, specifically, a trivial space. However, $ S $ is not a subspace of $ Re^1 $ because those aren't the inherited operations, since of course $ Re^1 $ has $ 1+1=2 $ . Being vector spaces themselves, subspaces must satisfy the closure conditions. The set $ Re^+ $ is not a subspace of the vector space $ Re^1 $ because with the inherited operations it is not closed under scalar multiplication: if $ vec{v}=1 $ then $ - 1cdotvec{v}notinRe^+ $ . The next result says that $ X $ is prototypical. The only way that a subset can fail to be a subspace, if it is nonempty and uses the inherited operations, is if it isn't closed. For a nonempty subset $ S $ of a vector space, under the inherited operations the following are equivalent statements.appendrefs{equivalence of statements} spacefactor=1000 $ S $ is a subspace of that vector space $ S $ is closed under linear combinations of pairs of vectors: for any vectors $ vec{s}_1, vec{s}_2in S $ and scalars $ r_1, r_2 $ the vector $ r_1vec{s}_1+r_2vec{s}_2 $ is in $ S $ $ S $ is closed under linear combinations of any number of vectors: for any vectors $ vec{s}_1, ldots, vec{s}_nin S $ and scalars $ r_1, ldots, r_n $ the vector $ r_1vec{s}_1+cdots+r_nvec{s}_n $ is an element of $ S $ . noindent Briefly, a subset is a subspace if and only if it is closed under linear combinations. `The following are equivalent' means that each pair of statements are equivalent. $ (1)!iff!(2) qquad (2)!iff!(3) qquad (3)!iff!(1) $ . We will prove the equivalence by establishing that $ (1)implies (3)implies (2)implies (1) $ . This strategy is suggested by the observation that the implications $ (1)implies (3) $ and $ (3)implies (2) $ are easy and so we need only argue that $ (2)implies (1) $ . Assume that $ S $ is a nonempty subset of a vector space $ V $ that is closed under combinations of pairs of vectors. We will show that $ S $ is a vector space by checking the conditions. The vector space definition has five conditions on addition. First, for closure under addition, if $ vec{s}_1, vec{s}_2in S $ then $ vec{s}_1+vec{s}_2in S $ , as it is a combination of a pair of vectors and we are assuming that  $ S $ is closed under those. Second, for any $ vec{s}_1, vec{s}_2in S $ , because addition is inherited from $ V $ , the sum $ vec{s}_1+vec{s}_2 $ in $ S $ equals the sum $ vec{s}_1+vec{s}_2 $ in $ V $ , and that equals the sum $ vec{s}_2+vec{s}_1 $ in $ V $ (because $ V $ is a vector space, its addition is commutative), and that in turn equals the sum $ vec{s}_2+vec{s}_1 $ in $ S $ . The argument for the third condition is similar to that for the second. For the fourth, consider the zero vector of $ V $ and note that closure of $ S $ under linear combinations of pairs of vectors gives that $ 0cdotvec{s}+0cdotvec{s}=zero $ is an element of $ S $ (where $ vec{s} $ is any member of the nonempty set $ S $ ); checking that $ zero $ acts under the inherited operations as the additive identity of $ S $ is easy. The fifth condition is satisfied because for any $ vec{s}in S $ , closure under linear combinations of pairs of vectors shows that $ 0cdotzero+( - 1)cdotvec{s} $ is an element of  $ S $ , and it is obviously the additive inverse of $ vec{s} $ under the inherited operations. The verifications for the scalar multiplication conditions are similar; see $ X $ . We will usually verify that a subset is a subspace by checking that it satisfies statement (2). At the start of this chapter we introduced vector spaces as collections in which linear combinations ``make sense.'' $ X $ 's statements (1) - (3) say that we can always make sense of an expression like $ r_1vec{s}_1+r_2vec{s}_2 $ in that the vector described is in the set  $ S $ . As a contrast, consider the set $ T $ of two - tall vectors whose entries add to a number greater than or equal to zero. Here we cannot just write any linear combination such as $ 2vec{t}_1 - 3vec{t}_2 $ and be confident the result is an element of $ T $ . $ X $ suggests that a good way to think of a vector space is as a collection of unrestricted linear combinations. The next two examples take some spaces and recasts their descriptions to be in that form. We can show that this plane through the origin subset of $ Re^3 $ $ S=set{colvec{x  y  z}suchthat x - 2y+z=0} $ . is a subspace under the usual addition and scalar multiplication operations of column vectors by checking that it is nonempty and closed under linear combinations of two vectors. But there is another way. Think of $ x - 2y+z=0 $ as a one - equation linear system and parametrize it by expressing the leading variable in terms of the free variables $ x=2y - z $ . $ S =set{colvec{2y - z  y  z}suchthat y, zinRe} =set{ycolvec[r]{2  1  0}+ zcolvec[r]{ - 1  0  1}suchthat y, zinRe} tag{ $ * $ } $ . Now, to show that this is a subspace consider $ r_1vec{s}_1+r_2vec{s}_2 $ . Each $ vec{s}_i $ is a linear combination of the two vectors in ( $ * $ ) so this is a linear combination of linear combinations. $ r_1cdot(y_1colvec[r]{2  1  0}+ z_1colvec[r]{ - 1  0  1}) + r_2cdot(y_2colvec[r]{2  1  0}+ z_2colvec[r]{ - 1  0  1}) $ . The Linear Combination Lemma, Lemma One.III., shows that the total is a linear combination of the two vectors and so $ X $ 's statement (2) is satisfied. This is a subspace of the $ nbyn{2} $ matrices $ matspace_{nbyn{2}} $ . $ L=set{ a &0  b &c suchthat a+b+c=0} $ . To parametrize, express the condition as $ a= - b - c $ . $ L =set{ - b - c &0  b &c suchthat b, cinRe} =set{b[r] - 1 &0  1 &0 +c[r] - 1 &0  0 &1 suchthat b, cinRe} $ . As above, we've described the subspace as a collection of unrestricted linear combinations. To show it is a subspace, note that a linear combination of vectors from $ L $ is a linear combination of linear combinations and so statement (2) is true. The span (or linear closure) of a nonempty subset $ S $ of a vector space is the set of all linear combinations of vectors from $ S $ . $ spanof{S} ={ c_1vec{s}_1+cdots+c_nvec{s}_n suchthat c_1, ldots, c_ninRe text{ and } vec{s}_1, ldots, vec{s}_nin S } $ . The span of the empty subset of a vector space is its trivial subspace. noindent No notation for the span is completely standard. The square brackets used here are common but so are ` $ mbox{span}(S) $ ' and ` $ mbox{sp}(S) $ '. In Chapter One, after we showed that we can write the solution set of a homogeneous linear system as $ set{c_1vec{beta}_1+cdots+c_kvec{beta}_ksuchthat c_1, ldots, c_kinRe} $ , we described that as the set `generated' by the $ smash{vec{beta}} $ 's. We now call that the span of $ set{vec{beta}_1, ldots, vec{beta}_k} $ . Recall also from that proof that the span of the empty set is defined to be the set $ set{zero} $ because of the convention that a trivial linear combination, a combination of zero - many vectors, adds to  $ zero $ . Besides, defining the empty set's span to be the trivial subspace is convenient because it keeps results like the next one from needing exceptions for the empty set. In a vector space, the span of any subset is a subspace. If the subset $ S $ is empty then by definition its span is the trivial subspace. If $ S $ is not empty then by $ X $ we need only check that the span $ spanof{S} $ is closed under linear combinations of pairs of elements. For a pair of vectors from that span, $ vec{v}=c_1vec{s}_1+cdots+c_nvec{s}_n $ and $ vec{w}=c_{n+1}vec{s}_{n+1}+cdots+c_mvec{s}_m $ , a linear combination pcdot(c_1vec{s}_1+cdots+c_nvec{s}_n)+ rcdot(c_{n+1}vec{s}_{n+1}+cdots+c_mvec{s}_m)  = pc_1vec{s}_1+cdots+pc_nvec{s}_n +rc_{n+1}vec{s}_{n+1}+cdots+rc_mvec{s}_m is a linear combination of elements of  $ S $ and so is an element of $ spanof{S} $ (possibly some of the $ vec{s}_i $ 's from $ vec{v} $ equal some of the $ vec{s}_j $ 's from $ vec{w} $ but that does not matter). The converse of the lemma holds: any subspace is the span of some set, because a subspace is obviously the span of itself, the set of all of its members. Thus a subset of a vector space is a subspace if and only if it is a span. This fits the intuition that a good way to think of a vector space is as a collection in which linear combinations are sensible. Taken together, $ X $ and $ X $ show that the span of a subset $ S $ of a vector space is the smallest subspace containing all of the members of $ S $ . In any vector space $ V $ , for any vector $ vec{v}in V $ , the set $ set{rcdotvec{v} suchthat rinRe} $ is a subspace of $ V $ . For instance, for any vector $ vec{v}inRe^3 $ the line through the origin containing that vector $ set{kvec{v}suchthat kinRe } $ is a subspace of $ Re^3 $ . This is true even if $ vec{v} $ is the zero vector, in which case it is the degenerate line, the trivial subspace. The span of this set is all of $ Re^2 $ . $ set{colvec[r]{1  1}, colvec[r]{1  - 1}} $ . We know that the span is some subspace of  $ Re^2 $ . To check that it is all of  $ Re^2 $ we must show that any member of $ Re^2 $ is a linear combination of these two vectors. So we ask: for which vectors with real components $ x $ and  $ y $ are there scalars $ c_1 $ and $ c_2 $ such that this holds? $ c_1colvec[r]{1  1}+c_2colvec[r]{1  - 1}=colvec{x  y} tag{ $ * $ } $ . Gauss's Method $ {2} c_1 &+ &c_2 &= &x  c_1 & - &c_2 &= &y grstep{ - rho_1+rho_2} {2} c_1 &+ &c_2 &= &xhfill  & & - 2c_2 &= & - x+y $ . with back substitution gives $ c_2=(x - y)/2 $ and $ c_1=(x+y)/2 $ . This shows that for any $ x, y $ there are appropriate coefficients $ c_1, c_2 $ making ( $ * $ ) trueDash we can write any element of $ Re^2 $ as a linear combination of the two given ones. For instance, for $ x=1 $ and $ y=2 $ the coefficients $ c_2= - 1/2 $ and $ c_1=3/2 $ will do. Since spans are subspaces, and we know that a good way to understand a subspace is to parametrize its description, we can try to understand a set's span in that way. Consider, in the vector space of quadratic polynomials  $ polyspace_2 $ , the span of the set $ S=set{3x - x^2, 2x} $ . By the definition of span, it is the set of unrestricted linear combinations of the two $ set{c_1(3x - x^2)+c_2(2x)suchthat c_1, c_2inRe} $ . Clearly polynomials in this span must have a constant term of zero. Is that necessary condition also sufficient? We are asking: for which members $ a_2x^2+a_1x+a_0 $ of $ polyspace_2 $ are there $ c_1 $ and $ c_2 $ such that $ a_2x^2+a_1x+a_0=c_1(3x - x^2)+c_2(2x) $ ? Polynomials are equal when their coefficients are equal so we want conditions on $ a_2 $ , $ a_1 $ , and $ a_0 $ making that triple a solution of this system. $ {2} - c_1 & & &= &a_2  3c_1 &+ &2c_2 &= &a_1  & &0 &= &a_0 $ . Gauss's Method and back - substitution gives $ c_1= - a_2 $ , and $ c_2=(3/2)a_2+(1/2)a_1 $ , and $ 0=a_0 $ . Thus as long as there is no constant term $ a_0=0 $ we can give coefficients $ c_1 $ and $ c_2 $ to describe that polynomial as an element of the span. For instance, for the polynomial $ 0 - 4x+3x^2 $ , the coefficients $ c_1= - 3 $ and $ c_2=5/2 $ will do. So the span of the given set is $ spanof{S}=set{a_1x+a_2x^2suchthat a_1, a_2inRe} $ . Incidentally, this shows that the set $ set{x, x^2} $ spans the same subspace. A space can have more than one spanning set. Two other sets spanning this subspace are $ set{x, x^2, - x+2x^2} $ and $ set{x, x+x^2, x+2x^2, ldots, } $ . The picture below shows the subspaces of $ Re^3 $ that we now know of: the trivial subspace, lines through the origin, planes through the origin, and the whole space. (Of course, the picture shows only a few of the infinitely many cases. Line segments connect subsets with their supersets.) In the next section we will prove that $ Re^3 $ has no other kind of subspace, so in fact this lists them all. This describes each subspace as the span of a set with a minimal number of members. With this, the subspaces fall naturally into levelsDash planes on one level, lines on another, etc. So far in this chapter we have seen that to study the properties of linear combinations, the right setting is a collection that is closed under these combinations. In the first subsection we introduced such collections, vector spaces, and we saw a great variety of examples. In this subsection we saw still more spaces, ones that are subspaces of others. In all of the variety there is a commonality. $ X $ above brings it out: vector spaces and subspaces are best understood as a span, and especially as a span of a small number of vectors. The next section studies spanning sets that are minimal.
The prior section ends with the observation that a spanning set is minimal when it is linearly independent and a linearly independent set is maximal when it spans the space. So the notions of minimal spanning set and maximal independent set coincide. In this section we will name this idea and study its properties. A basis for a vector space is a sequence of vectors that is linearly independent and that spans the space. Because a basis is a sequence, meaning that bases are different if they contain the same elements but in different orders, we denote it with angle brackets $ sequence{vec{beta}_1, vec{beta}_2, ldots} $ .appendrefs{sequences}@ spacefactor=1000{} (A sequence is linearly independent if the multiset consisting of the elements of the sequence is independent. Similarly, a sequence spans the space if the set of elements of the sequence spans the space.) This is a basis for $ Re^2 $ . $ sequence{ colvec{2  4}, colvec{1  1} } $ . It is linearly independent $ c_1colvec{2  4}+c_2colvec{1  1}=colvec{0  0} quadimpliesquad {2} 2c_1 &+ &1c_2 &= &0  4c_1 &+ &1c_2 &= &0 quadimpliesquad c_1=c_2=0 $ . and it spans $ Re^2 $ . $ {2} 2c_1 &+ &1c_2 &= &x  4c_1 &+ &1c_2 &= &y quadimpliesquad c_2=2x - ytext{ and } c_1=(y - x)/2 $ . This basis for $ Re^2 $ differs from the prior one $ sequence{colvec[r]{1  1}, colvec[r]{2  4}} $ . because it is in a different order. The verification that it is a basis is just as in the prior example. The space $ Re^2 $ has many bases. Another one is this. $ sequence{ colvec[r]{1  0}, colvec[r]{0  1} } $ . The verification is easy. For any $ Re^n $ $ stdbasis_n=sequence{ colvec[r]{1  0  vdotswithin{0}  0}, colvec[r]{0  1  vdotswithin{0}  0}, dots, , colvec[r]{0  0  vdotswithin{0}  1}} $ . is the standard (or natural) basis. We denote these vectors $ vec{e}_1, dots, vec{e}_n $ . noindent Calculus books denote $ Re^2 $ 's standard basis vectors as $ vec{imath} $ and $ vec{jmath} $ instead of $ vec{e}_1 $ and $ vec{e}_2 $ and they denote to $ Re^3 $ 's standard basis vectors as $ vec{imath} $ , $ vec{jmath} $ , and $ vec{k} $ instead of $ vec{e}_1 $ , $ vec{e}_2 $ , and $ vec{e}_3 $ . Note that $ vec{e}_1 $ means something different in a discussion of $ Re^3 $ than it means in a discussion of $ Re^2 $ . Consider the space $ set{acdotcostheta+bcdotsinthetasuchthat a, binRe} $ of functions of the real variable $ theta $ . This is a natural basis $ sequence{costheta, sintheta}=sequence{1cdotcostheta+0cdotsintheta, 0cdotcostheta+1cdotsintheta} $ . A more generic basis for this space is $ sequence{costheta - sintheta, 2costheta+3sintheta} $ . Verification that these two are bases is $ X $ . A natural basis for the vector space of cubic polynomials $ polyspace_3 $ is $ sequence{1, x, x^2, x^3} $ . Two other bases for this space are $ sequence{x^3, 3x^2, 6x, 6} $ and $ sequence{1, 1+x, 1+x+x^2, 1+x+x^2+x^3} $ . Checking that each is linearly independent and spans the space is easy. The trivial space $ set{zero} $ has only one basis, the empty one $ sequence{} $ . The space of finite - degree polynomials has a basis with infinitely many elements $ sequence{1, x, x^2, ldots} $ . We have seen bases before. In the first chapter we described the solution set of homogeneous systems such as this one $ {4} x &+ &y & & & - &w &= &0  & & & &z &+ &w &= &0 $ . by parametrizing. $ set{colvec[r]{ - 1  1  0  0}y +colvec[r]{1  0  - 1  1}w suchthat y, winRe } $ . Thus the vector space of solutions is the span of a two - element set. This two - vector set is also linearly independent, which is easy to check. Therefore the solution set is a subspace of $ Re^4 $ with a basis comprised of these two vectors. Parametrization finds bases for other vector spaces, not just for solution sets of homogeneous systems. To find a basis for this subspace of $ matspace_{nbyn{2}} $ $ set{ a &b  c &0 suchthat a+b - 2c=0} $ . we rewrite the condition as $ a= - b+2c $ . $ set{ - b+2c &b  c &0 suchthat b, c in Re} =set{b[r] - 1 &1  0 &0 + c[r] 2 &0  1 &0 suchthat b, c in Re} $ . Thus, this is a natural candidate for a basis. $ sequence{[r] - 1 &1  0 &0 , [r] 2 &0  1 &0 } $ . The above work shows that it spans the space. Linear independence is also easy. Consider again $ X $ . To verify that the set spans the space we looked at linear combinations that total to a member of the space $ c_1vec{beta}_1+c_2vec{beta}_2=binom{x}{y} $ . We only noted in that example that such a combination exists, that for each $ x, y $ there exists a $ c_1, c_2 $ , but in fact the calculation also shows that the combination is unique:  $ c_1 $ must be $ (y - x)/2 $ and $ c_2 $ must be $ 2x - y $ . In any vector space, a subset is a basis if and only if each vector in the space can be expressed as a linear combination of elements of the subset in one and only one way. noindent We consider linear combinations to be the same if they have the same summands but in a different order, or if they differ only in the addition or deletion of terms of the form ` $ 0cdotvec{beta} $ '. A sequence is a basis if and only if its vectors form a set that spans and that is linearly independent. A subset is a spanning set if and only if each vector in the space is a linear combination of elements of that subset in at least one way. Thus we need only show that a spanning subset is linearly independent if and only if every vector in the space is a linear combination of elements from the subset in at most one way. Consider two expressions of a vector as a linear combination of the members of the subset. Rearrange the two sums, and if necessary add some $ 0cdotvec{beta}_i $ terms, so that the two sums combine the same $ vec{beta} $ 's in the same order: $ vec{v}=lincombo{c}{vec{beta}} $ and $ vec{v}=lincombo{d}{vec{beta}} $ . Now $ lincombo{c}{vec{beta}}=lincombo{d}{vec{beta}} $ . holds if and only if $ (c_1 - d_1)vec{beta}_1+dots+(c_n - d_n)vec{beta}_n=zero $ . holds. So, asserting that each coefficient in the lower equation is zero is the same thing as asserting that $ c_i=d_i $ for each $ i $ , that is, that every vector is expressible as a linear combination of the $ vec{beta} $ 's in a unique way. In a vector space with basis $ B $ the definend{representation of $ vec{v} $ with respect to $ B $ } is the column vector of the coefficients used to express $ vec{v} $ as a linear combination of the basis vectors: $ rep{vec{v}}{B} = colvec{c_1  c_2  vdots  c_n}_{B} $ . where $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ and $ vec{v}=lincombo{c}{vec{beta}} $ . The $ c $ 's are the definend{coordinates of $ vec{v} $ with respect to $ B $ } . In $ polyspace_3 $ , with respect to the basis $ B=sequence{1, 2x, 2x^2, 2x^3} $ , the representation of $ x+x^2 $ is $ rep{x+x^2}{B}=colvec[r]{0  1/2  1/2  0}_B $ . because $ x+x^2=0cdot 1+(1/2)cdot 2x+(1/2)cdot 2x^2+0cdot 2x^3 $ . With respect to a different basis $ D=sequence{1+x, 1 - x, x+x^2, x+x^3} $ , the representation is different. $ rep{x+x^2}{D}=colvec[r]{0  0  1  0}_D $ . $ X $ requires that a basis be a sequence so that we can write these coordinates in an order. When there is only one basis around, we often omit the subscript naming that basis. In $ Re^2 $ , to find the coordinates of the vector $ vec{v}=binom{3}{2} $ with respect to the basis $ B=sequence{ colvec[r]{1  1}, colvec[r]{0  2} } $ . solve $ c_1colvec[r]{1  1} +c_2colvec[r]{0  2} = colvec[r]{3  2} $ . and get that $ c_1=3 $ and $ c_2= - 1/2 $ . $ rep{vec{v}}{B}=colvec[r]{3  - 1/2} $ . Writing the representation as a column generalizes the familiar case: in $ Re^n $ and with respect to the standard basis $ stdbasis_n $ , the vector starting at the origin and ending at $ (v_1, dots, v_n) $ has this representation. $ rep{colvec{v_1  vdots  v_n}}{stdbasis_n} = colvec{v_1  vdots  v_n}_{stdbasis_n} $ . This is an example. $ rep{colvec{ - 1  1}}{stdbasis_{n}} =colvec{ - 1  1} $ . The $ rep{vec{v}}{B} $ notation is not standard. The most common notation is $ [vec{v}]_B $ but one advantage that $ rep{vec{v}}{B} $ has is that it is harder to misinterpret or overlook. The column represents the vector in the sense that a linear relationship holds among a set of vectors if and only if that relationship holds among the set of representations. Where $ B $ is a basis with $ n $  elements, for any set of vectors, $ a_1vec{v}_1+cdots+a_kvec{v}_k=vec{0}_{V} $ if and only if $ a_1rep{vec{v}_1}{B}+cdots+a_krep{vec{v}_k}{B}=vec{0}_{Re^n} $ . Fix a basis $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ and suppose $ rep{vec{v}_1}{B}=colvec{c_{1, 1}  vdots  c_{n, 1}} quadldotsquad rep{vec{v}_k}{B}=colvec{c_{1, k}  vdots  c_{n, k}} $ . so that $ vec{v}_1=c_{1, 1}vec{beta}_1+dots+c_{n, 1}vec{beta}_n $ , etc. Then $ a_1vec{v}_1+dots+a_kvec{v}_k=vec{0} $ is equivalent to these. $ vec{0} &=a_1cdot(c_{1, 1}vec{beta}_1+dots+c_{n, 1}vec{beta}_n) +dots+ a_kcdot(c_{1, k}vec{beta}_1+dots+c_{n, k}vec{beta}_n)  &=(a_1c_{1, 1}+dots+a_kc_{1, k})cdotvec{beta}_1 +dots+ (a_1c_{n, 1}+dots+a_kc_{n, k})cdotvec{beta}_n $ . Obviously the bottom equation is true if the coefficients are zero. But, because $ B $ is a basis, $ X $ says that the bottom equation is true if and only if the coefficients are zero. So the relation is equivalent to this. $ a_1c_{1, 1}+dots+a_kc_{1, k} &=0  &alignedvdots  a_1c_{n, 1}+dots+a_kc_{n, k} &=0 $ . This is the equivalent recast into column vectors. $ a_1colvec{c_{1, 1}  vdots  c_{n, 1}} +dots+ a_kcolvec{c_{1, k}  vdots  c_{n, k}} =colvec{0  vdots  0} $ . Note that not only does a relationship hold for one set if and only if it holds for the other, but it is the same relationshipDash the $ a_i $ are the same. $ X $ finds the representation of $ x+x^2inpolyspace_3 $ with respect to $ B=sequence{1, 2x, 2x^2, 2x^3} $ . $ rep{x+x^2}{B}=colvec[r]{0  1/2  1/2  0}_B $ . This relationship $ 2cdot(x+x^2) - 1cdot(2x) - 2cdot(x^2) = 0+0x+0x^2+0x^3 $ . is represented by this one. $ 2cdotrep{x+x^2}{B} - rep{2x}{B} - 2cdotrep{x^2}{B} =2cdotcolvec{0  1/2  1/2  0} - colvec{0  1  0  0} - 2cdotcolvec{0  0  1/2  0} =colvec{0  0  0  0} $ . Our main use of representations will come later but the definition appears here because the fact that every vector is a linear combination of basis vectors in a unique way is a crucial property of bases, and also to help make a point. For calculation of coordinates among other things, we shall restrict our attention to spaces with bases having only finitely many elements. That will start in the next subsection. The previous subsection defines a basis of a vector space and shows that a space can have many different bases. So we cannot talk about ``the'' basis for a vector space. True, some vector spaces have bases that strike us as more natural than others, for instance, $ Re^2 $ 's basis $ stdbasis_2 $ or $ polyspace_2 $ 's basis $ sequence{1, x, x^2} $ . But for the vector space $ set{a_2x^2+a_1x+a_0suchthat 2a_2 - a_0=a_1} $ , no particular basis leaps out at us as the natural one. We cannot, in general, associate with a space any single basis that best describes it. We can however find something about the bases that is uniquely associated with the space. This subsection shows that any two bases for a space have the same number of elements. So with each space we can associate a number, the number of vectors in any of its bases. Before we start, we first limit our attention to spaces where at least one basis has only finitely many members. A vector space is finite - dimensional/ if it has a basis with only finitely many vectors. noindent One space that is not finite - dimensional is the set of polynomials with real coefficients, $ X $ . This is not spanned by any finite subset since that would contain a polynomial of largest degree but this space has polynomials of all degrees. Such spaces are interesting and important but we will focus in a different direction. From now on we will study only finite - dimensional vector spaces. In the rest of this book we shall take `vector space' to mean `finite - dimensional vector space'. To prove the main theorem we shall use a technical result, the Exchange Lemma. We first illustrate it with an example. Here is a basis for $ Re^3 $ and a vector given as a linear combination of members of that basis. $ B=sequence{colvec[r]{1  0  0}, colvec[r]{1  1  0}, colvec[r]{0  0  2}} qquad colvec[r]{1  2  0} =( - 1)cdotcolvec[r]{1  0  0} +2colvec[r]{1  1  0} +0cdotcolvec[r]{0  0  2} $ . Two of the basis vectors have non - zero coefficients. Pick one, for instance the first. Replace it with the vector that we've expressed as the combination $ hat{B}=sequence{colvec[r]{1  2  0}, colvec[r]{1  1  0}, colvec[r]{0  0  2}} $ . and the result is another basis for $ Re^3 $ . [Exchange Lemma] Assume that $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ is a basis for a vector space, and that for the vector $ vec{v} $ the relationship $ vec{v}=lincombo{c}{vec{beta}} $ has $ c_ineq 0 $ . Then exchanging $ vec{beta}_i $ for $ vec{v} $ yields another basis for the space. Call the outcome of the exchange $ hat{B}=sequence{vec{beta}_1, dots, vec{beta}_{i - 1}, vec{v}, vec{beta}_{i+1}, dots, vec{beta}_n} $ . We first show that $ hat{B} $ is linearly independent. Any relationship $ d_1vec{beta}_1+dots+d_ivec{v}+dots+d_nvec{beta}_n=zero $ among the members of $ hat{B} $ , after substitution for $ vec{v} $ , $ d_1vec{beta}_1+dots +d_icdot(c_1vec{beta}_1+dots+c_ivec{beta}_i+dots+c_nvec{beta}_n) +dots+d_nvec{beta}_n =zero tag*{( $ * $ )} $ . gives a linear relationship among the members of $ B $ . The basis $ B $ is linearly independent so the coefficient $ d_ic_i $ of $ vec{beta}_i $ is zero. Because we assumed that $ c_i $ is nonzero, $ d_i=0 $ . Using this in equation  $ (*) $ gives that all of the other $ d $ 's are also zero. Therefore $ hat{B} $ is linearly independent. We finish by showing that $ hat{B} $ has the same span as $ B $ . Half of this argument, that $ spanof{smash{hat{B}}}subseteqspanof{B} $ , is easy; we can write any member $ d_1vec{beta}_1+dots+d_ivec{v}+dots+d_nvec{beta}_n $ of $ spanof{smash{hat{B}}} $ as $ d_1vec{beta}_1+dots +d_icdot(c_1vec{beta}_1+dots+c_nvec{beta}_n) +dots+d_nvec{beta}_n $ , which is a linear combination of linear combinations of members of $ B $ , and hence is in $ spanof{B} $ . For the $ spanof{B}subseteqspanof{smash{hat{B}}} $ half of the argument, recall that if $ vec{v}=c_1vec{beta}_1+dots+c_nvec{beta}_n $ with $ c_ineq 0 $ then we can rearrange the equation to $ vec{beta}_i=( - c_1/c_i)vec{beta}_1+dots+(1/c_i)vec{v}+dots +( - c_n/c_i)vec{beta}_n $ . Now, consider any member $ d_1vec{beta}_1+dots+d_ivec{beta}_i+dots+d_nvec{beta}_n $ of $ spanof{B} $ , substitute for $ vec{beta}_i $ its expression as a linear combination of the members of $ hat{B} $ , and recognize, as in the first half of this argument, that the result is a linear combination of linear combinations of members of $ hat{B} $ , and hence is in $ spanof{smash{hat{B}}} $ . In any finite - dimensional vector space, all bases have the same number of elements. Fix a vector space with at least one finite basis. Choose, from among all of this space's bases, one $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ of minimal size. We will show that any other basis $ D=smash{sequence{vec{delta}_1, vec{delta}_2, ldots}} $ also has the same number of members, $ n $ . Because $ B $ has minimal size, $ D $ has no fewer than $ n $ vectors. We will argue that it cannot have more than $ n $ vectors. The basis $ B $ spans the space and $ vec{delta}_1 $ is in the space, so $ vec{delta}_1 $ is a nontrivial linear combination of elements of $ B $ . By the Exchange Lemma, we can swap $ vec{delta}_1 $ for a vector from $ B $ , resulting in a basis $ B_1 $ , where one element is $ vec{delta}_1 $ and all of the $ n - 1 $ other elements are $ vec{beta} $ 's. The prior paragraph forms the basis step for an induction argument. The inductive step starts with a basis $ B_k $ (for $ 1leq k<n $ ) containing $ k $ members of $ D $ and $ n - k $ members of $ B $ . We know that $ D $ has at least $ n $ members so there is a $ vec{delta}_{k+1} $ . Represent it as a linear combination of elements of $ B_k $ . The key point: in that representation, at least one of the nonzero scalars must be associated with a $ vec{beta}_i $ or else that representation would be a nontrivial linear relationship among elements of the linearly independent set $ D $ . Exchange $ vec{delta}_{k+1} $ for $ vec{beta}_i $ to get a new basis $ B_{k+1} $ with one $ vec{delta} $ more and one $ vec{beta} $ fewer than the previous basis $ B_k $ . Repeat that until no $ vec{beta} $ 's remain, so that $ B_n $ contains $ vec{delta}_1, dots, vec{delta}_n $ . Now, $ D $ cannot have more than these $ n $ vectors because any $ vec{delta}_{n+1} $ that remains would be in the span of $ B_n $ (since it is a basis) and hence would be a linear combination of the other $ vec{delta} $ 's, contradicting that $ D $ is linearly independent. The dimension of a vector space is the number of vectors in any of its bases. Any basis for $ Re^n $ has $ n $ vectors since the standard basis $ stdbasis_n $ has $ n $ vectors. Thus, this definition of `dimension' generalizes the most familiar use of term, that $ Re^n $ is $ n $ - dimensional. The space $ polyspace_n $ of polynomials of degree at most $ n $ has dimension $ n+1 $ . We can show this by exhibiting any basisDash $ sequence{1, x, dots, x^n} $ comes to mindDash and counting its members. The space of functions $ set{acdotcostheta+bcdotsinthetasuchthat a, binRe} $ of the real variable $ theta $ has dimension  $ 2 $ since this space has the basis $ sequence{costheta, sintheta} $ . A trivial space is zero - dimensional since its basis is empty. Again, although we sometimes say `finite - dimensional' for emphasis, from now on we take all vector spaces to be finite - dimensional. So in the next result the word `space' means `finite - dimensional vector space'. No linearly independent set can have a size greater than the dimension of the enclosing space. The proof of $ X $ never uses that $ D $ spans the space, only that it is linearly independent. Recall the diagram from Example I. showing the subspaces of $ Re^3 $ . Each subspace is described with a minimal spanning set, a basis. The whole space has a basis with three members, the plane subspaces have bases with two members, the line subspaces have bases with one member, and the trivial subspace has a basis with zero members. In that section we could not show that these are $ Re^3 $ 's only subspaces. We can show it now. The prior corollary proves that There are no, say, five - dimensional subspaces of three - space. Further, by $ X $ the dimension of every space is a whole number so there are no subspaces of $ Re^3 $ that are somehow $ (i)5 $ - dimensional, between lines and planes. Thus the list of subspaces that we gave is exhaustive; the only subspaces of $ Re^3 $ are either three - hbox{}, two - hbox{}, one - hbox{}, or zero - dimensional. Any linearly independent set can be expanded to make a basis. If a linearly independent set is not already a basis then it must not span the space. Adding to the set a vector that is not in the span will preserve linear independence by Lemma II.. Keep adding until the resulting set does span the space, which the prior corollary shows will happen after only a finite number of steps. Any spanning set can be shrunk to a basis. Call the spanning set $ S $ . If $ S $ is empty then it is already a basis (the space must be a trivial space). If $ S=set{zero} $ then it can be shrunk to the empty basis, thereby making it linearly independent, without changing its span. Otherwise, $ S $ contains a vector $ vec{s}_1 $ with $ vec{s}_1neqzero $ and we can form a basis $ B_1=sequence{vec{s}_1} $ . If $ spanof{B_1}=spanof{S} $ then we are done. If not then there is a $ vec{s}_2inspanof{S} $ such that $ vec{s}_2notinspanof{B_1} $ . Let $ B_2=sequence{vec{s}_1, vec{s_2}} $ ; by Lemma II. this is linearly independent so if $ spanof{B_2}=spanof{S} $ then we are done. We can repeat this process until the spans are equal, which must happen in at most finitely many steps. In an $ n $ - dimensional space, a set composed of $ n $ vectors is linearly independent if and only if it spans the space. First we will show that a subset with $ n $ vectors is linearly independent if and only if it is a basis. The `if' is trivially trueDash bases are linearly independent. `Only if' holds because a linearly independent set can be expanded to a basis, but a basis has $ n $ elements, so this expansion is actually the set that we began with. To finish, we will show that any subset with $ n $ vectors spans the space if and only if it is a basis. Again, `if' is trivial. `Only if' holds because any spanning set can be shrunk to a basis, but a basis has $ n $ elements and so this shrunken set is just the one we started with. The main result of this subsection, that all of the bases in a finite - dimensional vector space have the same number of elements, is the single most important result in this book. As $ X $ shows, it describes what vector spaces and subspaces there can be. One immediate consequence brings us back to when we considered the two things that could be meant by the term `minimal spanning set'. At that point we defined `minimal' as linearly independent but we noted that another reasonable interpretation of the term is that a spanning set is `minimal' when it has the fewest number of elements of any set with the same span. Now that we have shown that all bases have the same number of elements, we know that the two senses of `minimal' are equivalent. We will now reconsider linear systems and Gauss's Method, aided by the tools and terms of this chapter. We will make three points. For the first, recall the insight from the Chapter One that Gauss's Method works by taking linear combinations of rowsDash if two matrices are related by row operations $ Alongrightarrowcdotslongrightarrow B $ then each row of $ B $ is a linear combination of the rows of $ A $ . Therefore, the right setting in which to study row operations in general, and Gauss's Method in particular, is the following vector space. The row space/ of a matrix is the span of the set of its rows. The row rank/ is the dimension of this space, the number of linearly independent rows. If $ A=[r] 2 &3  4 &6 $ . then $ rowspace{A} $ is this subspace of the space of two - component row vectors. $ set{c_1cdotrowvec{2 &3}+c_2cdotrowvec{4 &6} suchthat c_1, c_2inRe } $ . The second row vector is linearly dependent on the first and so we can simplify the above description to $ set{ccdotrowvec{2 &3}suchthat cinRe } $ . If two matrices $ A $ and $ B $ are related by a row operation $ Agrstep{rho_ileftrightarrowrho_j}B quadtext{or}quad Agrstep{krho_i}B quadtext{or}quad Agrstep{krho_i+rho_j}B $ . (for $ ineq j $ and $ kneq 0 $ ) then their row spaces are equal. Hence, row - equivalent matrices have the same row space and therefore the same row rank. Corollary One.III. shows that when mbox{ $ Alongrightarrow B $ } then each row of $ B $ is a linear combination of the rows of $ A $ . That is, in the above terminology, each row of $ B $ is an element of the row space of $ A $ . Then $ rowspace{B}subseteqrowspace{A} $ follows because a member of the set $ rowspace{B} $ is a linear combination of the rows of $ B $ , so it is a combination of combinations of the rows of $ A $ , and by the Linear Combination Lemma is also a member of $ rowspace{A} $ . For the other set containment, recall Lemma One.III., that row operations are reversible so mbox{ $ Alongrightarrow B $ } if and only if mbox{ $ Blongrightarrow A $ }. Then $ rowspace{A}subseteqrowspace{B} $ follows as in the previous paragraph. Of course, Gauss's Method performs the row operations systematically, with the goal of echelon form. The nonzero rows of an echelon form matrix make up a linearly independent set. Lemma One.III. says that no nonzero row of an echelon form matrix is a linear combination of the other rows. This result restates that using this chapter's terminology. Thus, in the language of this chapter, Gaussian reduction works by eliminating linear dependences among rows, leaving the span unchanged, until no nontrivial linear relationships remain among the nonzero rows. In short, Gauss's Method produces a basis for the row space. From any matrix, we can produce a basis for the row space by performing Gauss's Method and taking the nonzero rows of the resulting echelon form matrix. For instance, $ [r] 1 &3 &1  1 &4 &1  2 &0 &5 grstep[ - 2rho_1+rho_3]{ - rho_1+rho_2} repeatedgrstep{6rho_2+rho_3} [r] 1 &3 &1  0 &1 &0  0 &0 &3 $ . produces the basis $ sequence{rowvec{1 &3 &1}, rowvec{0 &1 &0}, rowvec{0 &0 &3} } $ for the row space. This is a basis for the row space of both the starting and ending matrices, since the two row spaces are equal. Using this technique, we can also find bases for spans not directly involving row vectors. The column space of a matrix is the span of the set of its columns. The column rank is the dimension of the column space, the number of linearly independent columns. Our interest in column spaces stems from our study of linear systems. An example is that this system $ {3} c_1 &+ &3c_2 &+ &7c_3 &= &d_1  2c_1 &+ &3c_2 &+ &8c_3 &= &d_2  & &c_2 &+ &2c_3 &= &d_3  4c_1 & & &+ &4c_3 &= &d_4 $ . has a solution if and only if the vector of $ d $ 's is a linear combination of the other column vectors, $ c_1colvec[r]{1  2  0  4} +c_2colvec[r]{3  3  1  0} +c_3colvec[r]{7  8  2  4} =colvec{d_1  d_2  d_3  d_4} $ . meaning that the vector of $ d $ 's is in the column space of the matrix of coefficients. Given this matrix, $ [r] 1 &3 &7  2 &3 &8  0 &1 &2  4 &0 &4 $ . to get a basis for the column space, temporarily turn the columns into rows and reduce. $ [r] 1 &2 &0 &4  3 &3 &1 &0  7 &8 &2 &4 grstep[ - 7rho_1+rho_3]{ - 3rho_1+rho_2} repeatedgrstep{ - 2rho_2+rho_3} [r] 1 &2 &0 &4  0 & - 3 &1 & - 12  0 &0 &0 &0 $ . Now turn the rows back to columns. $ sequence{ colvec[r]{1  2  0  4}, colvec[r]{0  - 3  1  - 12} } $ . The result is a basis for the column space of the given matrix. The transpose of a matrix is the result of interchanging its rows and columns, so that column  $ j $ of the matrix $ A $ is row  $ j $ of $ trans{A} $ and vice versa. noindent So we can summarize the prior example as ``transpose, reduce, and transpose back.'' We can even, at the price of tolerating the as - yet - vague idea of vector spaces being ``the same, '' use Gauss's Method to find bases for spans in other types of vector spaces. To get a basis for the span of $ set{x^2+x^4, 2x^2+3x^4, - x^2 - 3x^4} $ in the space $ polyspace_4 $ , think of these three polynomials as ``the same'' as the row vectors $ rowvec{0 &0 &1 &0 &1} $ , $ rowvec{0 &0 &2 &0 &3} $ , and $ rowvec{0 &0 & - 1 &0 & - 3} $ , apply Gauss's Method $ [r] 0 &0 &1 &0 &1  0 &0 &2 &0 &3  0 &0 & - 1 &0 & - 3 grstep[rho_1+rho_3]{ - 2rho_1+rho_2} repeatedgrstep{2rho_2+rho_3} [r] 0 &0 &1 &0 &1  0 &0 &0 &0 &1  0 &0 &0 &0 &0 $ . and translate back to get the basis $ sequence{x^2+x^4, x^4} $ . (As mentioned earlier, we will make the phrase ``the same'' precise at the start of the next chapter.) Thus, the first point for this subsection is that the tools of this chapter give us a more conceptual understanding of Gaussian reduction. For the second point observe that row operations on a matrix can change its column space. $ [r] 1 &2  2 &4 grstep{ - 2rho_1+rho_2} [r] 1 &2  0 &0 $ . The column space of the left - hand matrix contains vectors with a second component that is nonzero but the column space of the right - hand matrix contains only vectors whose second component is zero, so the two spaces are different. This observation makes next result surprising. Row operations do not change the column rank. Restated, if $ A $ reduces to $ B $ then the column rank of $ B $ equals the column rank of $ A $ . This proof will be finished if we show that row operations do not affect linear relationships among columns, because the column rank is the size of the largest set of unrelated columns. That is, we will show that a relationship exists among columns (such as that the fifth column is twice the second plus the fourth) if and only if that relationship exists after the row operation. But this is exactly the first theorem of this book, Theorem One.I.: in a relationship among columns, $ c_1cdotcolvec{a_{1, 1}  a_{2, 1}  vdots  a_{m, 1}} +dots+ c_ncdot colvec{a_{1, n}  a_{2, n}  vdots  a_{m, n}} =colvec[r]{0  0  vdotswithin{0}  0} $ . row operations leave unchanged the set of solutions $ (c_1, ldots, c_n) $ . Another way to make the point that Gauss's Method has something to say about the column space as well as about the row space is with Gauss - Jordan reduction. It ends with the reduced echelon form of a matrix, as here. $ [r] 1 &3 &1 &6  2 &6 &3 &16  1 &3 &1 &6 grstep{}cdotsgrstep{} [r] 1 &3 &0 &2  0 &0 &1 &4  0 &0 &0 &0 $ . Consider the row space and the column space of this result. The first point made earlier in this subsection says that to get a basis for the row space we can just collect the rows with leading entries. However, because this is in reduced echelon form, a basis for the column space is just as easy: collect the columns containing the leading entries, $ sequence{vec{e}_1, vec{e}_2} $ . Thus, for a reduced echelon form matrix we can find bases for the row and column spaces in essentially the same way, by taking the parts of the matrix, the rows or columns, containing the leading entries. For any matrix, the row rank and column rank are equal. Bring the matrix to reduced echelon form. Then the row rank equals the number of leading entries since that equals the number of nonzero rows. Then also, the number of leading entries equals the column rank because the set of columns containing leading entries consists of some of the $ vec{e}_i $ 's from a standard basis, and that set is linearly independent and spans the set of columns. Hence, in the reduced echelon form matrix, the row rank equals the column rank, because each equals the number of leading entries. But $ X $ and $ X $ show that the row rank and column rank are not changed by using row operations to get to reduced echelon form. Thus the row rank and the column rank of the original matrix are also equal. The rank/ of a matrix is its row rank or column rank. So the second point that we have made in this subsection is that the column space and row space of a matrix have the same dimension. Our final point is that the concepts that we've seen arising naturally in the study of vector spaces are exactly the ones that we have studied with linear systems. For linear systems with $ n $ unknowns and with matrix of coefficients $ A $ , the statements the rank of $ A $ is $ r $ the vector space of solutions of the associated homogeneous system has dimension $ n - r $ are equivalent. parnoindent So if the system has at least one particular solution then for the set of solutions, the number of parameters equals $ n - r $ , the number of variables minus the rank of the matrix of coefficients. The rank of $ A $ is $ r $ if and only if Gaussian reduction on $ A $ ends with $ r $ nonzero rows. That's true if and only if echelon form matrices row equivalent to $ A $ have $ r $ - many leading variables. That in turn holds if and only if there are $ n - r $ free variables. Where the matrix $ A $ is $ nbyn{n} $ , these statements the rank of $ A $ is $ n $ $ A $ is nonsingular the rows of $ A $ form a linearly independent set the columns of $ A $ form a linearly independent set any linear system whose matrix of coefficients is $ A $ has one and only one solution are equivalent. Clearly $ text{(1)}ifftext{(2)}ifftext{(3)}ifftext{(4)} $ . The last, $ text{(4)}ifftext{(5)} $ , holds because a set of $ n $ column vectors is linearly independent if and only if it is a basis for $ Re^n $ , but the system $ c_1colvec{a_{1, 1}  a_{2, 1}  vdots  a_{m, 1}} +dots+ c_ncolvec{a_{1, n}  a_{2, n}  vdots  a_{m, n}} =colvec{d_1  d_2  vdots  d_m} $ . has a unique solution for all choices of $ d_1, dots, d_ninRe $ if and only if the vectors of $ a $ 's on the left form a basis. Sometimes the results of this subsection are mistakenly remembered to say that the general solution of an $ m $  equations, $ n $  unknowns system uses $ n - m $ parameters. The number of equations is not the relevant number; rather, what matters is the number of independent equations, the number of equations in a maximal independent set. Where there are $ r $ independent equations, the general solution involves $ n - r $ parameters. textit{This subsection is optional. It is required only for the last sections of Chapter Three and Chapter Five and for occasional exercises. You can pass it over without loss of continuity.} One way to understand something is to see how to build it from component parts. For instance, we sometimes think of $ Re^3 $ put together from the $ x $ - axis, the $ y $ - axis, and $ z $ - axis. In this subsection we will describe how to decompose a vector space into a combination of some of its subspaces. In developing this idea of subspace combination, we will keep the $ Re^3 $ example in mind as a prototype. Subspaces are subsets and sets combine via union. But taking the combination operation for subspaces to be the simple set union operation isn't what we want. For instance, the union of the $ x $ - axis, the $ y $ - axis, and $ z $ - axis is not all of $ Re^3 $ . In fact this union is not a subspace because it is not closed under addition: this vector $ colvec[r]{1  0  0} + colvec[r]{0  1  0} + colvec[r]{0  0  1} = colvec[r]{1  1  1} $ . is in none of the three axes and hence is not in the union. Therefore to combine subspaces, in addition to the members of those subspaces, we must at least also include all of their linear combinations. Where $ W_1, dots, W_k $ are subspaces of a vector space, their sum is the span of their union $ W_1+W_2+dots +W_k=spanof{W_1union W_2union cdots W_k} $ . noindent Writing ` $ + $ ' fits with the conventional practice of using this symbol for a natural accumulation operation. Our $ Re^3 $ prototype works with this. Any vector $ vec{w}inRe^3 $ is a linear combination $ c_1vec{v}_1+c_2vec{v}_2+c_3vec{v}_3 $ where $ vec{v}_1 $ is a member of the $ x $ - axis, etc., in this way $ colvec{w_1  w_2  w_3} =1cdotcolvec{w_1  0  0} +1cdotcolvec{0  w_2  0} +1cdotcolvec{0  0  w_3} $ . and so $ text{ $ x $ - axis}+text{ $ y $ - axis}+text{ $ z $ - axis}=Re^3 $ . A sum of subspaces can be less than the entire space. Inside of $ polyspace_4 $ , let $ L $ be the subspace of linear polynomials $ set{a+bxsuchthat a, binRe} $ and let $ C $ be the subspace of purely - cubic polynomials $ set{cx^3suchthat cinRe} $ . Then mbox{ $ L+C $ } is not all of $ polyspace_4 $ . Instead, $ mbox{ $ L+C $ }=set{a+bx+cx^3suchthat a, b, cinRe} $ . A space can be described as a combination of subspaces in more than one way. Besides the decomposition $ Re^3=text{ $ x $ - axis}+text{ $ y $ - axis}+text{ $ z $ - axis} $ , we can also write $ Re^3=text{ $ xy $ - plane}+text{ $ yz $ - plane} $ . To check this, note that any $ vec{w}inRe^3 $ can be written as a linear combination of a member of the $ xy $ - plane and a member of the $ yz $ - plane; here are two such combinations. $ colvec{w_1  w_2  w_3} =1cdotcolvec{w_1  w_2  0} +1cdotcolvec{0  0  w_3} qquad colvec{w_1  w_2  w_3} =1cdotcolvec{w_1  w_2/2  0} +1cdotcolvec{0  w_2/2  w_3} $ . The above definition gives one way in which we can think of a space as a combination of some of its parts. However, the prior example shows that there is at least one interesting property of our benchmark model that is not captured by the definition of the sum of subspaces. In the familiar decomposition of $ Re^3 $ , we often speak of a vector's ` $ x $  part' or ` $ y $  part' or ` $ z $  part'. That is, in our prototype each vector has a unique decomposition into pieces from the parts making up the whole space. But in the decomposition used in $ X $ , we cannot refer to the `` $ xy $  part'' of a vectorDash these three sums $ colvec[r]{1  2  3} =colvec[r]{1  2  0} +colvec[r]{0  0  3} =colvec[r]{1  0  0} +colvec[r]{0  2  3} =colvec[r]{1  1  0} +colvec[r]{0  1  3} $ . all describe the vector as comprised of something from the first plane plus something from the second plane, but the `` $ xy $  part'' is different in each. That is, when we consider how $ Re^3 $ is put together from the three axes we might mean ``in such a way that every vector has at least one decomposition, '' which gives the definition above. But if we take it to mean ``in such a way that every vector has one and only one decomposition'' then we need another condition on combinations. To see what this condition is, recall that vectors are uniquely represented in terms of a basis. We can use this to break a space into a sum of subspaces such that any vector in the space breaks uniquely into a sum of members of those subspaces. Consider $ Re^3 $ with its standard basis $ stdbasis_3=sequence{vec{e}_1, vec{e}_2, vec{e}_3} $ . The subspace with the basis $ B_1=sequence{vec{e}_1} $ is the $ x $ - axis, the subspace with the basis $ B_2=sequence{vec{e}_2} $ is the $ y $ - axis, and the subspace with the basis $ B_3=sequence{vec{e}_3} $ is the $ z $ - axis. The fact that any member of $ Re^3 $ is expressible as a sum of vectors from these subspaces $ colvec{x  y  z} =colvec{x  0  0} +colvec{0  y  0} +colvec{0  0  z} $ . reflects the fact that $ stdbasis_3 $ spans the spaceDash this equation $ colvec{x  y  z} =c_1colvec[r]{1  0  0} +c_2colvec[r]{0  1  0} +c_3colvec[r]{0  0  1} $ . has a solution for any $ x, y, zinRe $ . And the fact that each such expression is unique reflects that fact that $ stdbasis_3 $ is linearly independent, so any equation like the one above has a unique solution. We don't have to take the basis vectors one at a time, we can conglomerate them into larger sequences. Consider again the space $ Re^3 $ and the vectors from the standard basis $ stdbasis_3 $ . The subspace with the basis $ B_1=sequence{vec{e}_1, vec{e}_3} $ is the $ xz $ - plane. The subspace with the basis $ B_2=sequence{vec{e}_2} $ is the $ y $ - axis. As in the prior example, the fact that any member of the space is a sum of members of the two subspaces in one and only one way $ colvec{x  y  z} =colvec{x  0  z} +colvec{0  y  0} $ . is a reflection of the fact that these vectors form a basisDash this equation $ colvec{x  y  z} =(c_1colvec[r]{1  0  0} +c_3colvec[r]{0  0  1}) +c_2colvec[r]{0  1  0} $ . has one and only one solution for any $ x, y, zinRe $ . The concatenation of the sequences $ B_1=sequence{vec{beta}_{1, 1}, dots, vec{beta}_{1, n_1}} $ , ldots, $ B_k=sequence{vec{beta}_{k, 1}, dots, vec{beta}_{k, n_k}} $ adjoins them into a single sequence. $ cat{cat{cat{B_1}{B_2}}{cdots}}{B_k}= sequence{vec{beta}_{1, 1}, dots, vec{beta}_{1, n_1}, vec{beta}_{2, 1}, dots, vec{beta}_{k, n_k} } $ . Let $ V $ be a vector space that is the sum of some of its subspaces $ V=W_1+dots+W_k $ . Let $ B_1 $ , ldots, $ B_k $ be bases for these subspaces. The following are equivalent. The expression of any $ vec{v}in V $ as a combination $ vec{v}=vec{w}_1+dots+vec{w}_k $ with $ vec{w}_iin W_i $ is unique. The concatenation $ cat{cat{B_1}{cdots}}{B_k} $ is a basis for $ V $ . Among nonzero vectors from different $ W_i $ 's every linear relationship is trivial. We will show that $ text{(1)}impliestext{(2)} $ , that $ text{(2)}impliestext{(3)} $ , and finally that $ text{(3)}impliestext{(1)} $ . For these arguments, observe that we can pass from a combination of $ vec{w} $ 's to a combination of $ vec{beta} $ 's $ d_1vec{w}_1+dots+d_kvec{w}_k &= d_1(c_{1, 1}vec{beta}_{1, 1}+dots+c_{1, n_1}vec{beta}_{1, n_1})  &qquadhbox{}+dots +d_k(c_{k, 1}vec{beta}_{k, 1}+dots+c_{k, n_k}vec{beta}_{k, n_k})  &= d_1c_{1, 1}cdotvec{beta}_{1, 1} +dots +d_kc_{k, n_k}cdotvec{beta}_{k, n_k} tag{ $ * $ } $ . and vice versa (we can move from the bottom to the top by taking each $ d_i $ to be $ 1 $ ). For $ text{(1)}impliestext{(2)} $ , assume that all decompositions are unique. We will show that $ cat{cat{B_1}{cdots}}{B_k} $ spans the space and is linearly independent. It spans the space because the assumption that $ V=W_1+dots+W_k $ means that every $ vec{v} $ can be expressed as $ vec{v}=vec{w}_1+dots+vec{w}_k $ , which translates by equation ( $ * $ ) to an expression of $ vec{v} $ as a linear combination of the $ vec{beta} $ 's from the concatenation. For linear independence, consider this linear relationship. $ zero=c_{1, 1}vec{beta}_{1, 1}+dots+c_{k, n_k}vec{beta}_{k, n_k} $ . Regroup as in ( $ * $ ) (that is, move from bottom to top) to get the decomposition $ zero=vec{w}_1+dots+vec{w}_k $ . Because the zero vector obviously has the decomposition $ zero=zero+dots+zero $ , the assumption that decompositions are unique shows that each $ vec{w}_i $ is the zero vector. This means that $ c_{i, 1}vec{beta}_{i, 1}+dots+c_{i, n_i}vec{beta}_{i, n_i}=zero $ , and since each $ B_i $ is a basis we have the desired conclusion that all of the $ c $ 's are zero. For $ text{(2)}impliestext{(3)} $ assume that the concatenation of the bases is a basis for the entire space. Consider a linear relationship among nonzero vectors from different $ W_i $ 's. This might or might not involve a vector from $ W_1 $ , or one from $ W_2 $ , etc., so we write it $ zero=mbox{}cdots+d_ivec{w}_i+cdotsmbox{} $ . As in equation ( $ * $ ) expand the vector. $ zero &= mbox{}dots +d_i(c_{i, 1}vec{beta}_{i, 1}+dots+c_{i, n_i}vec{beta}_{i, n_i}) +cdotsmbox{}  &= mbox{}dots +d_ic_{i, 1}cdotvec{beta}_{i, 1} +dots+ d_ic_{i, n_i}cdotvec{beta}_{i, n_i} +cdotsmbox{} $ . The linear independence of $ cat{cat{B_1}{cdots}}{B_k} $ gives that each coefficient $ d_ic_{i, j} $ is zero. Since $ vec{w}_i $ is nonzero vector, at least one of the $ c_{i, j} $ 's is not zero, and thus $ d_i $ is zero. This holds for each $ d_i $ , and therefore the linear relationship is trivial. Finally, for $ text{(3)}impliestext{(1)} $ , assume that among nonzero vectors from different $ W_i $ 's any linear relationship is trivial. Consider two decompositions of a vector $ vec{v}=mbox{}cdots+vec{w}_i+cdotsmbox{} $ and $ vec{v}=mbox{}cdots+vec{u}_j+cdotsmbox{} $ where $ vec{w}_iin W_i $ and $ vec{u}_jin W_j $ . Subtract one from the other to get a linear relationship, something like this (if there is no $ vec{u}_i $ or $ vec{w}_j $ then leave those out). $ zero =mbox{}cdots+(vec{w}_i - vec{u}_i) +cdots +(vec{w}_j - vec{u}_j)+cdotsmbox{} $ . The case assumption that statement (3) holds implies that the terms each equal the zero vector $ vec{w}_i - vec{u}_i=zero $ . Hence decompositions are unique. A collection of subspaces $ set{W_1, ldots, W_k} $ is independent if no nonzero vector from any $ W_i $ is a linear combination of vectors from the other subspaces $ W_1, dots, W_{i - 1}, W_{i+1}, dots, W_k $ . A vector space $ V $ is the direct sum (or internal direct sum) of its subspaces $ W_1, dots, W_k $ if $ V=W_1+W_2+dots +W_k $ and the collection $ set{W_1, dots, W_k} $ is independent. We write $ V=W_1directsum W_2directsum cdotsdirectsum W_k $ . Our prototype works: $ Re^3=text{ $ x $ - axis}directsumtext{ $ y $ - axis}directsumtext{ $ z $ - axis} $ . The space of $ nbyn{2} $ matrices is this direct sum. $ set{ a &0  0 &d suchthat a, dinRe } , mbox{}directsummbox{}, set{ 0 &b  0 &0 suchthat binRe } , mbox{}directsummbox{}, set{ 0 &0  c &0 suchthat cinRe } $ . It is the direct sum of subspaces in many other ways as well; direct sum decompositions are not unique. The dimension of a direct sum is the sum of the dimensions of its summands. In $ X $ , the number of basis vectors in the concatenation equals the sum of the number of vectors in the sub - bases. The special case of two subspaces is worth its own mention. When a vector space is the direct sum of two of its subspaces then they are complements. A vector space $ V $ is the direct sum of two of its subspaces $ W_1 $ and $ W_2 $ if and only if it is the sum of the two $ V=W_1+W_2 $ and their intersection is trivial $ W_1intersection W_2=set{zero, } $ . Suppose first that $ V=W_1directsum W_2 $ . By definition, $ V $ is the sum of the two $ V=W_1+ W_2 $ . To show that their intersection is trivial let $ vec{v} $ be a vector from $ W_1intersection W_2 $ and consider the equation $ vec{v}=vec{v} $ . On that equation's left side is a member of $ W_1 $ and on the right is a member of $ W_2 $ , which we can think of as a linear combination of members of $ W_2 $ . But the two spaces are independent so the only way that a member of $ W_1 $ can be a linear combination of vectors from $ W_2 $ is if that member is the zero vector $ vec{v}=zero $ . For the other direction, suppose that $ V $ is the sum of two spaces with a trivial intersection. To show that $ V $ is a direct sum of the two we need only show that the spaces are independentDash that no nonzero member of the first is expressible as a linear combination of members of the second, and vice versa. This holds because any relationship $ vec{w}_1=c_1vec{w}_{2, 1}+dots+c_kvec{w}_{2, k} $ (with $ vec{w}_1in W_1 $ and $ vec{w}_{2, j}in W_2 $ for all $ j $ ) shows that the vector on the left is also in $ W_2 $ , since the right side is a combination of members of $ W_2 $ . The intersection of these two spaces is trivial, so $ vec{w}_1=zero $ . The same argument works for any $ vec{w}_2 $ . In $ Re^2 $ the $ x $ - axis and the $ y $ - axis are complements, that is, $ Re^2=text{ $ x $ - axis}directsumtext{ $ y $ - axis} $ . This points out that subspace complement is slightly different than set complement; the $ x $ and  $ y $ axes are not set complements because their intersection is not the empty set. A space can have more than one pair of complementary subspaces; another pair for  $ Re^2 $ are the subspaces consisting of the lines $ y=x $ and $ y=2x $ . In the space $ F=set{acostheta+bsinthetasuchthat a, binRe} $ , the subspaces $ W_1=set{acosthetasuchthat ainRe} $ and $ W_2=set{bsinthetasuchthat binRe} $ are complements. The prior example noted that a space can be decomposed into more than one pair of complements. In addition note that $ F $ can has more than one pair of complementary subspaces where the first in the pair is $ W_1 $ Dash another complement of $ W_1 $ is $ W_3=set{bsintheta+bcostheta suchthat binRe} $ . In $ Re^3 $ , the $ xy $ - plane and the $ yz $ - planes are not complements, which is the point of the discussion following $ X $ . One complement of the $ xy $ - plane is the $ z $ - axis. Here is a natural question that arises from $ X $ : for $ k>2 $ is the simple sum $ V=W_1+dots+W_k $ also a direct sum if and only if the intersection of the subspaces is trivial? If there are more than two subspaces then having a trivial intersection is not enough to guarantee unique decomposition (i.e., is not enough to ensure that the spaces are independent). In $ Re^3 $ , let $ W_1 $ be the $ x $ - axis, let $ W_2 $ be the $ y $ - axis, and let $ W_3 $ be this. $ W_3=set{colvec{q  q  r} suchthat q, rinRe} $ . The check that $ Re^3=W_1+W_2+W_3 $ is easy. The intersection $ W_1intersection W_2intersection W_3 $ is trivial, but decompositions aren't unique. $ colvec{x  y  z} =colvec{0  0  0} +colvec{0  y - x  0} +colvec{x  x  z} =colvec{x - y  0  0} +colvec{0  0  0} +colvec{y  y  z} $ . (This example also shows that this requirement is also not enough: that all pairwise intersections of the subspaces be trivial. See $ X $ .) In this subsection we have seen two ways to regard a space as built up from component parts. Both are useful; in particular we will use the direct sum definition at the end of the Chapter Five.
The prior section shows how to understand a vector space as a span, as an unrestricted linear combination of some of its elements. For example, the space of linear polynomials $ set{a+bxsuchthat a, binRe} $ is spanned by the set $ set{1, x} $ . The prior section also showed that a space can have many sets that span it. Two more sets that span the space of linear polynomials are $ set{1, 2x} $ and $ set{1, x, 2x} $ . At the end of that section we described some spanning sets as `minimal' but we never precisely defined that word. We could mean that a spanning set is minimal if it contains the smallest number of members of any set with the same span, so that $ set{1, x, 2x} $ is not minimal because it has three members while we can give two - element sets spanning the same space. Or we could mean that a spanning set is minimal when it has no elements that we can remove without changing the span. Under this meaning $ set{1, x, 2x} $ is not minimal because removing the $ 2x $ to get $ set{1, x} $ leaves the span unchanged. The first sense of minimality appears to be a global requirement, in that to check if a spanning set is minimal we seemingly must look at all the sets that span and find one with the least number of elements. The second sense of minimality is local since we need to look only at the set and consider the span with and without various elements. For instance, using the second sense we could compare the span of $ set{1, x, 2x} $ with the span of $ set{1, x} $ and note that $ 2x $ is a ``repeat'' in that its removal doesn't shrink the span. In this section we will use the second sense of `minimal spanning set' because of this technical convenience. However, the most important result of this book is that the two senses coincide. We will prove that in the next section. We saw ``repeats'' in the first chapter. There, Gauss's Method turned them into $ 0=0 $ equations. Recall the hyperlink{ex:Statics}{Statics} example from Chapter One's opening. We got two balances with the pair of unknown - mass objects, one at $ 40 $  cm and $ 15 $  cm and another at $ - 50 $  cm and $ 25 $  cm, and we then computed the value of those masses. Had we instead gotten the second balance at $ 20 $  cm and $ (vii)5 $  cm then Gauss's Method on the resulting two - equations, two - unknowns system would not have yielded a solution, it would have yielded a $ 0=0 $ equation along with an equation containing a free variable. Intuitively, the problem is that $ rowvec{20 &(vii)5} $ is half of $ rowvec{40 &15} $ , that is, $ rowvec{20 &(vii)5} $ is in the span of the set $ set{rowvec{40 &15}} $ and so is repeated data. We would have been trying to solve a two - unknowns problem with essentially only one piece of information. We take $ vec{v} $ to be a ``repeat'' of the vectors in a set  $ S $ if $ vec{v}inspanof{S} $ so that it depends on, that is, is expressible in terms of, elements of the set $ vec{v}=c_1vec{s}_1+cdots+c_nvec{s}_n $ . Where $ V $ is a vector space, $ S $ is a subset of that space, and $ vec{v} $ is an element of that space, $ spanof{Sunionset{vec{v}}} = spanof{S} $ if and only if $ vec{v}inspanof{S} $ . Half of the if and only if is immediate: if $ vec{v}notinspanof{S} $ then the sets are not equal because $ vec{v}inspanof{Sunionset{vec{v}}} $ . For the other half assume that $ vec{v}inspanof{S} $ so that $ vec{v}=c_1vec{s}_1+cdots+c_nvec{s}_n $ for some scalars  $ c_i $ and vectors $ vec{s}_iin S $ . We will use mutual containment to show that the sets $ spanof{Sunionset{vec{v}}} $ and $ spanof{S} $ are equal. The containment $ spanof{Sunionset{vec{v}}}supseteqspanof{S} $ is clear. To show containment in the other direction let $ vec{w} $ be an element of  $ spanof{Sunionset{vec{v}}} $ . Then $ vec{w} $ is a linear combination of elements of $ Sunionset{vec{v}} $ , which we can write as $ vec{w}=c_{n+1}vec{s}_{n+1}+cdots+c_{n+k}vec{s}_{n+k}+c_{n+k+1}vec{v} $ . (Possibly some of the $ vec{s}_i $ 's from $ vec{w} $ 's equation are the same as some of those from $ vec{v} $ 's equation but that does not matter.) Expand $ vec{v} $ . $ vec{w}=c_{n+1}vec{s}_{n+1}+cdots+c_{n+k}vec{s}_{n+k} +c_{n+k+1}cdot(c_1vec{s}_1+cdots+c_nvec{s}_n) $ . Recognize the right hand side as a linear combination of linear combinations of vectors from  $ S $ . Thus $ vec{w}inspanof{S} $ . The discussion at the section's opening involved removing vectors, not adding them. For $ vec{v}in S $ , omitting that vector does not shrink the span if and only if that vector is dependent on other vectors in the set. That is, $ spanof{S}=spanof{S - set{vec{v}}} $ if and only if $ vec{v}inspanof{S - set{vec{v}}} $ . Thus, to know whether removing a vector will decrease the span, we need to know whether the vector is a linear combination of others in the set. In any vector space, a set of vectors is linearly independent if none of its elements is a linear combination of the others from the set.footnote{See also $ X $ .}spacefactor=1000 Otherwise the set is linearly dependent. Thus the set $ set{vec{s}_0, ldots, vec{s}_n} $ is independent if there is no equality $ vec{s}_i=c_0vec{s}_0+ldots+c_{i - 1}vec{s}_{i - 1}+c_{i+1}vec{s}_{i+1}+ldots+c_nvec{s}_n $ . The definition's use of the word `others' means that writing $ vec{s}_i $ as a linear combination via $ vec{s}_i=1cdotvec{s}_i $ does not count. Observe that, although this way of writing one vector as a combination of the others $ vec{s}_0=lincombo{c}{vec{s}} $ . visually sets off $ vec{s}_0 $ , algebraically there is nothing special about that vector in that equation. For any $ vec{s}_i $ with a coefficient $ c_i $ that is non - $ 0 $ , we can rewrite to isolate $ vec{s}_i $ . $ vec{s}_i=(1/c_i)vec{s}_0+dots +( - c_{i - 1}/c_i)vec{s}_{i - 1}+( - c_{i+1}/c_i)vec{s}_{i+1} +dots+( - c_n/c_i)vec{s}_n $ . When we don't want to single out any vector we will instead say that $ vec{s}_0, vec{s}_1, dots, vec{s}_n $ are in a linear relationship and put all of the vectors on the same side. The next result rephrases the linear independence definition in this style. It is how we usually compute whether a finite set is dependent or independent. A subset $ S $ of a vector space is linearly independent if and only if among its elements the only linear relationship $ c_1vec{s}_1+dots+c_nvec{s}_n=zero $ is the trivial one, $ c_1=0, dots, , c_n=0 $ (where $ vec{s}_ineqvec{s}_j $ when $ ineq j $ ) . If $ S $ is linearly independent then no vector $ vec{s}_i $ is a linear combination of other vectors from $ S $ , so there is no linear relationship where some of the $ vec{s}, $ 's have nonzero coefficients. If $ S $ is not linearly independent then some $ vec{s}_i $ is a linear combination $ vec{s}_i=c_1vec{s}_1+dots+c_{i - 1}vec{s}_{i - 1} +c_{i+1}vec{s}_{i+1}+dots+c_nvec{s}_n $ of other vectors from $ S $ . Subtracting $ vec{s}_i $ from both sides gives a relationship involving a nonzero coefficient, the $ - 1 $ in front of $ vec{s}_i $ . In the vector space of two - wide row vectors, the two - element set $ set{ rowvec{40 &15}, rowvec{ - 50 &25}} $ is linearly independent. To check this, take $ c_1cdotrowvec{40 &15}+c_2cdotrowvec{ - 50 &25}=rowvec{0 &0} $ . and solve the resulting system. $ {2} 40c_1 & - &50c_2 &= &0  15c_1 &+ &25c_2 &= &0 grstep{ - (15/40)rho_1+rho_2} {2} 40c_1 & - &50c_2 &= &0  & &(175/4)c_2 &= &0 $ . Both $ c_1 $ and $ c_2 $ are zero. So the only linear relationship between the two given row vectors is the trivial relationship. In the same vector space, the set $ set{ rowvec{40 &15}, rowvec{20 &(vii)5}} $ is linearly dependent since we can satisfy $ c_1cdot rowvec{40 &15}+c_2cdotrowvec{20 &(vii)5}=rowvec{0 &0} $ with $ c_1=1 $ and $ c_2= - 2 $ . The set $ set{1+x, 1 - x} $ is linearly independent in $ polyspace_2 $ , the space of quadratic polynomials with real coefficients, because $ 0+0x+0x^2 = c_1(1+x)+c_2(1 - x) = (c_1+c_2)+(c_1 - c_2)x+0x^2 $ . gives $ {2} c_1 &+ &c_2 &= &0  c_1 & - &c_2 &= &0 grstep{ - rho_1+rho_2} {2} c_1 &+ &c_2 &= &0  & &2c_2 &= &0 $ . since polynomials are equal only if their coefficients are equal. Thus, the only linear relationship between these two members of $ polyspace_2 $ is the trivial one. The lemma specifies that $ vec{s}_ineqvec{s}_j $ when $ ineq j $ because of course if some vector  $ vec{s} $ appears twice then we can get a nontrivial $ c_1vec{s}_1+dots+c_nvec{s}_n=zero $ , by taking the associated coefficients to be $ 1 $ and  $ - 1 $ . Besides, if some vector appears more than once in an expression then we can always combine the coefficients. Note that the lemma allows the opposite of appearing more than once, that some vectors from  $ S $ don't appear at all. For instance, if  $ S $ is infinite then because linear relationships involve only finitely many vectors, any such relationship leaves out many of $ S $ 's vectors. However, note also that if  $ S $ is finite then where convenient we can take a combination $ c_1vec{s}_1+dots+c_nvec{s}_n $ to contain each of $ S $ 's vectors once and only once. If a vector is missing then we can add it by using a coefficient of  $ 0 $ . The rows of this matrix $ A= [r] 2 &3 &1 &0  0 & - 1 &0 & - 2  0 &0 &0 &1 $ . form a linearly independent set. This is easy to check for this case but also recall that Lemma One.III. shows that the rows of any echelon form matrix make a linearly independent set. In $ Re^3 $ , where $ vec{v}_1=colvec{3  4  5} quad vec{v}_2=colvec{2  9  2} quad vec{v}_3=colvec{4  18  4} $ . the set $ S=set{vec{v}_1, vec{v}_2, vec{v}_3} $ is linearly dependent because this is a relationship $ 0cdotvec{v}_1 +2cdotvec{v}_2 - 1cdotvec{v}_3 =zero $ . where not all of the scalars are zero (the fact that some of the scalars are zero doesn't matter). That example illustrates why, although $ X $ is a clearer statement of what independence means, $ X $ is better for computations. Working straight from the definition, someone trying to compute whether $ S $ is linearly independent would start by setting $ vec{v}_1=c_2vec{v}_2+c_3vec{v}_3 $ and concluding that there are no such $ c_2 $ and $ c_3 $ . But knowing that the first vector is not dependent on the other two is not enough. This person would have to go on to try $ vec{v}_2=c_1vec{v}_1+c_3vec{v}_3 $ , in order to find the dependence $ c_1=0 $ , $ c_3=1/2 $ . $ X $ gets the same conclusion with only one computation. The empty subset of a vector space is linearly independent. There is no nontrivial linear relationship among its members as it has no members. In any vector space, any subset containing the zero vector is linearly dependent. One example is, in the space $ polyspace_2 $ of quadratic polynomials, the subset $ set{1+x, x+x^2, 0} $ . It is linearly dependent because $ 0cdotvec{v}_1+0cdotvec{v}_2+1cdotzero=zero $ is a nontrivial relationship, since not all of the coefficients are zero. There is a subtle point that we shall see a number of times and that bears on the prior example. It is about the trivial sum, the sum of the empty set. One way to see how to define the trivial sum is to consider the progression $ vec{v}_1+vec{v}_2+vec{v}_3 $ , followed by $ vec{v}_1+vec{v}_2 $ , followed by $ vec{v}_1 $ . The difference between the sum of three vectors and the sum of two is $ vec{v}_3 $ . Then the difference between the sum of two and the sum of one is $ vec{v}_2 $ . In next passing to the trivial sum, the sum of zero - many vectors, we can expect to subtract  $ vec{v}_1 $ . So we define the sum of zero - many vectors to be the zero vector. The relation with the prior example is that if the zero vector is in a set then that set has an element that is a combination of a subset of other vectors from the set, specifically, the zero vector is a combination of the empty subset. Even the set $ S=set{zero} $ is linearly dependent, because $ zero $ is the sum of the empty set and the empty set is a subset of  $ S $ . The definition of linear independence, $ X $ , refers to a `set' of vectors. Sets are the most familiar kind of collection and in practice everyone uses the word `set' in this context. But to be complete, we will note that sets are not quite the right kind of collection for this purpose. Recall that a set is a collection with two properties: (i) order does not matter, so that the set $ set{1, 2} $ equals the set $ set{2, 1} $ , and (ii) duplicates collapse, so that the set $ set{1, 1, 2} $ equals the set $ set{1, 2} $ . Now consider this matrix reduction. $ 1 &1 &1  2 &2 &2  1 &2 &3 grstep{(1/2)rho_2} 1 &1 &1  1 &1 &1  1 &2 &3 $ . On the left the set of matrix rows $ set{rowvec{1 &1 &1}, rowvec{2 &2 &2}, rowvec{1 &2 &3}} $ is linearly dependent. On the right the set of rows is $ set{rowvec{1 &1 &1}, rowvec{1 &1 &1}, rowvec{1 &2 &3}} $ . Because duplicates collapse, that equals the set $ set{rowvec{1 &1 &1}, rowvec{1 &2 &3}} $ , which is linearly independent. This is a problem because Gauss's Method should preserve linear dependence. That is, strictly speaking, we need a type of collection where duplicates do not collapse. A collection where order does not matter and duplicates don't collapse is a multiset. However, while insisting on being completely correct has advantages, departing from the standard terminology of `set' would have pitfalls of its own, so we will continue to use that word. Later, we shall occasionally need to take combinations without letting duplicates collapse and we shall do that without further comment. A set $ S $ is linearly independent if and only if for any $ vec{v}in S $ , its removal shrinks the span $ spanof{S - set{v}}subsetneqspanof{S} $ . This follows from $ X $ . If $ S $ is linearly independent then none of its vectors is dependent on the other elements, so removal of any vector will shrink the span. If $ S $ is not linearly independent then it contains a vector that is dependent on other elements of the set, and removal of that vector will not shrink the span. So a spanning set is minimal if and only if it is linearly independent. The prior result addresses removing elements from a linearly independent set. The next one adds elements. Suppose that $ S $ is linearly independent and that $ vec{v}notin S $ . Then the set $ Sunionset{vec{v}} $ is linearly independent if and only if $ vec{v}notinspanof{S} $ . We will show that $ Sunionset{vec{v}} $ is not linearly independent if and only if $ vec{v}inspanof{S} $ . Suppose first that $ vec{v}inspanof{S} $ . Express $ vec{v} $ as a combination $ vec{v}=c_1vec{s}_1+cdots+c_nvec{s}_n $ . Rewrite that $ zero=c_1vec{s}_1+cdots+c_nvec{s}_n - 1cdotvec{v} $ . Since $ vec{v}notin S $ , it does not equal any of the $ vec{s}_i $ so this is a nontrivial linear dependence among the elements of $ Sunionset{vec{v}} $ . Thus that set is not linearly independent. Now suppose that $ Sunionset{vec{v}} $ is not linearly independent and consider a nontrivial dependence among its members $ zero=c_1vec{s}_1+cdots+c_nvec{s}_n+c_{n+1}cdotvec{v} $ . If $ c_{n+1}=0 $ then that is a dependence among the elements of  $ S $ , but we are assuming that  $ S $ is independent, so $ c_{n+1}neq 0 $ . Rewrite the equation as $ vec{v}=(c_1/c_{n+1})vec{s}_1+cdots+(c_n/c_{n+1})vec{s}_n $ to get $ vec{v}inspanof{S} $ This subset of $ Re^3 $ is linearly independent. $ S =set{colvec{1  0  0}} $ . The span of $ S $ is the $ x $ - axis. Here are two supersets, one that is linearly dependent and the other independent. We got the dependent superset by adding a vector from the $ x $ - axis and so the span did not grow. We got the independent superset by adding a vector that isn't in $ spanof{S} $ , because it has a nonzero $ y $  component, causing the span to grow. For the independent set $ S =set{colvec{1  0  0}, colvec{0  1  0} } $ . the span $ spanof{S} $ is the $ xy $ - plane. Here are two supersets. As above, the additional member of the dependent superset comes from $ spanof{S} $ , the $ xy $ - plane, while the added member of the independent superset comes from outside of that span. Finally, consider this independent set $ S =set{colvec{1  0  0}, colvec{0  1  0}, colvec{0  0  1} } $ . with $ spanof{S}=Re^3 $ . We can get a linearly dependent superset. But there is no linearly independent superset of $ S $ . One way to see that is to note that for any vector that we would add to $ S $ , the equation $ colvec{x  y  z} =c_1colvec{1  0  0} +c_2colvec{0  1  0} +c_3colvec{0  0  1} $ . has a solution $ c_1=x $ , $ c_2=y $ , and $ c_3=z $ . Another way to see it is that we cannot add any vectors from outside of the span $ spanof{S} $ because that span is $ Re^3 $ . In a vector space, any finite set has a linearly independent subset with the same span. If $ S=set{ vec{s}_1, dots, vec{s}_n} $ is linearly independent then $ S $ itself satisfies the statement, so assume that it is linearly dependent. By the definition of dependent, $ S $ contains a vector $ vec{v}_1 $ that is a linear combination of the others. Define the set $ S_1=S - set{vec{v}_1} $ . By $ X $ {lm:ShrinkSpanByRemovingNonRepeat} the span does not shrink $ spanof{S_1}=spanof{S} $ . If $ S_1 $ is linearly independent then we are done. Otherwise iterate: take a vector $ vec{v}_2 $ that is a linear combination of other members of $ S_1 $ and discard it to derive $ S_2=S_1 - set{vec{v}_2} $ such that $ spanof{S_2}=spanof{S_1} $ . Repeat this until a linearly independent set $ S_j $ appears; one must appear eventually because $ S $ is finite and the empty set is linearly independent. (Formally, this argument uses induction on the number of elements in $ S $ . $ X $ asks for the details.) Thus if we have a set that is linearly dependent then we can, without changing the span, pare down by discarding what we have called ``repeat'' vectors. This set spans $ Re^3 $ (the check is routine) but is not linearly independent. $ S=set{colvec[r]{1  0  0}, colvec[r]{0  2  0}, colvec[r]{1  2  0}, colvec[r]{0  - 1  1}, colvec[r]{3  3  0} } $ . We will calculate which vectors to drop in order to get a subset that is independent but has the same span. This linear relationship $ c_1colvec[r]{1  0  0} +c_2colvec[r]{0  2  0} +c_3colvec[r]{1  2  0} +c_4colvec[r]{0  - 1  1} +c_5colvec[r]{3  3  0} =colvec[r]{0  0  0} qquadtag{ $ * $ } $ . gives a system $ {5} c_1 & & &+ &c_3 &+ & &+ &3c_5 &= &0  & &2c_2 &+ &2c_3 & - &c_4 &+ &3c_5 &= &0  & & & & & &c_4 & & &= &0 $ . whose solution set has this parametrization. $ set{colvec{c_1  c_2  c_3  c_4  c_5}= c_3colvec[r]{ - 1  - 1  1  0  0} +c_5colvec[r]{ - 3  - 3/2  0  0  1} suchthat c_3, c_5inRe } $ . Set $ c_5=1 $ and  $ c_3=0 $ to get an instance of ( $ * $ ). $ - 3cdotcolvec[r]{1  0  0} - frac{3}{2}cdotcolvec[r]{0  2  0} +0cdotcolvec[r]{1  2  0} +0cdotcolvec[r]{0  - 1  1} +1cdotcolvec[r]{3  3  0} =colvec[r]{0  0  0} $ . This shows that the vector from $ S $ that we've associated with  $ c_5 $ is in the span of the set of $ c_1 $ 's vector and $ c_2 $ 's vector. We can discard $ S $ 's fifth vector without shrinking the span. Similarly, set $ c_3=1 $ , and $ c_5=0 $ to get an instance of ( $ * $ ) that shows we can discard $ S $ 's third vector without shrinking the span. Thus this set has the same span as $ S $ . $ set{colvec[r]{1  0  0}, colvec[r]{0  2  0}, colvec[r]{0  - 1  1} } $ . The check that it is linearly independent is routine. A subset $ S=set{vec{s}_1, dots, vec{s}_n} $ of a vector space is linearly dependent if and only if some $ vec{s_i} $ is a linear combination of the vectors $ vec{s}_1 $ , ldots, $ vec{s}_{i - 1} $ listed before it. Consider $ S_0=set{} $ , $ S_1=set{vec{s_1}} $ , $ S_2=set{vec{s}_1, vec{s}_2 } $ , etc. Some index $ igeq 1 $ is the first one with $ S_{i - 1}unionset{vec{s}_i } $ linearly dependent, and there $ vec{s}_iinspanof{ S_{i - 1} } $ . The proof of $ X $ describes producing a linearly independent set by shrinking, by taking subsets. And the proof of $ X $ describes finding a linearly dependent set by taking supersets. We finish this subsection by considering how linear independence and dependence interact with the subset relation between sets. Any subset of a linearly independent set is also linearly independent. Any superset of a linearly dependent set is also linearly dependent. Both are clear. Restated, subset preserves independence and superset preserves dependence. Those are two of the four possible cases. The third case, whether subset preserves linear dependence, is covered by $ X $ , which gives a linearly dependent set $ S $ with one subset that is linearly dependent and another that is independent. The fourth case, whether superset preserves linear independence, is covered by $ X $ , which gives cases where a linearly independent set has both an independent and a dependent superset. This table summarizes. smallskip smallskip $ X $ has something else to say about the interaction between linear independence and superset. It names a linearly independent set that is maximal in that it has no supersets that are linearly independent. By $ X $ a linearly independent set is maximal if and only if it spans the entire space, because that is when all the vectors in the space are already in the span. This nicely complements $ X $ , that a spanning set is minimal if and only if it is linearly independent.
We've defined two matrices $ H $ and $ hat{H} $ to be matrix equivalent if there are nonsingular $ P $ and $ Q $ such that $ hat{H}=PHQ $ . We were motivated by this diagram showing $ H $ and $ hat{H} $ both representing a map $ h $ , but with respect to different pairs of bases, $ B, D $ and $ hat{B}, hat{D} $ . $ V_{wrt{B}} @>h>H> W_{wrt{D}}  @V{scriptstyleidentity} VV @V{scriptstyleidentity} VV  V_{wrt{hat{B}}} @>h>hat{H}> W_{wrt{hat{D}}} $ . We now consider the special case of transformations, where the codomain equals the domain, and we add the requirement that the codomain's basis equals the domain's basis. So, we are considering representations with respect to $ B, B $ and $ D, D $ . $ V_{wrt{B}} @>t>T> V_{wrt{B}}  @V{scriptstyleidentity} VV @V{scriptstyleidentity} VV  V_{wrt{D}} @>t>hat{T}> V_{wrt{D}} $ . In matrix terms, $ rep{t}{D, D} =rep{identity}{B, D};rep{t}{B, B};bigl(rep{identity}{B, D}bigr)^{ - 1} $ . Consider the derivative transformation $ map{d/dx}{polyspace_2}{polyspace_2} $ , and two bases for that space $ B=sequence{x^2, x, 1} $ and $ D=sequence{1, 1+x, 1+x^2} $ We will compute the four sides of the arrow square. $ {polyspace_2, }_{wrt{B}} @>d/dx>T> {polyspace_2, }_{wrt{B}}  @V{scriptstyleidentity} VV @V{scriptstyleidentity} VV  {polyspace_2, }_{wrt{D}} @>d/dx>hat{T}> {polyspace_2, }_{wrt{D}} $ . The top is first. The effect of the transformation on the starting basis  $ B $ $ x^2mapsunder{d/dx} 2x qquad xmapsunder{d/dx} 1 qquad 1mapsunder{d/dx} 0 $ . represented with respect to the ending basis (also  $ B $ ) $ rep{2x}{B}=colvec{0  2  0} qquad rep{1}{B}=colvec{0  0  1} qquad rep{0}{B}=colvec{0  0  0} $ . gives the representation of the map. $ T= rep{d/dx}{B, B}= 0 &0 &0  2 &0 &0  0 &1 &0 $ . Next, computing the matrix for the right - hand side involves finding the effect of the identity map on the elements of  $ B $ . Of course, the identity map does not transform them at all so to find the matrix we represent $ B $ 's elements with respect to  $ D $ . $ rep{x^2}{D}=colvec{ - 1  0  1} quad rep{x}{D}=colvec{ - 1  1  0} quad rep{1}{D}=colvec{1  0  0} $ . So the matrix for going down the right side is the concatenation of those. $ P=rep{id}{B, D}= - 1 & - 1 &1  0 &1 &0  1 &0 &0 $ . With that, we have two options to compute the matrix for going up on left side. The direct computation represents elements of  $ D $ with respect to  $ B $ $ rep{1}{B}=colvec{0  0  1} quad rep{1+x}{B}=colvec{0  1  1} quad rep{1+x^2}{B}=colvec{1  0  1} $ . and concatenates to make the matrix. $ 0 &0 &1  0 &1 &0  1 &1 &1 $ . The other option to compute the matrix for going up on the left is to take the inverse of the matrix  $ P $ for going down on the right. $ {ccc|ccc} - 1 & - 1 &1 &1 &0 &0  0 &1 &0 &0 &1 &0  1 &0 &0 &0 &0 &1 grstep{} cdots grstep{} {ccc|ccc} 1 &0 &0 &0 &0 &1  0 &1 &0 &0 &1 &0  0 &0 &1 &1 &1 &1 $ . That leaves the bottom of the square. There are two ways to compute the matrix  $ hat{T} $ . One is to compute it directly by finding the effect of the transformation on elements of  $ D $ $ 1mapsunder{d/dx} 0 qquad 1+xmapsunder{d/dx} 1 qquad 1+x^2mapsunder{d/dx} 2x $ . represented with respect to  $ D $ . $ hat{T}= rep{d/dx}{D, D}= 0 &1 & - 2  0 &0 &2  0 &0 &0 $ . The other way to compute  $ hat{T} $ , and this is the way we will usually do it, is to follow the diagram up, over, and then down. $ rep{d/dx}{D, D} &=rep{identity}{B, D}, rep{d/dx}{B, B}, rep{identity}{D, B}  hat{T} &=rep{identity}{B, D}, T, rep{identity}{D, B}  &= - 1 & - 1 &1  0 &1 &0  1 &0 &0 0 &0 &0  2 &0 &0  0 &1 &0 0 &0 &1  0 &1 &0  1 &1 &1 $ . Multiplying out gives the same matrix  $ hat{T} $ as we found above. The matrices $ T $ and $ hat{T} $ are similar if there is a nonsingular $ P $ such that $ hat{T}=PTP^{ - 1} $ . noindent Since nonsingular matrices are square, $ T $ and $ hat{T} $ must be square and of the same size. $ X $ checks that similarity is an equivalence relation. The definition does not require that we consider a map. Calculation with these two $ P= [r] 2 &1  1 &1 qquad T= [r] 2 & - 3  1 & - 1 $ . gives that $ T $ is similar to this matrix. $ hat{T}= [r] 12 & - 19  7 & - 11 $ . The only matrix similar to the zero matrix is itself:  $ PZP^{ - 1}=PZ=Z $ . The identity matrix has the same property:  $ PIP^{ - 1}=PP^{ - 1}=I $ . A common special case is where the vector space is $ C^n $ and the matrix  $ T $ represents a map with respect to the standard bases. $ C^n_{wrt{E_n}} @>t>T> C^n_{wrt{E_n}}  @V{scriptstyleidentity} VV @V{scriptstyleidentity} VV  C^n_{wrt{D}} @>t>hat{T}> C^n_{wrt{D}} $ . In this case in the similarity equation $ hat{T}=PTP^{ - 1} $ , the columns of  $ P $ are the elements of  $ D $ . Matrix similarity is a special case of matrix equivalence so if two matrices are similar then they are matrix equivalent. What about the converse: if they are square, must any two matrix equivalent matrices be similar? No; the matrix equivalence class of an identity matrix consists of all nonsingular matrices of that size while the prior example shows that the only member of the similarity class of an identity matrix is itself. Thus these two are matrix equivalent but not similar. $ T= [r] 1 &0  0 &1 qquad S= [r] 1 &2  0 &3 $ . So some matrix equivalence classes split into two or more similarity classesDash similarity gives a finer partition than does matrix equivalence. This shows some matrix equivalence classes subdivided into similarity classes. To understand the similarity relation we shall study the similarity classes. We approach this question in the same way that we've studied both the row equivalence and matrix equivalence relations, by finding a canonical form for representatives appendrefs{representatives}spacefactor=1000 of the similarity classes, called Jordan form. With this canonical form, we can decide if two matrices are similar by checking whether they are in a class with the same representative. We've also seen with both row equivalence and matrix equivalence that a canonical form gives us insight into the ways in which members of the same class are alike (e.g., two identically - sized matrices are matrix equivalent if and only if they have the same rank). The prior subsection shows that although similar matrices are necessarily matrix equivalent, the converse does not hold. Some matrix equivalence classes break into two or more similarity classes; for instance, the nonsingular $ nbyn{2} $ matrices form one matrix equivalence class but more than one similarity class. The diagram below illustrates. Solid curves show the matrix equivalence classes while dashed dividers mark the similarity classes. Each star is a matrix representing its similarity class. We cannot use the canonical form for matrix equivalence, a block partial - identity matrix, as a canonical form for similarity because each matrix equivalence class has only one partial identity matrix. To develop a canonical form for representatives of the similarity classes we naturally build on previous work. So, if a similarity class does contain a partial identity matrix then it should represent that class. Beyond that, representatives should be as simple as possible. The simplest extension of the partial identity form is diagonal form. A transformation is diagonalizable if it has a diagonal representation with respect to the same basis for the codomain as for the domain. A diagonalizable matrix is one that is similar to a diagonal matrix:  $ T $ is diagonalizable if there is a nonsingular $ P $ such that $ PTP^{ - 1} $ is diagonal. The matrix $ [r] 4 & - 2  1 &1 $ . is diagonalizable. $ [r] 2 &0  0 &3 = [r] - 1 &2  1 & - 1 [r] 4 & - 2  1 &1 [r] - 1 &2  1 & - 1 ^{ - 1} $ . Below we will see how to find the matrix  $ P $ but first we note that not every matrix is similar to a diagonal matrix, so diagonal form will not suffice as a canonical form for similarity. This matrix is not diagonalizable. $ N=[r] 0 &0  1 &0 $ . The fact that $ N $ is not the zero matrix means that it cannot be similar to the zero matrix, because the zero matrix is similar only to itself. Thus if $ N $ were to be similar to a diagonal matrix  $ D $ then $ D $ would have at least one nonzero entry on its diagonal. The crucial point is that a power of $ N $ is the zero matrix, specifically $ N^2 $ is the zero matrix. This implies that for any map  $ n $ represented by $ N $ with respect to some $ B, B $ , the composition $ composed{n}{n} $ is the zero map. This in turn implies that any matrix representing  $ n $ with respect to some $ hat{B}, hat{B} $ has a square that is the zero matrix. But for any nonzero diagonal matrix  $ D^2 $ , the entries of  $ D^2 $ are the squares of the entries of  $ D $ , so $ D^2 $ cannot be the zero matrix. Thus $ N $ is not diagonalizable. So not every similarity class contains a diagonal matrix. We now characterize when a matrix is diagonalizable. A transformation $ t $ is diagonalizable if and only if there is a basis $ B=sequence{vec{beta}_1, ldots, vec{beta}_n } $ and scalars $ lambda_1, ldots, lambda_n $ such that $ t(vec{beta}_i)=lambda_ivec{beta}_i $ for each $ i $ . Consider a diagonal representation matrix. $ rep{t}{B, B}= {c@{hspace*{1em}}c@{hspace*{1em}}c} vdots & &vdots  rep{t(vec{beta}_1)}{B} &cdots &rep{t(vec{beta}_n)}{B}  vdots & &vdots = {c@{hspace*{1em}}c@{hspace*{1em}}c} lambda_1 & &0  vdots &ddots &vdots  0 & &lambda_n $ . Consider the representation of a member of this basis with respect to the basis $ rep{vec{beta}_i}{B} $ . The product of the diagonal matrix and the representation vector $ rep{t(vec{beta}_i)}{B} ={c@{hspace*{1em}}c@{hspace*{1em}}c} lambda_1 & &0  vdots &ddots &vdots  0 & &lambda_n colvec{0  vdots  1  vdots  0} =colvec{0  vdots  lambda_i  vdots  0} $ . has the stated action. To diagonalize $ T=[r] 3 &2  0 &1 $ . we take $ T $ as the representation of a transformation with respect to the standard basis $ rep{t}{stdbasis_2, stdbasis_2} $ and look for a basis $ B=sequence{vec{beta}_1, vec{beta}_2} $ such that $ rep{t}{B, B} = lambda_1 &0  0 &lambda_2 $ . that is, such that $ t(vec{beta}_1)=lambda_1vec{beta}_1 $ and $ t(vec{beta}_2)=lambda_2vec{beta}_2 $ . $ [r] 3 &2  0 &1 vec{beta}_1=lambda_1cdotvec{beta}_1 qquad [r] 3 &2  0 &1 vec{beta}_2=lambda_2cdotvec{beta}_2 $ . We are looking for scalars $ x $ such that this equation $ [r] 3 &2  0 &1 colvec{b_1  b_2}=xcdotcolvec{b_1  b_2} $ . has solutions $ b_1 $ and $ b_2 $ that are not both $ 0 $ (the zero vector is not the member of any basis). That's a linear system. $ {2} (3 - x)cdot b_1 &+ &2cdot b_2 &= &0  & &(1 - x)cdot b_2 &= &0 tag*{( $ * $ )} $ . Focus first on the bottom equation. There are two cases: either $ b_2=0 $ or  $ x=1 $ . In the $ b_2=0 $ case the first equation gives that either $ b_1=0 $ or $ x=3 $ . Since we've disallowed the possibility that both $ b_2=0 $ and  $ b_1=0 $ , we are left with the first diagonal entry $ lambda_1=3 $ . With that, ( $ * $ )'s first equation is $ 0cdot b_1+2cdot b_2=0 $ and so associated with $ lambda_1=3 $ are vectors having a second component of zero while the first component is free. $ [r] 3 &2  0 &1 colvec{b_1  0}=3cdotcolvec{b_1  0} $ . To get a first basis vector choose any nonzero $ b_1 $ . $ vec{beta}_1=colvec[r]{1  0} $ . The other case for the bottom equation of ( $ * $ ) is $ lambda_2=1 $ . Then ( $ * $ )'s first equation is $ 2cdot b_1+2cdot b_2=0 $ and so associated with this case are vectors whose second component is the negative of the first. $ [r] 3 &2  0 &1 colvec{b_1  - b_1}=1cdotcolvec{b_1  - b_1} $ . Get the second basis vector by choosing a nonzero one of these. $ vec{beta}_2=colvec[r]{1  - 1} $ . Now draw the similarity diagram (recall that we are working with scalars that are complex, not real), $ C^2_{wrt{stdbasis_2}} @>t>T> C^2_{wrt{stdbasis_2}}  @V{scriptstyleidentity} VV @V{scriptstyleidentity} VV  C^2_{wrt{B}} @>t>D> C^2_{wrt{B}} $ . and note that the matrix $ rep{identity}{B, stdbasis_2} $ is easy, giving this diagonalization. $ [r] 3 &0  0 &1 = [r] 1 &1  0 & - 1 ^{ - 1} [r] 3 &2  0 &1 [r] 1 &1  0 & - 1 $ . The rest of this section expands on that example by considering more closely the property of $ X $ , including seeing a streamlined way to find the $ lambda $ 's. The section after that expands on $ X $ , to understand what can prevent diagonalization. Then the final section puts these two together, to produce a canonical form that is in some sense as simple as possible. We will next focus on the property of $ X $ . A transformation $ map{t}{V}{V} $ has a scalar eigenvalue $ lambda $ if there is a nonzero eigenvector $ vec{zeta}in V $ such that $ t(vec{zeta})=lambdacdotvec{zeta} $ . noindent ``Eigen'' is German for ``characteristic of'' or ``peculiar to.'' Some authors call these characteristic values and vectors. No authors call them ``peculiar'' vectors. The projection map $ colvec{x  y  z} mapsunder{pi} colvec{x  y  0} qquad x, y, zinC $ . has an eigenvalue of $ 1 $ associated with any eigenvector $ colvec{x  y  0} $ . where $ x $ and $ y $ are scalars that are not both zero. In contrast, a number that is not an eigenvalue of this map is $ 2 $ , since assuming that $ pi $ doubles a vector leads to the three equations $ x=2x $ , $ y=2y $ , and $ 0=2z $ , and thus no non - $ zero $ vector is doubled. Note that the definition requires that the eigenvector be non - $ zero $ . Some authors allow $ zero $ as an eigenvector for $ lambda $ as long as there are also non - $ zero $ vectors associated with $ lambda $ . The key point is to disallow the trivial case where $ lambda $ is such that $ t(vec{v})=lambdavec{v} $ for only the single vector $ vec{v}=zero $ . Also, note that the eigenvalue $ lambda $ could be  $ 0 $ . The issue is whether $ vec{zeta} $ equals $ zero $ . The only transformation on the trivial space $ set{zero} $ is $ zeromapstozero $ . This map has no eigenvalues because there are no non - $ zero $ vectors $ vec{v} $ mapped to a scalar multiple $ lambdacdotvec{v} $ of themselves. Consider the homomorphism $ map{t}{polyspace_1}{polyspace_1} $ given by $ c_0+c_1xmapsto(c_0+c_1)+(c_0+c_1)x $ . While the codomain $ polyspace_1 $ of $ t $ is two - dimensional, its range is one - dimensional $ rangespace{t}=set{c+cxsuchthat cinC} $ . Application of $ t $ to a vector in that range will simply rescale the vector $ c+cxmapsto (2c)+(2c)x $ . That is, $ t $ has an eigenvalue of $ 2 $ associated with eigenvectors of the form $ c+cx $ , where $ cneq 0 $ . This map also has an eigenvalue of $ 0 $ associated with eigenvectors of the form $ c - cx $ where $ cneq 0 $ . The definition above is for maps. We can give a matrix version. A square matrix $ T $ has a scalar eigenvalue $ lambda $ associated with the nonzero eigenvector $ vec{zeta} $ if $ Tvec{zeta}=lambdacdotvec{zeta} $ . This extension of the definition for maps to a definition for matrices is natural but there is a point on which we must take care. The eigenvalues of a map are also the eigenvalues of matrices representing that map, and so similar matrices have the same eigenvalues. However, the eigenvectors can differDash similar matrices need not have the same eigenvectors. The next example explains. These matrices are similar $ T= 2 &0  0 &0 quad hat{T} = [r] 4 & - 2  4 & - 2 $ . since $ hat{T}=PTP^{ - 1} $ for this $ P $ . $ P= 1 &1  1 &2 quad P^{ - 1}= [r] 2 & - 1  - 1 &1 $ . The matrix $ T $ has two eigenvalues, $ lambda_1=2 $ and  $ lambda_2=0 $ . The first one is associated with this eigenvector. $ Tvec{e}_1= 2 &0  0 &0 colvec{1  0} =colvec{2  0} =2vec{e}_1 $ . Suppose that $ T $ represents a transformation $ map{t}{C^2}{C^2} $ with respect to the standard basis. Then the action of this transformation $ t $ is simple. $ colvec{x  y}mapsunder{t}colvec{2x  0} $ . Of course, $ hat{T} $ represents the same transformation but with respect to a different basis  $ B $ . We can find this basis. Following the arrow diagram from the lower left to the upper left $ V_{wrt{stdbasis_2}} @>t>T> V_{wrt{stdbasis_2}}  @V{scriptstyleidentity} VV @V{scriptstyleidentity} VV  V_{wrt{B}} @>t>hat{T}> V_{wrt{B}} $ . shows that $ P^{ - 1}=rep{identity}{B, stdbasis_2} $ . By the definition of the matrix representation of a map, its first column is $ rep{identity(vec{beta}_1)}{stdbasis_2}=rep{vec{beta}_1}{stdbasis_2} $ . With respect to the standard basis any vector is represented by itself, so the first basis element $ vec{beta}_1 $ is the first column of $ P^{ - 1} $ . The same goes for the other one. $ B=sequence{colvec[r]{2  - 1}, colvec[r]{ - 1  1} } $ . Since the matrices $ T $ and  $ hat{T} $ both represent the transformation  $ t $ , both reflect the action $ t(vec{e}_1)=2vec{e}_1 $ . $ &rep{t}{stdbasis_2, stdbasis_2}cdotrep{vec{e}_1}{stdbasis_2} =Tcdotrep{vec{e}_1}{stdbasis_2} =2cdotrep{vec{e}_1}{stdbasis_2}  &rep{t}{B, B}cdotrep{vec{e}_1}{B} =hat{T}cdotrep{vec{e}_1}{B} =2cdotrep{vec{e}_1}{B} $ . But while in those two equations the eigenvalue $ 2 $ 's are the same, the vector representations differ. $ Tcdotrep{vec{e}_1}{stdbasis_2} =Tcolvec{1  0} &=2cdotcolvec{1  0}  hat{T}cdotrep{vec{e}_1}{B} =hat{T}cdotcolvec{1  1} &=2cdotcolvec{1  1} $ . That is, when the matrix representing the transformation is $ T=rep{t}{stdbasis_2, stdbasis_2} $ then it ``assumes'' that column vectors are representations with respect to $ stdbasis_2 $ . However $ hat{T}=rep{t}{B, B} $ ``assumes'' that column vectors are representations with respect to $ B $ and so the column vectors that get doubled are different. We next see the basic tool for finding eigenvectors and eigenvalues. If $ T= [r] 1 &2 &1  2 &0 & - 2  - 1 &2 &3 $ . then to find the scalars $ x $ such that $ Tvec{zeta}=xvec{zeta} $ for nonzero eigenvectors $ vec{zeta} $ , bring everything to the left - hand side $ [r] 1 &2 &1  2 &0 & - 2  - 1 &2 &3 colvec{z_1  z_2  z_3} - xcolvec{z_1  z_2  z_3} =zero $ . and factor $ (T - x I)vec{zeta}=zero $ . (Note that it says $ T - xI $ . The expression $ T - x $ doesn't make sense because $ T $ is a matrix while $ x $ is a scalar.) This homogeneous linear system $ 1 - x &2 &1  2 &0 - x & - 2  - 1 &2 &3 - x colvec{z_1  z_2  z_3} = colvec[r]{0  0  0} $ . has a nonzero solution $ vec{z} $ if and only if the matrix is singular. We can determine when that happens. $ 0 &=deter{T - x I}  &= 1 - x &2 &1  2 &0 - x & - 2  - 1 &2 &3 - x  &=x^3 - 4x^2+4x  &=x(x - 2)^2 $ . The eigenvalues are $ lambda_1=0 $ and $ lambda_2=2 $ . To find the associated eigenvectors plug in each eigenvalue. Plugging in $ lambda_1=0 $ gives $ 1 - 0 &2 &1  2 &0 - 0 & - 2  - 1 &2 &3 - 0 colvec{z_1  z_2  z_3} = colvec[r]{0  0  0} quadLongrightarrowquad colvec{z_1  z_2  z_3} = colvec[r]{a  - a  a} $ . for $ aneq 0 $ ( $ a $ must be non - $ 0 $ because eigenvectors are defined to be non - $ zero $ ). Plugging in $ lambda_2=2 $ gives $ 1 - 2 &2 &1  2 &0 - 2 & - 2  - 1 &2 &3 - 2 colvec{z_1  z_2  z_3} = colvec[r]{0  0  0} quadLongrightarrowquad colvec{z_1  z_2  z_3} = colvec{b  0  b} $ . with $ bneq 0 $ . If $ S= [r] pi &1  0 &3 $ . (here $ pi $ is not a projection map, it is the number $ (iii)14ldots $ ) then $ pi - x &1  0 &3 - x = (x - pi)(x - 3) $ . so $ S $ has eigenvalues of $ lambda_1=pi $ and $ lambda_2=3 $ . To find associated eigenvectors, first plug in $ lambda_1 $ for $ x $ $ pi - pi &1  0 &3 - pi colvec{z_1  z_2} = colvec[r]{0  0} qquadLongrightarrowqquad colvec{z_1  z_2} = colvec{a  0} $ . for a scalar $ aneq 0 $ . Then plug in $ lambda_2 $ $ pi - 3 &1  0 &3 - 3 colvec{z_1  z_2} = colvec[r]{0  0} qquadLongrightarrowqquad colvec{z_1  z_2} = colvec{ - b/(pi - 3)  b} $ . where $ bneq 0 $ . The characteristic polynomial of a square matrix $ T $ is the determinant $ deter{T - x I} $ , where $ x $ is a variable. The characteristic equation is $ deter{T - xI}=0 $ . The characteristic polynomial of a transformation $ t $ is the characteristic polynomial of any matrix representation $ rep{t}{B, B} $ . noindent The characteristic polynomial of an $ nbyn{n} $ matrix, or of a transformation $ map{t}{C^n}{C^n} $ , is of degree  $ n $ . $ X $ checks that the characteristic polynomial of a transformation is well - defined, that is, that the characteristic polynomial is the same no matter which basis we use for the representation. A linear transformation on a nontrivial vector space has at least one eigenvalue. Any root of the characteristic polynomial is an eigenvalue. Over the complex numbers, any polynomial of degree one or greater has a root. That result is the reason that in this chapter we use scalars that are complex numbers. Had we stuck to real number scalars then there would be characteristic polynomials, such as $ x^2+1 $ , that do not factor. The eigenspace of a transformation  $ t $ associated with the eigenvalue  $ lambda $ is $ V_{lambda}=set{vec{zeta}suchthat t(vec{zeta}, )=lambdavec{zeta}, } $ . The eigenspace of a matrix is analogous. An eigenspace is a subspace. It is a nontrivial subspace. Notice first that $ V_{lambda} $ is not empty; it contains the zero vector since $ t(zero)=zero $ , which equals $ lambdacdot zero $ . To show that an eigenspace is a subspace, what remains is to check closure of this set under linear combinations. Take $ vec{zeta}_1, ldots, vec{zeta}_nin V_{lambda} $ and then $ t(lincombo{c}{vec{zeta}}) &=c_1t(vec{zeta}_1)+dots+c_nt(vec{zeta}_n)  &=c_1lambdavec{zeta}_1+dots+c_nlambdavec{zeta}_n  &=lambda(c_1vec{zeta}_1+dots+c_nvec{zeta}_n) $ . that the combination is also an element of $ V_{lambda} $ . The space  $ V_lambda $ contains more than just the zero vector because by definition $ lambda $ is an eigenvalue only if $ t(vec{zeta}, )=lambdavec{zeta} $ has solutions for $ vec{zeta} $ other than $ zero $ . These are the eigenspaces associated with the eigenvalues $ 0 $ and $ 2 $ of $ X $ . $ V_0=set{colvec[r]{a  - a  a}suchthat ainC}, qquad V_2=set{colvec{b  0  b}suchthat binC}. $ . These are the eigenspaces for the eigenvalues $ pi $ and  $ 3 $ of $ X $ . $ V_{pi}=set{colvec{a  0}suchthat ainC} qquad V_3=set{colvec{ - b/(pi - 3)  b}suchthat binC} $ . The characteristic equation in $ X $ is $ 0=x(x - 2)^2 $ so in some sense $ 2 $ is an eigenvalue twice. However there are not twice as many eigenvectors in that the dimension of the associated eigenspace $ V_2 $ is one, not two. The next example is a case where a number is a double root of the characteristic equation and the dimension of the associated eigenspace is two. With respect to the standard bases, this matrix $ [r] 1 &0 &0  0 &1 &0  0 &0 &0 $ . represents projection. $ colvec{x  y  z} mapsunder{pi} colvec{x  y  0} qquad x, y, zinC $ . Its characteristic equation $ 0 &=deter{T - x I}  &= 1 - x &0 &0  0 &1 - x &0  0 &0 &0 - x  &=(1 - x)^2(0 - x) $ . has the double root  $ x=1 $ along with the single root  $ x=0 $ . Its eigenspace associated with the eigenvalue $ 1 $ and its eigenspace associated with the eigenvalue $ 0 $ are easy to find. $ V_1=set{colvec{c_1  c_2  0}suchthat c_1, c_2inC} qquad V_0=set{colvec{0  0  c_3}suchthat c_3inC} $ . Note that $ V_1 $ has dimension two. Where a characteristic polynomial factors into $ (x - lambda_1)^{m_1}cdots (x - lambda_k)^{m_k} $ then the eigenvalue $ lambda_i $ has algebraic multiplicity  $ m_i $ . Its geometric multiplicity is the dimension of the associated eigenspace  $ V_{lambda_i} $ . In $ X $ , there are two eigenvalues, For $ lambda_1=1 $ both the algebraic and geometric multiplicities are  $ 2 $ . For $ lambda_2=0 $ both the algebraic and geometric multiplicities are  $ 1 $ . In contrast, $ X $ shows that the eigenvalue $ lambda=2 $ has algebraic multiplicity  $ 2 $ but geometric multiplicity  $ 1 $ . For every transformation, each eigenvalue has geometric multiplicity greater than or equal to  $ 1 $ by $ X $ . (And, an eigenvalue must have geometric multiplicity less than or equal to its algebraic multiplicity, although proving this is beyond our scope.) By $ X $ if two eigenvectors $ vec{v}_1 $ and $ vec{v}_2 $ are associated with the same eigenvalue then a linear combination of those two is also an eigenvector, associated with the same eigenvalue. As an illustration, referring to the prior example, this sum of two members of $ V_1 $ $ colvec[r]{1  0  0}+colvec[r]{0  1  0} $ . yields another member of $ V_1 $ . The next result speaks to the situation where the vectors come from different eigenspaces. For any set of distinct eigenvalues of a map or matrix, a set of associated eigenvectors, one per eigenvalue, is linearly independent. We will use induction on the number of eigenvalues. The base step is that there are zero eigenvalues. Then the set of associated vectors is empty and so is linearly independent. For the inductive step assume that the statement is true for any set of $ kgeq 0 $ distinct eigenvalues. Consider distinct eigenvalues $ lambda_1, dots, lambda_{k+1} $ and let $ vec{v}_1, dots, vec{v}_{k+1} $ be associated eigenvectors. Suppose that $ zero=c_1vec{v}_1+dots+c_kvec{v}_k+c_{k+1}vec{v}_{k+1} $ . Derive two equations from that, the first by multiplying by $ lambda_{k+1} $ on both sides $ zero=c_1lambda_{k+1}vec{v}_1+dots+c_{k+1}lambda_{k+1}vec{v}_{k+1} $ and the second by applying the map to both sides $ zero=c_1t(vec{v}_1)+dots+c_{k+1}t(vec{v}_{k+1}) =c_1lambda_1vec{v}_1+dots+c_{k+1}lambda_{k+1}vec{v}_{k+1} $ (applying the matrix gives the same result). Subtract the second from the first. $ zero= c_1(lambda_{k+1} - lambda_1)vec{v}_1+dots +c_k(lambda_{k+1} - lambda_k)vec{v}_k +c_{k+1}(lambda_{k+1} - lambda_{k+1})vec{v}_{k+1} $ . The $ vec{v}_{k+1} $ term vanishes. Then the induction hypothesis gives that $ c_1(lambda_{k+1} - lambda_1)=0 $ , ldots, $ c_k(lambda_{k+1} - lambda_k)=0 $ . The eigenvalues are distinct so the coefficients $ c_1, , dots, , c_k $ are all  $ 0 $ . With that we are left with the equation $ zero=c_{k+1}vec{v}_{k+1} $ so $ c_{k+1} $ is also  $ 0 $ . The eigenvalues of $ [r] 2 & - 2 &2  0 &1 &1  - 4 &8 &3 $ . are distinct: $ lambda_1=1 $ , $ lambda_2=2 $ , and  $ lambda_3=3 $ . A set of associated eigenvectors $ set{ colvec[r]{2  1  0}, colvec[r]{9  4  4}, colvec[r]{2  1  2} } $ . is linearly independent. An $ nbyn{n} $ matrix with $ n $ distinct eigenvalues is diagonalizable. Form a basis of eigenvectors. Apply $ X $ . In the prior example we showed that the matrix $ T=[r] 2 & - 2 &2  0 &1 &1  - 4 &8 &3 $ . is diagonalizable with eigenvalues $ lambda_1=1 $ , $ lambda_2=2 $ , and  $ lambda_3=3 $ . These are associated eigenvectors, which make up a basis  $ B $ . $ vec{beta}_1=colvec[r]{2  1  0}quad vec{beta}_2=colvec[r]{9  4  4}quad vec{beta}_3=colvec[r]{2  1  2} $ . The arrow diagram $ V_{wrt{stdbasis_3}} @>t>T> V_{wrt{stdbasis_3}}  @V{scriptstyleidentity} VV @V{scriptstyleidentity} VV  V_{wrt{B}} @>t>D> V_{wrt{B}} $ . gives this, where $ P=rep{identity}{stdbasis_3, B} $ . $ D &= PTP^{ - 1}  1 &0 &0  0 &2 &0  0 &0 &3 &= - 2 &5 & - 1/2  1 & - 2 &0  - 2 &4 &1/2 [r] 2 & - 2 &2  0 &1 &1  - 4 &8 &3 2 &9 &2  1 &4 &1  0 &4 &2 $ . The bottom of the diagram has $ t(vec{beta}_1)=1cdotvec{beta}_1 $ , along with $ t(vec{beta}_2)=2cdotvec{beta}_2 $ , and $ t(vec{beta}_3)=3cdotvec{beta}_3 $ . Rewriting gives that the map $ t - 1 $ sends $ vec{beta}_1 $ to  $ zero $ , that $ t - 2 $ sends $ vec{beta}_2 $ to  $ zero $ , and that $ (t - 3)(vec{beta}_3)=zero $ . That is, the action on $ B $ is this. $ vec{beta}_1mapsunder{t - 1}zero qquad vec{beta}_2mapsunder{t - 2}zero qquad vec{beta}_3mapsunder{t - 3}zero $ . Turning to the representations in that arrow diagram, the top line says that the matrix $ T - (1cdot I) $ , which represents $ t - 1 $ with respect to $ stdbasis_3, stdbasis_3 $ , multiplied by the representation of $ vec{beta}_1 $ with respect to  $ stdbasis_3 $ , will give zero. $ 1 & - 2 &2  0 &0 &1  - 4 &8 &2  colvec{2  1  0} = colvec{0  0  0} $ . Similarly, the matrix $ T - (2cdot I) $ , representing $ t - 2 $ with respect to $ stdbasis_3, stdbasis_3 $ , when multiplied by the representation of $ vec{beta}_2 $ with respect to  $ stdbasis_3 $ , gives zero. $ 0 & - 2 &2  0 &01 &1  - 4 &8 &1  colvec{9  4  4} = colvec{0  0  0} $ . And of course, the matrix $ T - (3cdot I) $ times the representation of $ vec{beta}_3 $ with respect to  $ stdbasis_3 $ also gives zero. In summary, this section observes that some matrices are similar to a diagonal matrix. The idea of eigenvalues arose as the entries of that diagonal matrix, although the definition applies more broadly than just to diagonalizable matrices. To find eigenvalues we defined the characteristic equation and that led to the final result, a criterion for diagonalizability. (While it is useful for the theory, note that in applications finding eigenvalues this way is typically impractical; for one thing the matrix may be large and finding roots of large - degree polynomials is hard.) In the next section we study matrices that cannot be diagonalized.
In the examples after the definition of a vector space we expressed the intuition that some spaces are essentially the same as others. For instance, we may perceive that the space of two - by - two matrices and the space of four tall vectors $ matspace_{nbyn{2}}=set{ a &b  c &d suchthat a, b, c, dinR} qquad R^4=set{colvec{a  b  c  d}suchthat a, b, c, dinR} $ . are not equal because their elements are not the same, but that they differ only cosmetically. We will now make this precise. This illustrates a common phase of a mathematical investigation. With the help of some examples we've gotten an idea. We will next give a formal definition and then produce some results backing our contention that the definition captures the idea. We've seen this happen already, for instance in the first section of the Vector Space chapter. There, the study of linear systems led us to consider collections closed under linear combinations. We defined such a collection as a vector space and we followed it with some supporting results. That wasn't an end point, instead it led to new insights such as the idea of a basis. Here also, after producing a definition and supporting it, we will get two pleasant surprises. First, we will find that the definition applies to some unforeseen, and interesting, cases. Second, the study of the definition will lead to new ideas. In this way, our investigation will build momentum. inition and Examples} We start with two examples that suggest the right definition. The space of two - wide row vectors and the space of two - tall column vectors are ``the same'' in that if we associate the vectors that have the same components, e.g., $ rowvec{1 &2} quadlongleftrightarrowquad colvec[r]{1  2} $ . (read the double arrow as ``corresponds to'') then this association respects the operations. For instance these corresponding vectors add to corresponding totals $ rowvec{1 &2}+rowvec{3 &4}=rowvec{4 &6} quadlongleftrightarrowquad colvec[r]{1  2}+colvec[r]{3  4}=colvec[r]{4  6} $ . and here is an example of the correspondence respecting scalar multiplication. $ 5cdotrowvec{1 &2}=rowvec{5 &10} quadlongleftrightarrowquad 5cdotcolvec[r]{1  2}=colvec[r]{5  10} $ . Stated generally, under the correspondence $ rowvec{a_0 &a_1} quadlongleftrightarrowquad colvec{a_0  a_1} $ . both operations are preserved: $ rowvec{a_0 &a_1}+rowvec{b_0 &b_1}=rowvec{a_0+b_0 &a_1+b_1} hfilllongleftrightarrowhfill colvec{a_0  a_1}+colvec{b_0  b_1}=colvec{a_0+b_0  a_1+b_1} $ . and $ rcdotrowvec{a_0 &a_1}=rowvec{ra_0 &ra_1} quadlongleftrightarrowquad rcdotcolvec{a_0  a_1}=colvec{ra_0  ra_1} $ . (all of the variables are scalars). Another two spaces that we can think of as ``the same'' are $ polyspace_2 $ , the space of quadratic polynomials, and $ Re^3 $ . A natural correspondence is this. $ a_0+a_1x+a_2x^2 quadlongleftrightarrowquad colvec{a_0  a_1  a_2} tag*{mbox{(e.g., $ 1+2x+3x^2, longleftrightarrow, colvec[r]{1  2  3} $ )}} $ . This preserves structure: corresponding elements add in a corresponding way $ mbox{{r} $ a_0+a_1x+a_2x^2 $  +, , $ b_0+b_1x+b_2x^2 $  hline $ (a_0+b_0)+(a_1+b_1)x+(a_2+b_2)x^2 $ } longleftrightarrow colvec{a_0  a_1  a_2} +colvec{b_0  b_1  b_2} =colvec{a_0+b_0  a_1+b_1  a_2+b_2} $ . and scalar multiplication corresponds also. $ rcdot(a_0+a_1x+a_2x^2)= (ra_0)+(ra_1)x+(ra_2)x^2 quadlongleftrightarrowquad rcdotcolvec{a_0  a_1  a_2} =colvec{ra_0  ra_1  ra_2} $ . An isomorphism between two vector spaces $ V $ and $ W $ is a map $ map{f}{V}{W} $ that is a correspondence: $ f $ is one - to - one and onto; footnotemark keep fn in minipage appendrefs{correspondences} preserves structure: if $ vec{v}_1, vec{v}_2in V $ then $ f(vec{v}_1+vec{v}_2)=f(vec{v}_1)+f(vec{v}_2) $ . and if $ vec{v}in V $ and $ rinRe $ then $ f(rvec{v})=r{}f(vec{v}) suppress kerning $ . (we write $ Visomorphicto W $ , read `` $ V $ is isomorphic to $ W $ '', when such a map exists). footnotetext{More information on one - to - one and onto maps noindent ``Morphism'' means map, so ``isomorphism'' means a map expressing sameness. The vector space $ G=set{c_1costheta+c_2sinthetasuchthat c_1, c_2inRe} $ of functions of $ theta $ is isomorphic to $ Re^2 $ under this map. $ c_1costheta+c_2sinthetamapsunder{f}colvec{c_1  c_2} $ . We will check this by going through the conditions in the definition. We will first verify condition (1), that the map is a correspondence between the sets underlying the spaces. To establish that $ f $ is one - to - one we must prove that $ f(vec{a})=f(vec{b}) $ only when $ vec{a}=vec{b} $ . If $ f(a_1costheta+a_2sintheta)=f(b_1costheta+b_2sintheta) $ . then by the definition of $ f $ $ colvec{a_1  a_2}=colvec{b_1  b_2} $ . from which we conclude that $ a_1=b_1 $ and $ a_2=b_2 $ , because column vectors are equal only when they have equal components. Thus $ a_1costheta+a_2sintheta=b_1costheta+b_2sintheta $ , and as required we've verified that $ f(vec{a})=f(vec{b}) $ implies that $ vec{a}=vec{b} $ . To prove that $ f $ is onto we must check that any member of the codomain $ Re^2 $ is the image of some member of the domain $ G $ . So, consider a member of the codomain $ colvec{x  y} $ . and note that it is the image under $ f $ of $ xcostheta+ysintheta $ . Next we will verify condition (2), that $ f $ preserves structure. This computation shows that $ f $ preserves addition. fbigl(, (a_1costheta+a_2sintheta) +(b_1costheta+b_2sintheta), bigr)  &=fbigl(, (a_1+b_1)costheta+(a_2+b_2)sintheta, bigr)  &=colvec{a_1+b_1  a_2+b_2}  &=colvec{a_1  a_2}+colvec{b_1  b_2}  &=f(a_1costheta+a_2sintheta)+f(b_1costheta+b_2sintheta) The computation showing that $ f $ preserves scalar multiplication is similar. $ fbigl(, rcdot(a_1costheta+a_2sintheta), bigr) &=f(, ra_1costheta+ra_2sintheta, )  &=colvec{ra_1  ra_2}  &=rcdotcolvec{a_1  a_2}  &=rcdot, f(a_1costheta+a_2sintheta) $ . With both (1) and (2) verified, we know that $ f $ is an isomorphism and we can say that the spaces are isomorphic $ GisomorphictoRe^2 $ . Let $ V $ be the space $ set{c_1x+c_2y+c_3zsuchthat c_1, c_2, c_3inRe} $ of linear combinations of the three variables under the natural addition and scalar multiplication operations. Then $ V $ is isomorphic to $ polyspace_2 $ , the space of quadratic polynomials. To show this we must produce an isomorphism map. There is more than one possibility; for instance, here are four to choose among. The first map is the more natural correspondence in that it just carries the coefficients over. However we shall do $ f_2 $ to underline that there are isomorphisms other than the obvious one. (Checking that $ f_1 $ is an isomorphism is $ X $ .) To show that $ f_2 $ is one - to - one we will prove that if $ f_2(c_1x+c_2y+c_3z)=f_2(d_1x+d_2y+d_3z) $ then $ c_1x+c_2y+c_3z=d_1x+d_2y+d_3z $ . The assumption that $ f_2(c_1x+c_2y+c_3z)=f_2(d_1x+d_2y+d_3z) $ gives, by the definition of $ f_2 $ , that $ c_2+c_3x+c_1x^2=d_2+d_3x+d_1x^2 $ . Equal polynomials have equal coefficients so $ c_2=d_2 $ , $ c_3=d_3 $ , and $ c_1=d_1 $ . Hence $ f_2(c_1x+c_2y+c_3z)=f_2(d_1x+d_2y+d_3z) $ implies that $ c_1x+c_2y+c_3z=d_1x+d_2y+d_3z $ , and $ f_2 $ is one - to - one. The map $ f_2 $ is onto because a member $ a+bx+cx^2 $ of the codomain is the image of a member of the domain, namely it is $ f_2(cx+ay+bz) $ . For instance, $ 2+3x - 4x^2 $ is $ f_2( - 4x+2y+3z) $ . The computations for structure preservation are like those in the prior example. The map $ f_2 $ preserves addition f_2bigl((c_1x+c_2y+c_3z) +(d_1x+d_2y+d_3z)bigr)  &=f_2bigl((c_1+d_1)x+(c_2+d_2)y+(c_3+d_3)zbigr)  &=(c_2+d_2)+(c_3+d_3)x+(c_1+d_1)x^2  &=(c_2+c_3x+c_1x^2)+(d_2+d_3x+d_1x^2)  &=f_2(c_1x+c_2y+c_3z)+f_2(d_1x+d_2y+d_3z) and scalar multiplication. $ f_2bigl(rcdot(c_1x+c_2y+c_3z)bigr) &=f_2(rc_1x+rc_2y+rc_3z)  &=rc_2+rc_3x+rc_1x^2  &=rcdot(c_2+c_3x+c_1x^2)  &=rcdot, f_2(c_1x+c_2y+c_3z) $ . Thus $ f_2 $ is an isomorphism. We write $ Visomorphictopolyspace_2 $ . Every space is isomorphic to itself under the identity map. The check is easy. An automorphism is an isomorphism of a space with itself. A dilation map $ map{d_s}{Re^2}{Re^2} $ that multiplies all vectors by a nonzero scalar $ s $ is an automorphism of $ Re^2 $ . Another automorphism is a rotation/ or turning map, $ map{t_{theta}}{Re^2}{Re^2} $ that rotates all vectors through an angle  $ theta $ . A third type of automorphism of $ Re^2 $ is a map $ map{f_ell}{Re^2}{Re^2} $ that flips or reflects all vectors over a line $ ell $ through the origin. Checking that these are automorphisms is $ X $ . Consider the space $ polyspace_5 $ of polynomials of degree  $ 5 $ or less and the map $ f $ that sends a polynomial $ p(x) $ to $ p(x - 1) $ . For instance, under this map $ x^2mapsto (x - 1)^2=x^2 - 2x+1 $ and $ x^3+2xmapsto (x - 1)^3+2(x - 1)=x^3 - 3x^2+5x - 3 $ . This map is an automorphism of this space; the check is $ X $ . This isomorphism of $ polyspace_5 $ with itself does more than just tell us that the space is ``the same'' as itself. It gives us some insight into the space's structure. Below is a family of parabolas, graphs of members of $ polyspace_5 $ . Each has a vertex at $ y= - 1 $ , and the left - most one has zeroes at $ - (ii)25 $ and $ - (i)75 $ , the next one has zeroes at $ - (i)25 $ and $ - 0.75 $ , etc. Substitution of $ x - 1 $ for $ x $ in any function's argument shifts its graph to the right by one. Thus, $ f(p_0)=p_1 $ , and $ f $ 's action is to shift all of the parabolas to the right by one. Notice that the picture before $ f $ is applied is the same as the picture after $ f $ is applied because while each parabola moves to the right, another one comes in from the left to take its place. This also holds true for cubics, etc. So the automorphism $ f $ expresses the idea that $ P_5 $ has a certain horizontal - homogeneity: if we draw two pictures showing all members of $ polyspace_5 $ , one picture centered at $ x=0 $ and the other centered at  $ x=1 $ , then the two pictures would be indistinguishable. As described in the opening to this section, having given the definition of isomorphism, we next look to support the thesis that it captures our intuition of vector spaces being the same. First, the definition itself is persuasive: a vector space consists of a set and some structure and the definition simply requires that the sets correspond and that the structures correspond also. Also persuasive are the examples above, such as $ X $ , which dramatize that isomorphic spaces are the same in all relevant respects. Sometimes people say, where $ Visomorphicto W $ , that `` $ W $ is just $ V $ painted green''Dash differences are merely cosmetic. The results below further support our contention that under an isomorphism all the things of interest in the two vector spaces correspond. Because we introduced vector spaces to study linear combinations, ``of interest'' means ``pertaining to linear combinations.'' Not of interest is the way that the vectors are presented typographically (or their color!spacefactor1000). An isomorphism maps a zero vector to a zero vector. Where $ map{f}{V}{W} $ is an isomorphism, fix some $ vec{v}in V $ . Then $ f(zero_V)=f(0cdotvec{v})=0cdot f(vec{v})=zero_W $ . For any map $ map{f}{V}{W} $ between vector spaces these statements are equivalent. $ f $ preserves structure $ f(vec{v}_1+vec{v}_2)=f(vec{v}_1)+f(vec{v}_2) quadtext{and}quad f(cvec{v})=c, f(vec{v}) $ . $ f $ preserves linear combinations of two vectors $ f(c_1vec{v}_1+c_2vec{v}_2)=c_1f(vec{v}_1)+c_2f(vec{v}_2) $ . $ f $ preserves linear combinations of any finite number of vectors $ f(c_1vec{v}_1+dots+c_nvec{v}_n)= c_1f(vec{v}_1)+dots+c_nf(vec{v}_n) $ . Since the implications mbox{ $ text{(3)}!implies!text{(2)} $ } and mbox{ $ text{(2)}!implies!text{(1)} $ } are clear, we need only show that mbox{ $ text{(1)}!implies!text{(3)} $ }. So assume statement (1). We will prove (3) by induction on the number of summands $ n $ . The one - summand base case, that $ f(cvec{v}_1)=c, f(vec{v}_1) $ , is covered by the second clause of statement (1). For the inductive step assume that statement (3) holds whenever there are $ k $ or fewer summands. Consider the $ k+1 $ - summand case. Use the first half of (1) to break the sum along the final ` $ + $ '. $ f(c_1vec{v}_1+dots+c_kvec{v}_k+c_{k+1}vec{v}_{k+1}) =f(c_1vec{v}_1+dots+c_kvec{v}_k)+f(c_{k+1}vec{v}_{k+1}) $ . Use the inductive hypothesis to break up the $ k $ - term sum on the left. $ =f(c_1vec{v}_1)+dots+f(c_kvec{v}_k)+f(c_{k+1}vec{v}_{k+1}) $ . Now the second half of (1) gives $ =c_1, f(vec{v}_1)+dots+c_k, f(vec{v}_k)+c_{k+1}, f(vec{v}_{k+1}) $ . when applied $ k+1 $  times. noindent We often use item (2) to simplify the verification that a map preserves structure. Finally, a summary. In the prior chapter, after giving the definition of a vector space, we looked at examples and noted that some spaces seemed to be essentially the same as others. Here we have defined the relation ` $ cong $ ' and have argued that it is the right way to precisely say what we mean by ``the same'' because it preserves the features of interest in a vector spaceDash in particular, it preserves linear combinations. In the next section we will show that isomorphism is an equivalence relation and so partitions the collection of vector spaces. In the prior subsection, after stating the definition of isomorphism, we gave some results supporting our sense that such a map describes spaces as ``the same.'' Here we will develop this intuition. When two (unequal) spaces are isomorphic we think of them as almost equal, as equivalent. We shall make that precise by proving that the relationship `is isomorphic to' is an equivalence relation. appendrefs{equivalence relations and equivalence classes} The inverse of an isomorphism is also an isomorphism. Suppose that $ V $ is isomorphic to $ W $ via $ map{f}{V}{W} $ . An isomorphism is a correspondence between the sets so $ f $ has an inverse function $ map{f^{ - 1}}{W}{V} $ that is also a correspondence.appendrefs{inverse functions}spacefactor1000 We will show that because $ f $ preserves linear combinations, so also does $ f^{ - 1} $ . Suppose that $ vec{w}_1, vec{w}_2in W $ . Because it is an isomorphism, $ f $ is onto and there are $ vec{v}_1, vec{v}_2in V $ such that $ vec{w}_1=f(vec{v}_1) $ and $ vec{w}_2=f(vec{v}_2) $ . Then f^{ - 1}(c_1cdotvec{w}_1+c_2cdotvec{w}_2) =f^{ - 1}bigl(, c_1cdot f(vec{v}_1) +c_2cdot f(vec{v}_2), bigr)  =f^{ - 1}(, fbigl(c_1vec{v}_1+c_2vec{v}_2), bigr) =c_1vec{v}_1+c_2vec{v}_2 =c_1cdot f^{ - 1}(vec{w}_1)+c_2cdot f^{ - 1}(vec{w}_2) since $ f^{ - 1}(vec{w}_1)=vec{v}_1 $ and $ f^{ - 1}(vec{w}_2)=vec{v}_2 $ . With that, by $ X $ 's second statement, this map preserves structure. Isomorphism is an equivalence relation between vector spaces. We must prove that the relation is symmetric, reflexive, and transitive. To check reflexivity, that any space is isomorphic to itself, consider the identity map. It is clearly one - to - one and onto. This shows that it preserves linear combinations. $ mbox{id}(c_1cdot vec{v}_1+c_2cdot vec{v}_2) =c_1vec{v}_1+c_2vec{v}_2 =c_1cdot mbox{id}(vec{v}_1)+c_2cdot mbox{id}(vec{v}_2) $ . Symmetry, that if $ V $ is isomorphic to  $ W $ then also $ W $ is isomorphic to  $ V $ , holds by $ X $ since each isomorphism map from $ V $ to $ W $ is paired with an isomorphism from $ W $ to $ V $ . To finish we must check transitivity, that if $ V $ is isomorphic to $ W $ and $ W $ is isomorphic to $ U $ then $ V $ is isomorphic to $ U $ . Let $ map{f}{V}{W} $ and $ map{g}{W}{U} $ be isomorphisms. Consider their composition $ map{composed{g}{f}}{V}{U} $ . Because the composition of correspondences is a correspondence, we need only check that the composition preserves linear combinations. $ composed{g}{f}:bigl(c_1cdotvec{v}_1+c_2cdotvec{v}_2bigr) &=gbigl(, f(, c_1cdot vec{v}_1+c_2cdotvec{v}_2, ), bigr)  &=gbigl(, c_1cdot f(vec{v}_1)+c_2cdot f(vec{v}_2), bigr)  &=c_1cdot gbigl(f(vec{v}_1))+c_2cdot g(f(vec{v}_2)bigr)  &=c_1cdot(composed{g}{f}), (vec{v}_1) +c_2cdot(composed{g}{f}), (vec{v}_2) $ . Thus the composition is an isomorphism. Since it is an equivalence, isomorphism partitions the universe of vector spaces into classes: each space is in one and only one isomorphism class. The next result characterizes these classes by dimension. That is, we can describe each class simply by giving the number that is the dimension of all of the spaces in that class. Vector spaces are isomorphic if and only if they have the same dimension. In this double implication statement the proof of each half involves a significant idea so we will do the two separately. If spaces are isomorphic then they have the same dimension. We shall show that an isomorphism of two spaces gives a correspondence between their bases. That is, we shall show that if $ map{f}{V}{W} $ is an isomorphism and a basis for the domain $ V $ is $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ then its image $ D=sequence{f(vec{beta}_1), dots, f(vec{beta}_n)} $ is a basis for the codomain $ W $ . (The other half of the correspondence, that for any basis of $ W $ the inverse image is a basis for $ V $ , follows from the fact that $ f^{ - 1} $ is also an isomorphism and so we can apply the prior sentence to $ f^{ - 1} $ .) To see that $ D $ spans $ W $ , fix any $ vec{w}in W $ . Because $ f $ is an isomorphism it is onto and so there is a $ vec{v}in V $ with $ vec{w}=f(vec{v}) $ . Expand $ vec{v} $ as a combination of basis vectors. $ vec{w}=f(vec{v}) =f(v_1vec{beta}_1+dots+v_nvec{beta}_n) =v_1cdot f(vec{beta}_1)+dots+v_ncdot f(vec{beta}_n) $ . For linear independence of $ D $ , if $ zero_W =c_1f(vec{beta}_1)+dots+c_nf(vec{beta}_n) =f(c_1vec{beta}_1+dots+c_nvec{beta}_n) $ . then, since $ f $ is one - to - one and so the only vector sent to $ zero_W $ is $ zero_V $ , we have that $ zero_V=c_1vec{beta}_1+dots+c_nvec{beta}_n $ , which implies that all of the $ c $ 's are zero. If spaces have the same dimension then they are isomorphic. We will prove that any space of dimension  $ n $ is isomorphic to $ Re^n $ . Then we will have that all such spaces are isomorphic to each other by transitivity, which was shown in $ X $ . Let $ V $ be $ n $ - dimensional. Fix a basis $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ for the domain $ V $ . Consider the operation of representing the members of $ V $ with respect to $ B $ as a function from $ V $ to $ Re^n $ . $ vec{v}=v_1vec{beta}_1+dots+v_nvec{beta}_n , mapsunder{text{small Rep}_B}, colvec{v_1  vdots  v_n} $ . It is well - definedappendrefs{well - defined}spacefactor1000 since every $ vec{v} $ has one and only one such representation (see $ X $ following this proof). This function is one - to - one because if $ text{Rep}_B(u_1vec{beta}_1+dots+u_nvec{beta}_n) =text{Rep}_B(v_1vec{beta}_1+dots+v_nvec{beta}_n) $ . then $ colvec{u_1  vdots  u_n} = colvec{v_1  vdots  v_n} $ . and so $ u_1=v_1 $ , ldots, $ u_n=v_n $ , implying that the original arguments $ u_1vec{beta}_1+dots+u_nvec{beta}_n $ and $ v_1vec{beta}_1+dots+v_nvec{beta}_n $ are equal. This function is onto; any member of $ Re^n $ $ vec{w}=colvec{w_1  vdots  w_n} $ . is the image of some $ vec{v}in V $ , namely $ vec{w}=rep{w_1vec{beta}_1+dots+w_nvec{beta}_n}{B} $ . Finally, this function preserves structure. $ rep{rcdotvec{u}+scdotvec{v}}{B} &=rep{, (ru_1+sv_1)vec{beta}_1+dots+(ru_n+sv_n)vec{beta}_n, }{B}  &=colvec{ru_1+sv_1  vdots  ru_n+sv_n}  &=rcdotcolvec{u_1  vdots  u_n}+scdotcolvec{v_1  vdots  v_n}  &=rcdotrep{vec{u}}{B}+scdotrep{vec{v}}{B} $ . Therefore $ mbox{Rep}_B $ is an isomorphism. Consequently any $ n $ - dimensional space is isomorphic to $ Re^n $ . When we introduced the $ mbox{Rep}_B $ notation for vectors on page $ X $ , we noted that it is not standard and said that one advantage it has is that it is harder to overlook. Here we see its other advantage: this notation makes explicit that $ mbox{Rep}_B $ is a function from  $ V $ to $ R^n $ . The proof has a sentence about `well - defined.' Its point is that to be an isomorphism $ mbox{Rep}_B $ must be a function. The definition of function requires that for all inputs the associated output must exists and must be determined by the input. So we must check that every $ vec{v} $ is associated with at least one  $ mbox{Rep}_B(vec{v}) $ , and with no more than one. In the proof we express elements $ vec{v} $ of the domain space as combinations of members of the basis  $ B $ and then associate $ vec{v} $ with the column vector of coefficients. That there is at least one expansion of each  $ vec{v} $ holds because $ B $ is a basis and so spans the space. The worry that there is no more than one associated member of the codomain is subtler. A contrasting example, where an association fails this unique output requirement, illuminates the issue. Let the domain be $ polyspace_2 $ and consider a set that is not a basis (it is not linearly independent, although it does span the space). $ A=set{1+0x+0x^2, 0+1x+0x^2, 0+0x+1x^2, 1+1x+2x^2} $ . Call those polynomials $ vec{alpha}_1 $ , ldots, $ vec{alpha}_4 $ . In contrast to the situation when the set is a basis, here there can be more than one expression of a domain vector in terms of members of the set. For instance, consider $ vec{v}=1+x+x^2 $ . Here are two different expansions. $ vec{v}=1vec{alpha}_1+1vec{alpha}_2+1vec{alpha}_3+0vec{alpha}_4 qquad vec{v}=0vec{alpha}_1+0vec{alpha}_2 - 1vec{alpha}_3+1vec{alpha}_4 $ . So this input vector  $ vec{v} $ is associated with more than one column. $ colvec[r]{1  1  1  0} qquad colvec[r]{0  0  - 1  1} $ . Thus, with  $ A $ the association is not well - defined. (The issue is that $ A $ is not linearly independent; to show uniqueness Theorem Two.III.'s proof uses only linear independence.) In general, any time that we define a function we must check that output values are well - defined. Most of the time that condition is perfectly obvious but in the above proof it needs verification. See $ X $ . Each finite - dimensional vector space is isomorphic to one and only one of the $ Re^n $ . This gives us a collection of representatives of the isomorphism classes. appendrefs{equivalence class representatives}spacefactor1000 The proofs above pack many ideas into a small space. Through the rest of this chapter we'll consider these ideas again, and fill them out. As a taste of this we will expand here on the proof of $ X $ . The space $ matspace_{nbyn{2}} $ of $ nbyn{2} $ matrices is isomorphic to $ Re^4 $ . With this basis for the domain $ B=sequence{[r] 1 &0  0 &0 , [r] 0 &1  0 &0 , [r] 0 &0  1 &0 , [r] 0 &0  0 &1 } $ . the isomorphism given in the lemma, the representation map $ f_1=mbox{Rep}_B $ , carries the entries over. $ a &b  c &d mapsunder{f_1} colvec{a  b  c  d} $ . One way to think of the map $ f_1 $ is: fix the basis $ B $ for the domain, use the standard basis $ stdbasis_4 $ for the codomain, and associate $ vec{beta}_1 $ with $ vec{e}_1 $ , $ vec{beta}_2 $ with $ vec{e}_2 $ , etc. Then extend this association to all of the members of two spaces. $ a &b  c &d =avec{beta}_1+bvec{beta}_2+cvec{beta}_3+dvec{beta}_4 ;;mapsunder{f_1};; avec{e}_1+bvec{e}_2+cvec{e}_3+dvec{e}_4 =colvec{a  b  c  d} $ . We can do the same thing with different bases, for instance, taking this basis for the domain. $ A=sequence{ [r] 2 &0  0 &0 , [r] 0 &2  0 &0 , [r] 0 &0  2 &0 , [r] 0 &0  0 &2 } $ . Associating corresponding members of $ A $ and $ stdbasis_4 $ gives this. a &b  c &d =(a/2)vec{alpha}_1+(b/2)vec{alpha}_2 +(c/2)vec{alpha}_3+(d/2)vec{alpha}_4  mapsunder{f_2};; (a/2)vec{e}_1+(b/2)vec{e}_2+(c/2)vec{e}_3+(d/2)vec{e}_4 =colvec{a/2  b/2  c/2  d/2} gives rise to an isomorphism that is different than $ f_1 $ . The prior map arose by changing the basis for the domain. We can also change the basis for the codomain. Go back to the basis $ B $ above and use this basis for the codomain. $ D=sequence{colvec[r]{1  0  0  0}, colvec[r]{0  1  0  0}, colvec[r]{0  0  0  1}, colvec[r]{0  0  1  0}} $ . Associate $ vec{beta}_1 $ with $ vec{delta}_1 $ , etc. Extending that gives another isomorphism. $ a &b  c &d =avec{beta}_1+bvec{beta}_2+cvec{beta}_3+dvec{beta}_4 ;;mapsunder{f_3};; avec{delta}_1+bvec{delta}_2+cvec{delta}_3+dvec{delta}_4 =colvec{a  b  d  c} $ . We close with a recap. Recall that the first chapter defines two matrices to be row equivalent if they can be derived from each other by row operations. There we showed that relation is an equivalence and so the collection of matrices is partitioned into classes, where all the matrices that are row equivalent together fall into a single class. Then for insight into which matrices are in each class we gave representatives for the classes, the reduced echelon form matrices. In this section we have followed that pattern except that the notion here of ``the same'' is vector space isomorphism. We defined it and established some properties, including that it is an equivalence. Then, as before, we developed a list of class representatives to help us understand the partitionDash it classifies vector spaces by dimension. In Chapter Two, with the definition of vector spaces, we seemed to have opened up our studies to many examples of new structures besides the familiar $ Re^n $ 's. We now know that isn't the case. Any finite - dimensional vector space is actually ``the same'' as a real space.
This chapter shows that every square matrix is similar to one that is a sum of two kinds of simple matrices. The prior section focused on the first simple kind, diagonal matrices. We now consider the other kind. Because a linear transformation $ map{t}{V}{V} $ has the same domain as codomain, we can compose $ t $ with itself: $ t^2=composed{t}{t} $ , and $ t^3=composed{t}{composed{t}{t}} $ , etc.appendrefs{function iteration}spacefactor=1000 The superscript power notation $ t^j $ for iterates of the transformations fits with the notation that we've used for their square matrix representations because if $ rep{t}{B, B}=T $ then $ rep{t^j}{B, B}=T^j $ . For the derivative map $ map{d/dx}{polyspace_3}{polyspace_3} $ given by $ a+bx+cx^2+dx^3xmapsunder{d/dx} b+2cx+3dx^2 $ . the second power is the second derivative, $ a+bx+cx^2+dx^3xmapsunder{d^2/dx^2} 2c+6dx $ . the third power is the third derivative, $ a+bx+cx^2+dx^3xmapsunder{d^3/dx^3} 6d $ . and any higher power is the zero map. This transformation of the space $ matspace_{nbyn{2}} $ of $ nbyn{2} $ matrices $ a &b  c &d mapsunder{t} b &a  d &0 $ . has this second power $ a &b  c &d mapsunder{t^2} a &b  0 &0 $ . and this third power. $ a &b  c &d mapsunder{t^3} b &a  0 &0 $ . After that, $ t^4=t^2 $ and $ t^5=t^3 $ , etc. Consider the shift transformation $ map{t}{C^3}{C^3} $ . $ colvec{x  y  z} mapsunder{t} colvec{0  x  y} $ . We have that $ colvec{x  y  z} mapsunder{t} colvec{0  x  y} mapsunder{t} colvec{0  0  x} mapsunder{t} colvec{0  0  0} $ . so the range spaces descend to the trivial subspace. $ rangespace{t}=set{colvec{0  a  b}suchthat a, binC} qquad rangespace{t^2}=set{colvec{0  0  c}suchthat cinC} qquad rangespace{t^3}=set{colvec{0  0  0}} $ . These examples suggest that after some number of iterations the map settles down. For any transformation $ map{t}{V}{V} $ , the range spaces of the powers form a descending chain $ Vsupseteq rangespace{t}supseteqrangespace{t^2}supseteqcdots $ . and the null spaces form an ascending chain. $ set{vec{0}}subseteqnullspace{t}subseteqnullspace{t^2}subseteqcdots $ . Further, there is a $ k>0 $ such that for powers less than $ k $ the subsets are proper: if $ j<k $ then $ rangespace{t^j}supsetrangespace{t^{j+1}} $ and $ nullspace{t^j}subsetnullspace{t^{j+1}} $ , while if $ jgeq k $ then $ rangespace{t^j}=rangespace{t^{j+1}} $ and $ nullspace{t^j}=nullspace{t^{j+1}} $ ). noindent (The $ k=1 $ case can happen, for instance if $ t $ is the identity map, so that in the chains none of the subsets are proper subsets.) First recall that for any map the dimension of its range space plus the dimension of its null space equals the dimension of its domain. So if the dimensions of the range spaces shrink then the dimensions of the null spaces must rise. We will do the range space half here and leave the rest for $ X $ . We start by showing that the range spaces form a chain. If $ vec{w}inrangespace{t^{j+1}} $ , so that $ vec{w}=t^{j+1}(vec{v}) $ for some $ vec{v} $ , then $ vec{w}=t^{j}(, t(vec{v}), ) $ . Thus $ vec{w}inrangespace{t^{j}} $ . Next we verify the ``further'' property: in the chain the subsets containments are proper initially, and then from some power $ k $ onward the range spaces are equal. We first show that if any pair of adjacent range spaces in the chain are equal $ rangespace{t^{k}}=rangespace{t^{k+1}} $ then all later ones are also equal: $ rangespace{t^{k+1}}=rangespace{t^{k+2}} $ , etc. This holds because $ map{t}{rangespace{t^{k+1}}}{rangespace{t^{k+2}}} $ is the same map, with the same domain, as $ map{t}{rangespace{t^{k}}}{rangespace{t^{k+1}}} $ , and it therefore has the same range $ rangespace{t^{k+1}}=rangespace{t^{k+2}} $ (it holds for all higher powers by induction). So if the chain of range spaces ever stops strictly decreasing then from that point onward it is stable. We end by showing that the chain must eventually stop decreasing. Each range space is a subspace of the one before it. For it to be a proper subspace, it must be of strictly lower dimension (see $ X $ ). These spaces are finite - dimensional and so the chain can fall for only finitely many steps. That is, the power $ k $ is at most the dimension of $ V $ . The derivative map $ a+bx+cx^2+dx^3xmapsunder{d/dx} b+2cx+3dx^2 $ on $ polyspace_3 $ has this chain of range spaces. $ rangespace{t^0}=polyspace_3 :supset:rangespace{t^1}=polyspace_2 :supset:rangespace{t^2}=polyspace_1 :supset:rangespace{t^3}=polyspace_0 :supset:rangespace{t^4}=set{zero} $ . All later elements of the chain are the trivial space. It has this chain of null spaces. $ nullspace{t^0}=set{zero} :subset: nullspace{t^1}=polyspace_0 :subset:nullspace{t^2}=polyspace_1 :subset:nullspace{t^3}=polyspace_2 :subset:nullspace{t^4}=polyspace_3 $ . Later elements are the entire space. Let $ map{t}{polyspace_2}{polyspace_2} $ be the map $ d_0+d_1x+d_2x^2 mapsto 2d_0+d_2x. $ As the lemma describes, on iteration the range space shrinks $ rangespace{t^0}=polyspace_2 quad rangespace{t}=set{a_0+a_1xsuchthat a_0, a_1inC} quad rangespace{t^2}=set{a_0suchthat a_0inC} $ . and then stabilizes, so that $ rangespace{t^2}=rangespace{t^3}=cdots $ . The null space grows $ nullspace{t^0}=set{0} quad nullspace{t}=set{b_1xsuchthat b_1inC} quad nullspace{t^2}=set{b_1x+b_2x^2suchthat b_1, b_2inC} $ . and then stabilizes $ nullspace{t^2}=nullspace{t^3}=cdots $ . The transformation $ map{pi}{C^3}{C^3} $ projecting onto the first two coordinates $ colvec{c_1  c_2  c_3} mapsunder{pi} colvec{c_1  c_2  0} $ . has $ C^3supsetrangespace{pi}=rangespace{pi^2}=cdots $ and $ set{zero}subsetnullspace{pi}=nullspace{pi^2}=cdots, $ where this is the range space and the null space. $ rangespace{pi}=set{colvec{a  b  0}suchthat a, binC} qquad nullspace{pi}=set{colvec{0  0  c}suchthat cinC} $ . Let $ t $ be a transformation on an $ n $ - dimensional space. The generalized range space (or closure of the range space/ ) is $ genrangespace{t}=rangespace{t^n} $ . The generalized null space (or closure of the null space/ ) is $ gennullspace{t}=nullspace{t^n} $ . This graph illustrates. The horizontal axis gives the power  $ j $ of a transformation. The vertical axis gives the dimension of the range space of $ t^j $ as the distance above zero, and thus also shows the dimension of the null space because the two add to the dimension $ n $ of the domain. On iteration the rank falls and the nullity rises until there is some $ k $ such that the map reaches a steady state $ rangespace{t^k}=rangespace{t^{k+1}}=genrangespace{t} $ and $ nullspace{t^k}=nullspace{t^{k+1}}=gennullspace{t} $ . This must happen by the $ n $ - th iterate. noindenttextit{This requires material from the optional Combining Subspaces subsection.} The prior subsection shows that as $ j $ increases the dimensions of the $ rangespace{t^j} $ 's fall while the dimensions of the $ nullspace{t^j} $ 's rise, in such a way that this rank and nullity split between them the dimension of $ V $ . Can we say more; do the two split a basisDash is $ V=rangespace{t^j}directsumnullspace{t^j} $ ? The answer is yes for the smallest power $ j=0 $ since $ V=rangespace{t^0}directsumnullspace{t^0}=Vdirectsumset{zero} $ . The answer is also yes at the other extreme. For any linear $ map{t}{V}{V} $ the function $ map{t}{genrangespace{t}}{genrangespace{t}} $ is one - to - one. Let the dimension of $ V $ be $ n $ . Because $ rangespace{t^n}=rangespace{t^{n+1}} $ , the map $ map{t}{genrangespace{t}}{genrangespace{t}} $ is a dimension - preserving homomorphism. Therefore, by Theorem Three.II. it is one - to - one. Where $ map{t}{V}{V} $ is a linear transformation, the space is the direct sum $ V=genrangespace{t}directsumgennullspace{t} $ . That is, both (1)  $ dim(V)=dim(genrangespace{t})+dim(gennullspace{t}) $ and (2)  $ genrangespace{t}intersectiongennullspace{t}=set{zero} $ . Let the dimension of $ V $ be $ n $ . We will verify the second sentence, which is equivalent to the first. Clause (1) is true because any transformation satisfies that its rank plus its nullity equals the dimension of the space, and in particular this holds for the transformation $ t^n $ . For clause (2), assume that $ vec{v}ingenrangespace{t}intersectiongennullspace{t} $ to prove that $ vec{v}=zero $ . Because $ vec{v} $ is in the generalized null space, $ t^n(vec{v})=zero $ . On the other hand, by the lemma $ map{t}{genrangespace{t}}{genrangespace{t}} $ is one - to - one and a composition of one - to - one maps is one - to - one, so $ map{t^n}{genrangespace{t}}{genrangespace{t}} $ is one - to - one. Only $ zero $ is sent by a one - to - one linear map to $ zero $ so the fact that $ t^n(vec{v})=zero $ implies that $ vec{v}=zero $ . Technically there is a difference between the map $ map{t}{V}{V} $ and the map on the subspace $ map{t}{genrangespace{t}}{genrangespace{t}} $ if the generalized range space is not equal to $ V $ , because the domains are different. But the difference is small because the second is the restriction appendrefs{map restrictions}spacefactor=1000 of the first to $ genrangespace{t} $ . For powers between $ j=0 $ and  $ j=n $ , the space $ V $ might not be the direct sum of $ rangespace{t^j} $ and $ nullspace{t^j} $ . The next example shows that the two can have a nontrivial intersection. Consider the transformation of $ C^2 $ defined by this action on the elements of the standard basis. $ colvec[r]{1  0} mapsunder{n} colvec[r]{0  1} quad colvec[r]{0  1} mapsunder{n} colvec[r]{0  0} qquad N=rep{n}{stdbasis_2, stdbasis_2}=[r] 0 &0  1 &0 $ . This is a shift map because it shifts the entries down, with the bottom entry shifting entirely out of the vector. $ colvec{x  y}mapsto colvec{0  x} $ . On the basis, this map's action gives a string. $ {ccccc} colvec{1  0} &mapsto &colvec{0  1} &mapsto &colvec{0  0} qquadtext{that is}qquad {ccccc} vec{e}_1 &mapsto &vec{e}_2 &mapsto &zero $ . This map is a natural way to have a vector in both the range space and null space; the string depiction shows that this is one such vector. $ vec{e}_2=colvec[r]{0  1} $ . Observe also that although $ n $ is not the zero map, the function $ n^2=composed{n}{n} $ is the zero map. A linear function $ map{hat{n}}{C^4}{C^4} $ whose action on $ stdbasis_4 $ is given by the string $ {ccccccccc} vec{e}_1 &mapsto &vec{e}_2 &mapsto &vec{e}_3 &mapsto &vec{e}_4 &mapsto &zero $ . has $ rangespace{hat{n}}intersectionnullspace{hat{n}} $ equal to the span $ spanof{set{vec{e}_4}} $ , has $ rangespace{hat{n}^2}intersectionnullspace{hat{n}^2}= spanof{set{vec{e}_3, vec{e}_4}} $ , and has $ rangespace{hat{n}^3}intersectionnullspace{hat{n}^3}= spanof{set{vec{e}_4}} $ . The matrix representation is all zeros except for some subdiagonal ones. $ hat{N}=rep{hat{n}}{stdbasis_4, stdbasis_4} =[r] 0 &0 &0 &0  1 &0 &0 &0  0 &1 &0 &0  0 &0 &1 &0  $ . Although $ hat{n} $ is not the zero map, and neither is $ hat{n}^2 $ or  $ hat{n}^3 $ , the function $ hat{n}^4 $ is the zero function. Transformations can act via more than one string. The transformation $ t $ acting on a basis $ B=sequence{vec{beta}_1, dots, vec{beta}_5} $ by $ {ccccccc} vec{beta}_1 &mapsto &vec{beta}_2 &mapsto &vec{beta}_3 &mapsto &zero  vec{beta}_4 &mapsto &vec{beta}_5 &mapsto &zero $ . will have, for instance, $ vec{beta}_3 $ in the intersection of its range space and null space. The strings make clear that $ t^3 $ is the zero map. This map is represented by a matrix that is all zeros except for blocks of subdiagonal ones $ rep{t}{B, B}= {rrr|rr} 0 &0 &0 &0 &0  1 &0 &0 &0 &0  0 &1 &0 &0 &0  hline 0 &0 &0 &0 &0  0 &0 &0 &1 &0 $ . (the lines just visually organize the blocks). In those examples all vectors are eventually transformed to zero. A nilpotent transformation is one with a power that is the zero map. A nilpotent matrix is one with a power that is the zero matrix. In either case, the least such power is the index of nilpotency. In $ X $ the index of nilpotency is two. In $ X $ it is four. In $ X $ it is three. The differentiation map $ map{d/dx}{polyspace_2}{polyspace_2} $ is nilpotent of index three since the third derivative of any quadratic polynomial is zero. This map's action is described by the string $ x^2mapsto 2xmapsto 2mapsto 0 $ and taking the basis $ B=sequence{x^2, 2x, 2} $ gives this representation. $ rep{d/dx}{B, B}= [r] 0 &0 &0  1 &0 &0  0 &1 &0 $ . Not all nilpotent matrices are all zeros except for blocks of subdiagonal ones. With the matrix $ hat{N} $ from $ X $ , and this four - vector basis $ D=sequence{colvec[r]{1  0  1  0}, colvec[r]{0  2  1  0}, colvec[r]{1  1  1  0}, colvec[r]{0  0  0  1}} $ . a change of basis operation produces this representation with respect to $ D, D $ . $ [r] 1 &0 &1 &0  0 &2 &1 &0  1 &1 &1 &0  0 &0 &0 &1 [r] 0 &0 &0 &0  1 &0 &0 &0  0 &1 &0 &0  0 &0 &1 &0 [r] 1 &0 &1 &0  0 &2 &1 &0  1 &1 &1 &0  0 &0 &0 &1 ^{ - 1}!! = [r] - 1 &0 &1 &0  - 3 & - 2 &5 &0  - 2 & - 1 &3 &0  2 &1 & - 2 &0 $ . The new matrix is nilpotent; its fourth power is the zero matrix. We could verify this with a tedious computation or we can instead just observe that it is nilpotent since its fourth power is similar to $ hat{N}^4 $ , the zero matrix, and the only matrix similar to the zero matrix is itself. $ (Phat{N}P^{ - 1})^4 =Phat{N}P^{ - 1}cdot Phat{N}P^{ - 1}cdot Phat{N}P^{ - 1}cdot Phat{N}P^{ - 1} =Phat{N}^4P^{ - 1} $ . The goal of this subsection is to show that the prior example is prototypical in that every nilpotent matrix is similar to one that is all zeros except for blocks of subdiagonal ones. Let $ t $ be a nilpotent transformation on $ V $ . A definend{ $ t $ - string generated by $ vec{v}in V $ } is a sequence $ sequence{vec{v}, t(vec{v}), ldots, t^{k - 1}(vec{v})} $ such that $ t^k(vec{v})=zero $ . A $ t $ - string basis is a basis that is a concatenation of $ t $ - strings. noindent (The strings cannot form a basis under concatenation unless they are disjoint because a basis cannot have a repeated vector.) This linear map $ map{t}{C^3}{C^3} $ $ colvec{x  y  z}mapsunder{t}colvec{y  z  0} $ . is nilpotent, of index  $ 3 $ . $ colvec{x  y  z} mapsunder{t}colvec{y  z  0} mapsunder{t}colvec{z  0  0} mapsunder{t}colvec{0  0  0} $ . This is a $ t $ - string. $ sequence{colvec{0  0  1}, colvec{0  1  0}, colvec{1  0  0}} $ . Because that sequence is a basis, it is a $ t $ - string basis for the space  $ C^3 $ . Where the sequence of the prior example is $ sequence{vec{beta}_1, vec{beta}_2, vec{beta}_3} $ , another $ t $ - string is $ sequence{vec{beta}_2, vec{beta}_3} $ . But of course the second sequence is not a basis for $ C^3 $ . In that sense, the $ t $ - strings in a $ t $ - string basis must be maximal. The linear map of differentiation $ map{d/dx}{polyspace_2}{polyspace_2} $ is nilpotent. The sequence $ sequence{x^2, 2x, 2} $ is a $ d/dx $ - string of length  $ 3 $ ; in particular, this string satisfies the requirement that $ d/dx(2)=0 $ . Because it is a basis, that sequence is a $ d/dx $ - string basis for $ polyspace_2 $ . In $ X $ , we can concatenate the $ t $ - strings $ sequence{vec{beta}_1, vec{beta}_2, vec{beta}_3} $ and $ sequence{vec{beta}_4, vec{beta}_5} $ to make a basis for the domain of $ t $ . If a space has a $ t $ - string basis then the index of nilpotency of $ t $ equals the length of the longest string in that basis. Let the space have a $ t $ - string basis and let $ t $ 's index of nilpotency be  $ k $ . Then $ t^k $ sends any vector to $ zero $ . That must include the vector starting any string, so each string in the string basis has length at most  $ k $ . Now instead suppose that the space has a $ t $ - string basis  $ B $ where all of the strings are shorter than length  $ k $ . Because $ t $ has the index of nilpotency  $ k $ , there is a $ vec{v} $ such that $ t^{k - 1}(vec{v})neqzero $ . Represent $ vec{v} $ as a linear combination of elements from  $ B $ and apply $ t^{k - 1} $ . We are supposing that $ t^{k - 1} $ maps each element of  $ B $ to $ zero $ . It therefore maps each term in the linear combination to $ zero $ , contradicting that it does not map $ vec{v} $ to $ zero $ . We shall show that each nilpotent map has an associated string basis, a basis of disjoint strings. To see the main idea of the argument, imagine that we want to construct a counterexample, a map that is nilpotent but without an associated basis of disjoint strings. We might think to make something like the map $ map{t}{C^5}{C^5} $ with this action. But, the fact that the shown basis isn't disjoint doesn't mean that there isn't another basis that consists of disjoint strings. To produce such a basis for this map we will first find the number and lengths of its strings. Observe that $ t $ 's index of nilpotency is two. $ X $ says that in a disjoint string basis at least one string has length two. There are five basis elements so if there is a disjoint string basis then the map must act in one of these ways. $ {ccccc} vec{beta}_1 &mapsto &vec{beta}_2 &mapsto &zero  vec{beta}_3 &mapsto &vec{beta}_4 &mapsto &zero  vec{beta}_5 &mapsto &zero hspace*{3em} {ccccc} vec{beta}_1 &mapsto &vec{beta}_2 &mapsto &zero  vec{beta}_3 &mapsto &zero  vec{beta}_4 &mapsto &zero  vec{beta}_5 &mapsto &zero $ . Now, the key point. A transformation with the left - hand action has a null space of dimension three since that's how many basis vectors are mapped to zero. A transformation with the right - hand action has a null space of dimension four. With the matrix representation above we can determine which of the two possible shapes is right. $ nullspace{t}= set{colvec{x  - x  z  0  r}suchthat x, z, rinC } $ . This is three - dimensional, meaning that of the two disjoint string basis forms above, $ t $ 's basis has the left - hand one. To produce a string basis for  $ t $ , first pick $ vec{beta}_2 $ and $ vec{beta}_4 $ from $ rangespace{t}intersectionnullspace{t} $ . $ vec{beta}_2=colvec[r]{0  0  1  0  0}qquad vec{beta}_4=colvec[r]{0  0  0  0  1} $ . (Other choices are possible, just be sure that the set $ set{vec{beta}_2, vec{beta}_4} $ is linearly independent.) For $ vec{beta}_5 $ pick a vector from $ nullspace{t} $ that is not in the span of $ set{ vec{beta}_2, vec{beta}_4 } $ . $ vec{beta}_5=colvec[r]{1  - 1  0  0  0} $ . Finally, take $ vec{beta}_1 $ and $ vec{beta}_3 $ such that $ t(vec{beta}_1)=vec{beta}_2 $ and $ t(vec{beta}_3)=vec{beta}_4 $ . $ vec{beta}_1=colvec[r]{0  1  0  0  0}qquad vec{beta}_3=colvec[r]{0  0  0  1  0} $ . Therefore, we have a string basis $ B=sequence{vec{beta}_1, ldots, vec{beta}_5} $ and with respect to that basis the matrix of $ t $ has blocks of subdiagonal  $ 1 $ 's. $ rep{t}{B, B}= {rr|rr|r} 0 &0 &0 &0 &0  1 &0 &0 &0 &0  hline 0 &0 &0 &0 &0  0 &0 &1 &0 &0  hline 0 &0 &0 &0 &0 $ . Any nilpotent transformation $ t $ is associated with a $ t $ - string basis. While the basis is not unique, the number and the length of the strings is determined by $ t $ . This illustrates the proof, which describes three kinds of basis vectors. They are in squares if they are members of the null space and in circles if they are not. $ {ccccccccccccccccccc} digitincirc{3} &mapsto &digitincirc{1} &mapsto &cdots & & & & & & & &cdots &mapsto &digitincirc{1} &mapsto &digitinsq{1} &mapsto &zero  $ .75ex] digitincirc{3} &mapsto &digitincirc{1} &mapsto &cdots & & & & & & & &cdots &mapsto &digitincirc{1} &mapsto &digitinsq{1} &mapsto &zero  $ .75ex] &smash{vdotswithin{mapsto}}  digitincirc{3} &mapsto &digitincirc{1} &mapsto &cdots & &mapsto &digitincirc{1} &mapsto &digitinsq{1} &mapsto &zero  $ 1ex] digitinsq{2} &mapsto &zero  $ .75ex] &smash{vdotswithin{mapsto}}  digitinsq{2} &mapsto &zero $ . Fix a vector space $ V $ . We will argue by induction on the index of nilpotency. If the map $ map{t}{V}{V} $ has index of nilpotency  $ 1 $ then it is the zero map and any basis is a string basis, $ vec{beta}_1mapstozero $ , ldots, $ vec{beta}_nmapstozero $ . For the inductive step, assume that the theorem holds for any transformation $ map{t}{V}{V} $ with an index of nilpotency from $ 1 $ up to and including  $ k - 1 $ (with $ k>1 $ ), and consider the index  $ k $ case. What gets the induction going is the observaton that the restriction of  $ t $ to the range space  $ rangespace{t} $ is also nilpotent, of index $ k - 1 $ . So apply the inductive hypothesis to get a string basis for $ rangespace{t} $ , where the number and length of the strings is determined by $ t $ . $ B=cat{cat{sequence{vec{beta}_1, t(vec{beta}_1), dots, t^{h_1}(vec{beta}_1)}}{ cat{sequence{vec{beta}_2, ldots, t^{h_2}(vec{beta}_2)}}}{cdots}}{ sequence{vec{beta}_i, ldots, t^{h_i}(vec{beta}_i)} } $ . We write  $ i $ for the number of strings. In the illustration above, these are the vectors of kind  $ 1 $ . Taking the final vector in each string gives a basis $ C=sequence{t^{h_1}(vec{beta}_1), dots, t^{h_i}(vec{beta}_i)} $ for the intersection $ rangespace{t}intersectionnullspace{t} $ . This is because a member of $ rangespace{t} $ maps to zero if and only if it is a linear combination of basis vectors that map to zero. The illustration shows these as $ 1 $ 's in squares. Now extend $ C $ to a basis for the entire nullspace, $ nullspace{t} $ . $ hat{C}=cat{C}{sequence{vec{xi}_1, dots, vec{xi}_p}} $ . While the $ vec{xi} $ 's aren't uniquely determined by $ t $ , what is uniquely determined is the number of them: it is the dimension of $ nullspace{t} $ minus the dimension of $ rangespace{t}intersectionnullspace{t} $ . In the illustration, the $ vec{xi} $ 's are the vectors of kind  $ 2 $ , and so $ hat{C} $ is the set of vectors in squares. Finally, $ cat{B}{sequence{vec{xi}_1, dots, vec{xi}_p}} $ is a basis for $ rangespace{t}+nullspace{t} $ . This is because a sum of something in the range space with something in the null space can be represented using elements of $ B $ for the range space part along with $ xi $ 's for any part from $ nullspace{t} - rangespace{t} $ . Note that $ dimbig(rangespace{t}+nullspace{t}big) &= dim (rangespace{t})+dim (nullspace{t}) - dimbig(rangespace{t}intersectionnullspace{t}big)  &= rank (t)+nullity (t) - i  &= dim (V) - i $ . and so we can extend the basis $ cat{B}{sequence{vec{xi}_1, dots, vec{xi}_p}} $ to a $ t $ - string basis for the entirety of $ V $ with the addition of  $ i $ more vectors. To produce those, recall that each of $ vec{beta}_1, dots, vec{beta}_i $ is in the range space, $ rangespace{t} $ , and so use vectors $ vec{v}_1, dots, vec{v}_iin V $ such that $ t(vec{v}_1)=vec{beta}_1, dots, t(vec{v}_i)=vec{beta}_i $ . The check that this extension preserves linear independence is $ X $ . In the illustration, $ vec{v}_1, dots, vec{v}_i $ are the $ 3 $ 's. Every nilpotent matrix is similar to a matrix that is all zeros except for blocks of subdiagonal ones. That is, every nilpotent map is represented with respect to some basis by such a matrix. This form is unique in the sense that if a nilpotent matrix is similar to two such matrices then those two simply have their blocks ordered differently. Thus this is a canonical form for the similarity classes of nilpotent matrices provided that we order the blocks, say, from longest to shortest. The matrix $ M=[r] 1 & - 1  1 & - 1 $ . has an index of nilpotency of two, as this calculation shows. Because the matrix is $ nbyn{2} $ , any transformation that it represents is on a space of dimension two. The nullspace of one application of the map $ nullspace{m} $ has dimension one, and the nullspace of two applications $ nullspace{m^2} $ has dimension two. Thus the action of $ m $ on a string basis is $ vec{beta}_1mapstovec{beta}_2mapstozero $ and the canonical form of the matrix is this. $ N=[r] 0 &0  1 &0 $ . We can exhibit such a string basis, and also the change of basis matrices witnessing the matrix similarity between $ M $ and  $ N $ . Suppose that $ map{m}{C^2}{C^2} $ is such that $ M $ represents it with respect to the standard bases. (We could take $ M $ to be a representation with respect to some other basis but the standard one is convenient.) Pick $ vec{beta}_2innullspace{m} $ . Also pick $ vec{beta}_1 $ so that $ m(vec{beta}_1)=vec{beta}_2 $ . $ vec{beta}_2=colvec[r]{1  1} qquad vec{beta}_1=colvec[r]{1  0} $ . For the change of basis matrices, recall the similarity diagram. $ C^2_{wrt{stdbasis_2}} @>m>M> C^2_{wrt{stdbasis_2}}  @Vscriptstyleidentity Vscriptstyle PV @Vscriptstyleidentity Vscriptstyle PV  C^2_{wrt{B}} @>m>N> C^2_{wrt{B}} $ . The canonical form is $ rep{m}{B, B}=PMP^{ - 1} $ , where $ P^{ - 1} =bigl(rep{identity}{stdbasis_2, B}bigr)^{ - 1} =rep{identity}{B, stdbasis_2} =[r] 1 &1  0 &1 qquad P=(P^{ - 1})^{ - 1} =[r] 1 & - 1  0 &1 $ . and the verification of the matrix calculation is routine. $ [r] 1 & - 1  0 &1 [r] 1 & - 1  1 & - 1 [r] 1 &1  0 &1 = [r] 0 &0  1 &0 $ . This matrix $ [r] 0 &0 &0 &0 &0  1 &0 &0 &0 &0  - 1 &1 &1 & - 1 &1  0 &1 &0 &0 &0  1 &0 & - 1 &1 & - 1 $ . is nilpotent, of index  $ 3 $ . The table tells us this about any string basis: the null space after one map application has dimension two so two basis vectors map directly to zero, the null space after the second application has dimension four so two additional basis vectors map to zero by the second iteration, and the null space after three applications is of dimension five so the remaining one basis vector maps to zero in three hops. $ {ccccccc} vec{beta}_1 &mapsto &vec{beta}_2 &mapsto &vec{beta}_3 &mapsto &zero  vec{beta}_4 &mapsto &vec{beta}_5 &mapsto &zero $ . To produce such a basis, first pick two vectors from $ nullspace{n} $ that form a linearly independent set. $ vec{beta}_3=colvec[r]{0  0  1  1  0} quad vec{beta}_5=colvec[r]{0  0  0  1  1} $ . Then add $ vec{beta}_2, vec{beta}_4innullspace{n^2} $ such that $ n(vec{beta}_2)=vec{beta}_3 $ and $ n(vec{beta}_4)=vec{beta}_5 $ . $ vec{beta}_2=colvec[r]{0  1  0  0  0} quad vec{beta}_4=colvec[r]{0  1  0  1  0} $ . Finish by adding $ vec{beta}_1 $ such that $ n(vec{beta}_1)=vec{beta}_2 $ . $ vec{beta}_1=colvec[r]{1  0  1  0  0} $ .
We have shown that for any homomorphism there are bases $ B $ and  $ D $ such that the matrix representing the map has a block partial - identity form. $ rep{h}{B, D} = {c|c} text{textit{Identity}} &text{textit{Zero}}  hline text{textit{Zero}} &text{textit{Zero}} $ . This representation describes the map as sending $ c_1vec{beta}_1+dots+c_nvec{beta}_n $ to $ c_1vec{delta}_1+dots+c_kvec{delta}_k+zero+dots+zero $ , where $ n $ is the dimension of the domain and $ k $ is the dimension of the range. Under this representation the action of the map is easy to understand because most of the matrix entries are zero. This chapter considers the special case where the domain and codomain are the same. Here we naturally ask for the domain basis and codomain basis to be the same. That is, we want a basis $ B $ so that $ rep{t}{B, B} $ is as simple as possible, where we take `simple' to mean that it has many zeroes. We will find that we cannot always get a matrix having the above block partial - identity form but we will develop a form that comes close, a representation that is nearly diagonal. This chapter requires that we factor polynomials. But many polynomials do not factor over the real numbers; for instance, $ x^2+1 $ does not factor into a product of two linear polynomials with real coefficients; instead it requires complex numbers $ x^2+1=(x - i)(x+i) $ . Consequently in this chapter we shall use complex numbers for our scalars, including entries in vectors and matrices. That is, we shift from studying vector spaces over the real numbers to vector spaces over the complex numbers. Any real number is a complex number and in this chapter most of the examples use only real numbers but nonetheless, the critical theorems require that the scalars be complex. So this first section is a review of complex numbers. In this book our approach is to shift to this more general context of taking scalars to be complex for the pragmatic reason that we must do so in order to move forward. However, the idea of doing vector spaces by taking scalars from a structure other than the real numbers is an interesting and useful one. Delightful presentations that take this approach from the start are in and . textit{This subsection is a review only. For a full development, including proofs, see .} Consider a polynomial $ p(x)=c_nx^n+dots+c_1x+c_0 $ with leading coefficient $ c_nneq 0 $ . We say that it is a degree  $ n $ polynomial. If $ n=0 $ then $ p $ is a constant polynomial $ p(x)=c_0 $ . Constant polynomials that are not the zero polynomial, $ c_0neq 0 $ , have degree zero. We define the zero polynomial to have degree $ - infty $ . Defining the degree of the zero polynomial to be $ - infty $ , which most authors do, allows the equation $ text{degree}(fg)=text{degree}(f)+text{degree}(g) $ to hold for all polynomials. Just as integers have a division operationDash e.g., ` $ 4 $ goes $ 5 $ times into $ 21 $ with remainder $ 1 $ 'Dash so do polynomials. [Division Theorem for Polynomials] Let $ p(x) $ be a polynomial. If $ d(x) $ is a non - zero polynomial then there are quotient and remainder polynomials $ q(x) $ and $ r(x) $ such that $ p(x)=d(x)cdot q(x)+r(x) $ . where the degree of $ r(x) $ is strictly less than the degree of $ d(x) $ . The point of the integer statement ` $ 4 $ goes $ 5 $ times into $ 21 $ with remainder $ 1 $ ' is that the remainder is less than $ 4 $ Dash while $ 4 $ goes $ 5 $ times, it does not go $ 6 $ times. Similarly, the final clause of the polynomial division statement is crucial. If $ p(x)=2x^3 - 3x^2+4x $ and $ d(x)=x^2+1 $ then $ q(x)=2x - 3 $ and $ r(x)=2x+3 $ . Note that $ r(x) $ has a lower degree than does $ d(x) $ . The remainder when $ p(x) $ is divided by $ x - lambda $ is the constant polynomial $ r(x)=p(lambda) $ . The remainder must be a constant polynomial because it is of degree less than the divisor $ x - lambda $ . To determine the constant, take the theorem's divisor $ d(x) $ to be $ x - lambda $ and substitute $ lambda/ $ for $ x $ . If a divisor $ d(x) $ goes into a dividend $ p(x) $ evenly, meaning that $ r(x) $ is the zero polynomial, then $ d(x) $ is a called a factor of $ p(x) $ . Any root of the factor, any $ lambdainRe $ such that $ d(lambda)=0 $ , is a root of $ p(x) $ since $ p(lambda)=d(lambda)cdot q(lambda)=0 $ . If $ lambda $ is a root of the polynomial $ p(x) $ then $ x - lambda $ divides $ p(x) $ evenly, that is, $ x - lambda $ is a factor of $ p(x) $ . By the above corollary $ p(x)=(x - lambda)cdot q(x)+p(lambda) $ . Since $ lambda $ is a root, $ p(lambda)=0 $ so $ x - lambda $ is a factor. A repeated root of a polynomial is a number $ lambda $ such that the polynomial is evenly divisible by $ (x - lambda)^n $ for some power larger than one. The largest such power is called the multiplicity of  $ lambda $ . Finding the roots and factors of a high - degree polynomial can be hard. But for second - degree polynomials we have the quadratic formula: the roots of $ ax^2+bx+c $ are these $ lambda_1=frac{ - b+sqrt{b^2 - 4ac}}{2a} qquad lambda_2=frac{ - b - sqrt{b^2 - 4ac}}{2a} $ . (if the discriminant $ b^2 - 4ac $ is negative then the polynomial has no real number roots). A polynomial that cannot be factored into two lower - degree polynomials with real number coefficients is said to be irreducible over the reals. Any constant or linear polynomial is irreducible over the reals. A quadratic polynomial is irreducible over the reals if and only if its discriminant is negative. No cubic or higher - degree polynomial is irreducible over the reals. Any polynomial with real coefficients factors into a product of linear and irreducible quadratic polynomials with real coefficients. That factorization is unique; any two factorizations have the same factors raised to the same powers. Note the analogy with the prime factorization of integers. In both cases the uniqueness clause is very useful. Because of uniqueness we know, without multiplying them out, that $ (x+3)^2(x^2+1)^3 $ does not equal $ (x+3)^4(x^2+x+1)^2 $ . By uniqueness, if $ c(x)=m(x)cdot q(x) $ then where $ c(x)=(x - 3)^2(x+2)^3 $ and $ m(x)=(x - 3)(x+2)^2 $ , we know that $ q(x)=(x - 3)(x+2) $ . While $ x^2+1 $ has no real roots and so doesn't factor over the real numbers, if we imagine a rootDash traditionally denoted $ i $ , so that $ i^2+1=0 $ Dash then $ x^2+1 $ factors into a product of linears, $ (x - i)(x+i) $ . When we adjoin this root $ i $ to the reals and close the new system with respect to addition and multiplication then we have the complex numbers, $ C=set{a+bisuchthat text{ $ a, binR $ and $ i^2= - 1 $ }} $ . For a scalar $ zinC $ , where $ z=a+bi $ , we call $ a $ the real part of  $ z $ , and  $ b $ the imaginary part. We often picture complex numbers on the complex plane, with $ a $ plotted on the real axis, the horizontal axis, and $ b $ plotted on the imaginary axis, the vertical axis. Note that the distance of the point from the origin is the length, $ |a+bi|=sqrt{a^2+b^2} $ . Recall the definitions of the complex number addition and scalar multiplication $ (a+bi), +, (c+di)=(a+c)+(b+d)i qquad rcdot(a+bi)=(ra)+(rb)i $ . (and consequently subtraction is $ (a+bi) - (c+di)=(a - c)+(b - d)i $ ). Recall also the definition of complex - complex multiplication. $ (a+bi)(c+di) &=ac+adi+bci+bd( - 1)  &=(ac - bd)+(ad+bc)i $ . For instance, $ (2 - 3i)(4 - 0.5i)=(vi)5 - 13i $ . Over the complex numbers, any quadratic polynomial factors into linears. $ ax^2+bx+c= acdot big(x - frac{ - b+sqrt{b^2 - 4ac}}{2a}big) cdot big(x - frac{ - b - sqrt{b^2 - 4ac}}{2a}big) $ . The second degree polynomial $ x^2+x+1 $ factors over the complex numbers into the product of two first degree polynomials. $ big(x - frac{ - 1+sqrt{ - 3}}{2}big) big(x - frac{ - 1 - sqrt{ - 3}}{2}big) = big(x - ( - frac{1}{2}+frac{sqrt{3}}{2}i)big) big(x - ( - frac{1}{2} - frac{sqrt{3}}{2}i)big) $ . In $ C $ , in contrast with the reals, there are no irreducible quadratics. All polynomials factor completely into linears. [Fundamental Theorem of Algebra] hspace*{0em plus2em} Polynomials with complex coefficients factor into linear polynomials with complex coefficients. The factorization is unique. With the above definitions for the complex numbers, all of the operations that we've used for real vector spaces carry over unchanged to vector spaces with complex scalars. Matrix multiplication is the same, although the computation can involve more arithmetic. 1+1i &2 - 0i  i & - 2+3i 1+0i &1 - 0i  3i & - i  &= (1+1i)cdot(1+0i)+(2 - 0i)cdot(3i) &(1+1i)cdot(1 - 0i)+(2 - 0i)cdot( - i)  (i)cdot(1+0i)+( - 2+3i)cdot(3i) &(i)cdot(1 - 0i)+( - 2+3i)cdot( - i)  &= 1+7i &1 - 1i  - 9 - 5i &3+3i We shall carry over unchanged from the previous chapters everything that we can. For instance, we shall call this $ sequence{colvec{1+0i  0+0i  vdots  0+0i}, dots, colvec{0+0i  0+0i  vdots  1+0i}} $ . the standard basis for $ C^n $ as a vector space over $ C $ and again denote it $ stdbasis_n $ . Another example is that $ polyspace_n $ will be the vector space of degree  $ n $ polynomials with coefficients that are complex.
The prior section shows that a linear map is determined by its action on a basis. The equation $ h(vec{v}) =h(c_1cdotvec{beta}_1+dots+c_ncdotvec{beta}_n) =c_1cdot h(vec{beta}_1)+dots +c_ncdot h(vec{beta}_n) tag*{} $ . describes how we get the value of the map on any vector $ vec{v} $ by starting from the value of the map on the vectors $ vec{beta}_i $ in a basis and extending linearly. This section gives a convenient scheme based on matrices to use the representations of $ h(vec{beta}_1) $ , ldots, $ h(vec{beta}_n) $ to compute, from the representation of a vector in the domain $ rep{vec{v}}{B} $ , the representation of that vector's image in the codomain $ rep{h(vec{v})}{D} $ . For the spaces $ Re^2 $ and $ Re^3 $ fix these bases. $ B=sequence{ colvec[r]{2  0}, colvec[r]{1  4}} qquad D=sequence{ colvec[r]{1  0  0}, colvec[r]{0  - 2  0}, colvec[r]{1  0  1}} $ . Consider the map $ map{h}{Re^2}{Re^3} $ that is determined by this association. $ colvec[r]{2  0} mapsunder{h} colvec[r]{1  1  1} qquad colvec[r]{1  4} mapsunder{h} colvec[r]{1  2  0} $ . To compute the action of this map on any vector at all from the domain we first represent the vector $ h(vec{beta}_1) $ $ colvec[r]{1  1  1}= 0colvec[r]{1  0  0} - frac{1}{2}colvec[r]{0  - 2  0} +1colvec[r]{1  0  1} qquad rep{ h(vec{beta}_1) }{D}=colvec[r]{0  - 1/2  1}_D $ . and $ h(vec{beta}_2) $ . $ colvec[r]{1  2  0}= 1colvec[r]{1  0  0} - 1colvec[r]{0  - 2  0} +0colvec[r]{1  0  1} qquad rep{ h(vec{beta}_2) }{D}=colvec[r]{1  - 1  0}_D $ . With these, for any member $ vec{v} $ of the domain we can compute $ h(vec{v}) $ . $ h(vec{v}) &=h(c_1cdot colvec[r]{2  0}+c_2cdot colvec[r]{1  4})  &=c_1cdot h(colvec[r]{2  0})+c_2cdot h(colvec[r]{1  4})  &=c_1cdot ( 0colvec[r]{1  0  0} ! - frac{1}{2}colvec[r]{0  - 2  0} !+1colvec[r]{1  0  1}! ) +c_2cdot ( 1colvec[r]{1  0  0} ! - 1colvec[r]{0  - 2  0} !+0colvec[r]{1  0  1}! )  &=(0c_1+1c_2)cdot colvec[r]{1  0  0} +( - frac{1}{2}c_1 - 1c_2)cdot colvec[r]{0  - 2  0} +(1c_1+0c_2)cdot colvec[r]{1  0  1} $ . Thus, For instance, We express computations like the one above with a matrix notation. $ [r] 0 &1  - 1/2 & - 1  1 &0 _{B, D} colvec{c_1  c_2}_B = colvec{0c_1+1c_2  ( - 1/2)c_1 - 1c_2  1c_1+0c_2}_D $ . In the middle is the argument $ vec{v} $ to the map, represented with respect to the domain's basis $ B $ by the column vector with components $ c_1 $ and $ c_2 $ . On the right is the value of the map on that argument $ h(vec{v}) $ , represented with respect to the codomain's basis $ D $ . The matrix on the left is the new thing. We will use it to represent the map and we will think of the above equation as representing an application of the map to the matrix. That matrix consists of the coefficients from the vector on the right, $ 0 $ and $ 1 $ from the first row, $ - 1/2 $ and $ - 1 $ from the second row, and $ 1 $ and $ 0 $ from the third row. That is, we make it by adjoining the vectors representing the $ h(vec{beta}_i) $ 's. $ left({c|c} vdots &vdots  rep{, h(vec{beta}_1), }{D} &rep{, h(vec{beta}_2), }{D}  vdots &vdots right) $ . Suppose that $ V $ and $ W $ are vector spaces of dimensions $ n $ and $ m $ with bases $ B $ and $ D $ , and that $ map{h}{V}{W} $ is a linear map. If $ rep{h( vec{beta}_1 )}{D} = colvec{h_{1, 1}  h_{2, 1}  vdots  h_{m, 1}}_D quadldotsquad rep{h( vec{beta}_n )}{D} = colvec{h_{1, n}  h_{2, n}  vdots  h_{m, n}}_D $ . then $ rep{h}{B, D}=generalmatrix{h}{n}{m}_{B, D} $ . is the matrix representation of $ h $ with respect to $ B, D $ . noindent In that matrix the number of columns  $ n $ is the dimension of the map's domain while the number of rows  $ m $ is the dimension of the codomain. As with the notation for represenation of a vector, the $ mbox{Rep}_{B, D} $ notation here is not standard. The most common alternative is $ [h]_{B, D} $ . We use lower case letters for a map, upper case for the matrix, and lower case again for the entries of the matrix. Thus for the map $ h $ , the matrix representing it is $ H $ , with entries $ h_{i, j} $ . If $ map{h}{Re^3}{polyspace_1} $ is $ colvec{a_1  a_2  a_3} mapsunder{h} (2a_1+a_2)+( - a_3)x $ . then where $ B= sequence{colvec[r]{0  0  1}, colvec[r]{0  2  0}, colvec[r]{2  0  0} } qquad D= sequence{1+x, - 1+x} $ . the action of $ h $ on $ B $ is this. $ colvec[r]{0  0  1}mapsunder{h} - x qquad colvec[r]{0  2  0}mapsunder{h}2 qquad colvec[r]{2  0  0}mapsunder{h}4 $ . A simple calculation $ rep{ - x}{D}=colvec[r]{ - 1/2  - 1/2}_D quad rep{2}{D}=colvec[r]{1  - 1}_D quad rep{4}{D}=colvec[r]{2  - 2}_D $ . shows that this is the matrix representing $ h $ with respect to the bases. $ rep{h}{B, D} = [r] - 1/2 &1 &2  - 1/2 & - 1 & - 2 _{B, D} $ . Assume that $ V $ and $ W $ are vector spaces of dimensions $ n $ and $ m $ with bases $ B $ and $ D $ , and that $ map{h}{V}{W} $ is a linear map. If $ h $ is represented by $ rep{h}{B, D}=generalmatrix{h}{n}{m}_{B, D} $ . and $ vec{v}in V $ is represented by $ rep{vec{v}}{B}=colvec{c_1  c_2  vdots  c_n}_B $ . then the representation of the image of $ vec{v} $ is this. $ rep{, h(vec{v}) , }{D} = colvec{h_{1, 1}c_1+h_{1, 2}c_2+dots+h_{1, n}c_n  h_{2, 1}c_1+h_{2, 2}c_2+dots+h_{2, n}c_n  vdots  h_{m, 1}c_1+h_{m, 2}c_2+dots+h_{m, n}c_n}_D $ . This formalizes $ X $ . See $ X $ . The matrix - vector product of a $ nbym{m}{n} $ matrix and a $ nbym{n}{1} $ vector is this. $ generalmatrix{a}{n}{m} colvec{c_1  vdots  c_n} = colvec{a_{1, 1}c_1+dots+a_{1, n}c_n  a_{2, 1}c_1+dots+a_{2, n}c_n  vdots  a_{m, 1}c_1+dots+a_{m, n}c_n} $ . Briefly, application of a linear map is represented by the matrix - vector product of the map's representative and the vector's representative. $ X $ is not surprising, because we chose the matrix representative in $ X $ precisely to make the theorem trueDash if the theorem were not true then we would adjust the definition to make it so. Nonetheless, we need the verification. For the matrix from $ X $ we can calculate where that map sends this vector. $ vec{v}=colvec[r]{4  1  0} $ . With respect to the domain basis $ B $ the representation of this vector is $ rep{vec{v}}{B}=colvec[r]{0  1/2  2}_B $ . and so the matrix - vector product gives the representation of the value $ h(vec{v}) $ with respect to the codomain basis $ D $ . $ rep{h(vec{v})}{D} &=[r] - 1/2 &1 &2  - 1/2 & - 1 & - 2 _{B, D} colvec[r]{0  1/2  2}_B  &=colvec{( - 1/2)cdot 0+1cdot (1/2) + 2cdot 2  ( - 1/2)cdot 0 - 1cdot (1/2) - 2cdot 2}_D =colvec[r]{9/2  - 9/2}_D $ . To find $ h(vec{v}) $ itself, not its representation, take $ (9/2)(1+x) - (9/2)( - 1+x)=9 $ . Let $ map{pi}{Re^3}{Re^2} $ be projection onto the $ xy $ - plane. To give a matrix representing this map, we first fix some bases. $ B=sequence{ colvec[r]{1  0  0}, colvec[r]{1  1  0}, colvec[r]{ - 1  0  1} } qquad D=sequence{ colvec[r]{2  1}, colvec[r]{1  1} } $ . For each vector in the domain's basis, find its image under the map. $ colvec[r]{1  0  0}mapsunder{pi}colvec[r]{1  0} quad colvec[r]{1  1  0}mapsunder{pi}colvec[r]{1  1} quad colvec[r]{ - 1  0  1}mapsunder{pi}colvec[r]{ - 1  0} $ . Then find the representation of each image with respect to the codomain's basis. $ rep{colvec[r]{1  0}}{D}=colvec[r]{1  - 1} quad rep{colvec[r]{1  1}}{D}=colvec[r]{0  1} quad rep{colvec[r]{ - 1  0}}{D}=colvec[r]{ - 1  1} $ . Finally, adjoining these representations gives the matrix representing $ pi $ with respect to $ B, D $ . $ rep{pi}{B, D} =[r] 1 &0 & - 1  - 1 &1 &1 _{B, D} $ . We can illustrate $ X $ by computing the matrix - vector product representing this action by the projection map. $ pi( colvec[r]{2  2  1} )=colvec[r]{2  2} $ . Represent the domain vector with respect to the domain's basis $ rep{colvec[r]{2  2  1}}{B}= colvec[r]{1  2  1}_B $ . to get this matrix - vector product. $ rep{ , pi(colvec[r]{2  2  1}), }{D}= [r] 1 &0 & - 1  - 1 &1 &1 _{B, D} colvec[r]{1  2  1}_B = colvec[r]{0  2}_D $ . Expanding this into a linear combination of vectors from $ D $ $ 0cdotcolvec[r]{2  1} +2cdotcolvec[r]{1  1} = colvec[r]{2  2} $ . checks that the map's action is indeed reflected in the operation of the matrix. We will sometimes compress these three displayed equations into one. $ colvec[r]{2  2  1}=colvec[r]{1  2  1}_B ;overset{h}{underset{H}{longmapsto}}; colvec[r]{0  2}_D=colvec[r]{2  2} $ . We now have two ways to compute the effect of projection, the straightforward formula that drops each three - tall vector's third component to make a two - tall vector, and the above formula that uses representations and matrix - vector multiplication. The second way may seem complicated compared to the first, but it has advantages. The next example shows that for some maps this new scheme simplifies the formula. To represent a rotation map $ map{t_{theta}}{Re^2}{Re^2} $ that turns all vectors in the plane counterclockwise through an angle $ theta $ we start by fixing the standard bases $ stdbasis_2 $ for both the domain and codomain basis, Now find the image under the map of each vector in the domain's basis. $ colvec[r]{1  0}mapsunder{t_theta}colvec{costheta  sintheta} qquad colvec[r]{0  1}mapsunder{t_theta}colvec{ - sintheta  costheta} tag{ $ * $ } $ . Represent these images with respect to the codomain's basis. Because this basis is $ stdbasis_2 $ , vectors represent themselves. Adjoin the representations to get the matrix representing the map. $ rep{t_theta}{stdbasis_2, stdbasis_2} = costheta & - sintheta  sintheta &costheta $ . The advantage of this scheme is that we get a formula for the image of any vector at all just by knowing in ( $ * $ ) how to represent the image of the two basis vectors. For instance, here we rotate a vector by $ theta=pi/6 $ . $ colvec[r]{3  - 2} = colvec[r]{3  - 2}_{stdbasis_2}!mapsunder{t_{pi/6}}; [r] sqrt{3}/2 & - 1/2  1/2 &sqrt{3}/2 colvec[r]{3  - 2} approx colvec[r]{(iii)598  - 0.232}_{stdbasis_2} !!= colvec[r]{(iii)598  - 0.232} $ . More generally, we have a formula for rotation by $ theta=pi/6 $ . $ colvec[r]{x  y} mapsunder{t_{pi/6}}; [r] sqrt{3}/2 & - 1/2  1/2 &sqrt{3}/2 colvec[r]{x  y} = colvec[r]{(sqrt{3}/2)x - (1/2)y  (1/2)x+(sqrt{3}/2)y } $ . In the definition of matrix - vector product the width of the matrix equals the height of the vector. Hence, this product is not defined. $ [r] 1 &0 &0  4 &3 &1 colvec[r]{1  0} $ . It is undefined for a reason: the three - wide matrix represents a map with a three - dimensional domain while the two - tall vector represents a member of a two - dimensional space. So the vector cannot be in the domain of the map. Nothing in $ X $ forces us to view matrix - vector product in terms of representations. We can get some insights by focusing on how the entries combine. A good way to view matrix - vector product is that it is formed from the dot products of the rows of the matrix with the column vector. $ &vdots  a_{i, 1} &a_{i, 2} &ldots &a_{i, n}  &vdots colvec{c_1  c_2  vdots  c_n} = colvec{vdots  a_{i, 1}c_1+a_{i, 2}c_2+cdots+a_{i, n}c_n  vdots} $ . Looked at in this row - by - row way, this new operation generalizes dot product. We can also view the operation column - by - column. $ generalmatrix{h}{n}{m} colvec{c_1  c_2  vdots  c_n} &=colvec{h_{1, 1}c_1+h_{1, 2}c_2+dots+h_{1, n}c_n  h_{2, 1}c_1+h_{2, 2}c_2+dots+h_{2, n}c_n  vdots  h_{m, 1}c_1+h_{m, 2}c_2+dots+h_{m, n}c_n}  &=c_1colvec{h_{1, 1}  h_{2, 1}  vdots  h_{m, 1}} +dots +c_ncolvec{h_{1, n}  h_{2, n}  vdots  h_{m, n}} $ . The result is the columns of the matrix weighted by the entries of the vector. $ [r] 1 &0 & - 1  2 &0 &3 colvec[r]{2  - 1  1} = 2colvec[r]{1  2} - 1colvec[r]{0  0} +1colvec[r]{ - 1  3} = colvec[r]{1  7} $ . This way of looking at matrix - vector product brings us back to the objective stated at the start of this section, to compute $ h(c_1vec{beta}_1+dots+c_nvec{beta}_n) $ as $ c_1h(vec{beta}_1)+dots+c_nh(vec{beta}_n) $ . We began this section by noting that the equality of these two enables us to compute the action of $ h $ on any argument knowing only $ h(vec{beta}_1) $ , ldots, $ h(vec{beta}_n) $ . We have developed this into a scheme to compute the action of the map by taking the matrix - vector product of the matrix representing the map with the vector representing the argument. In this way, with respect to any bases, for any linear map there is a matrix representation. The next subsection will show the converse, that if we fix bases then for any matrix there is an associated linear map. The prior subsection shows that the action of a linear map $ h $ is described by a matrix $ H $ , with respect to appropriate bases, in this way. $ vec{v}=colvec{v_1  vdots  v_n}_B quadoverset{h}{underset{H}{longmapsto}}quad h(vec{v})= colvec{h_{1, 1}v_1+dots+h_{1, n}v_n  vdots  h_{m, 1}v_1+dots+h_{m, n}v_n}_D tag{ $ * $ } $ . Here we will show the converse, that each matrix represents a linear map. So we start with a matrix $ H=generalmatrix{h}{n}{m} $ . and we will describe how it defines a map  $ h $ . We require that the map be represented by the matrix so first note that in ( $ * $ ) the dimension of the map's domain is the number of columns  $ n $ of the matrix and the dimension of the codomain is the number of rows  $ m $ . Thus, for $ h $ 's domain fix an $ n $ - dimensional vector space $ V $ and for the codomain fix an $ m $ - dimensional space $ W $ . Also fix bases $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ and $ D=sequence{vec{delta}_1, dots, vec{delta}_m} $ for those spaces. Now let $ map{h}{V}{W} $ be: where $ vec{v} $ in the domain has the representation $ rep{vec{v}}{B} =colvec{v_1  vdots  v_n}_B $ . then its image $ h(vec{v}) $ is the member of the codomain with this representation. $ rep{, h(vec{v}), }{D} =colvec{h_{1, 1}v_1+dots+h_{1, n}v_n  vdots  h_{m, 1}v_1+dots+h_{m, n}v_n}_D $ . That is, to compute the action of $ h $ on any $ vec{v}in V $ , first express $ vec{v} $ with respect to the basis $ vec{v}=v_1vec{beta}_1+dots+v_nvec{beta}_n $ and then $ h(vec{v})= (h_{1, 1}v_1+dots+h_{1, n}v_n)cdotvec{delta}_1 +dots+ (h_{m, 1}v_1+dots+h_{m, n}v_n)cdotvec{delta}_m $ . Above we have made some choices; for instance $ V $ can be any $ n $ - dimensional space and $ B $ could be any basis for $ V $ , so $ H $ does not define a unique function. However, note once we have fixed $ V $ , $ B $ , $ W $ , and $ D $ then $ h $ is well - defined since $ vec{v} $ has a unique representation with respect to the basis $ B $ and the calculation of $ vec{w} $ from its representation is also uniquely determined. Consider this matrix. $ H= 1 &2  3 &4  5 &6 $ . It is $ nbym{3}{2} $ so any map that it defines must carry a dimension  $ 2 $ domain to a dimension  $ 3 $ codomain. We can choose the domain and codomain to be $ Re^2 $ and $ polyspace_2 $ , with these bases. $ B=sequence{colvec{1  1}, colvec[r]{1  - 1}} qquad D=sequence{x^2, x^2+x, x^2+x+1} $ . Then let $ map{h}{Re^2}{polyspace_2} $ be the function defined by  $ H $ . We will compute the image under $ h $ of this member of the domain. $ vec{v}=colvec[r]{ - 3  2} $ . The computation is straightforward. $ rep{h(vec{v})}{D} =Hcdotrep{vec{v}}{B} = 1 &2  3 &4  5 &6 colvec{ - 1/2  - 5/2} =colvec{ - 11/2  - 23/2  - 35/2} $ . From its representation, computation of $ h(vec{v}) $ is routine $ ( - 11/2)(x^2) - (23/2)(x^2+x) - (35/2)(x^2+x+1) =( - 69/2)x^2 - (58/2)x - (35/2) $ . Any matrix represents a homomorphism between vector spaces of appropriate dimensions, with respect to any pair of bases. We must check that for any matrix $ H $ and any domain and codomain bases $ B, D $ , the defined map $ h $ is linear. If $ vec{v}, vec{u}in V $ are such that $ rep{vec{v}}{B}=colvec{v_1  vdots  v_n} qquad rep{vec{u}}{B}=colvec{u_1  vdots  u_n} $ . and $ c, dinRe $ then the calculation $ h(cvec{v}+dvec{u}) &=bigl(h_{1, 1}(cv_1+du_1)+dots+ h_{1, n}(cv_n+du_n)bigr)cdotvec{delta}_1+  & hbox{}quadcdots+bigl(h_{m, 1}(cv_1+du_1)+dots +h_{m, n}(cv_n+du_n)bigr)cdotvec{delta}_m  &=ccdot h(vec{v})+dcdot h(vec{u}) $ . supplies that check. Even if the domain and codomain are the same, the map that the matrix represents depends on the bases that we choose. If $ H= [r] 1 &0  0 &0 , quad B_1=D_1=sequence{colvec[r]{1  0}, colvec[r]{0  1} }, quadtext{and}quad B_2=D_2=sequence{colvec[r]{0  1}, colvec[r]{1  0} }, $ . then $ map{h_1}{Re^2}{Re^2} $ represented by $ H $ with respect to $ B_1, D_1 $ maps $ colvec{c_1  c_2} =colvec{c_1  c_2}_{B_1} quad mapsto quad colvec{c_1  0}_{D_1} = colvec{c_1  0} $ . while $ map{h_2}{Re^2}{Re^2} $ represented by $ H $ with respect to $ B_2, D_2 $ is this map. $ colvec{c_1  c_2} =colvec{c_2  c_1}_{B_2} quad mapsto quad colvec{c_2  0}_{D_2} = colvec{0  c_2} $ . These are different functions. The first is projection onto the $ x $ - axis while the second is projection onto the $ y $ - axis. This result means that when convenient we can work solely with matrices, just doing the computations without having to worry whether a matrix of interest represents a linear map on some pair of spaces. When we are working with a matrix but we do not have particular spaces or bases in mind then we can take the domain and codomain to be $ Re^n $ and $ Re^m $ , with the standard bases. This is convenient because with the standard bases vector representation is transparentDash the representation of $ vec{v} $ is $ vec{v} $ . (In this case the column space of the matrix equals the range of the map and consequently the column space of $ H $ is often denoted by $ rangespace{H} $ .) Given a matrix, to come up with an associated map we can choose among many domain and codomain spaces, and many bases for those. So a matrix can represent many maps. We finish this section by illustrating how the matrix can give us information about the associated maps. The rank of a matrix equals the rank of any map that it represents. Suppose that the matrix $ H $ is $ nbym{m}{n} $ . Fix domain and codomain spaces $ V $ and $ W $ of dimension $ n $ and  $ m $ with bases $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ and $ D $ . Then $ H $ represents some linear map $ h $ between those spaces with respect to these bases whose range space $ set{h(vec{v})suchthat vec{v}in V} &=set{h(c_1vec{beta}_1+dots+c_nvec{beta}_n) suchthat c_1, dots, c_ninRe}  &=set{c_1h(vec{beta}_1)+dots+c_nh(vec{beta}_n) suchthat c_1, dots, c_ninRe} $ . is the span $ spanof{set{h(vec{beta}_1), dots, h(vec{beta}_n)}} $ . The rank of the map $ h $ is the dimension of this range space. The rank of the matrix is the dimension of its column space, the span of the set of its columns $ spanof{, set{rep{h(vec{beta}_1)}{D}, dots, rep{h(vec{beta}_n)}{D}}, } $ . To see that the two spans have the same dimension, recall from the proof of Lemma I. that if we fix a basis then representation with respect to that basis gives an isomorphism $ map{mbox{Rep}_D}{W}{Re^m} $ . Under this isomorphism there is a linear relationship among members of the range space if and only if the same relationship holds in the column space, e.g, $ zero=c_1cdot h(vec{beta}_1)+dots+c_ncdot h(vec{beta}_n) $ if and only if $ zero=c_1cdotrep{h(vec{beta}_1)}{D}+dots+c_ncdotrep{h(vec{beta}_n)}{D} $ . Hence, a subset of the range space is linearly independent if and only if the corresponding subset of the column space is linearly independent. Therefore the size of the largest linearly independent subset of the range space equals the size of the largest linearly independent subset of the column space, and so the two spaces have the same dimension. That settles the apparent ambiguity in our use of the same word `rank' to apply both to matrices and to maps. Any map represented by $ [r] 1 &2 &2  1 &2 &1  0 &0 &3  0 &0 &2 $ . must have three - dimensional domain and a four - dimensional codomain. In addition, because the rank of this matrix is two (we can spot this by eye or get it with Gauss's Method), any map represented by this matrix has a two - dimensional range space. Let $ h $ be a linear map represented by a matrix $ H $ . Then $ h $ is onto if and only if the rank of $ H $ equals the number of its rows, and $ h $ is one - to - one if and only if the rank of $ H $ equals the number of its columns. For the onto half, the dimension of the range space of $ h $ is the rank of $ h $ , which equals the rank of $ H $ by the theorem. Since the dimension of the codomain of $ h $ equals the number of rows in $ H $ , if the rank of $ H $ equals the number of rows then the dimension of the range space equals the dimension of the codomain. But a subspace with the same dimension as its superspace must equal that superspace (because any basis for the range space is a linearly independent subset of the codomain whose size is equal to the dimension of the codomain, and thus this basis for the range space must also be a basis for the codomain). For the other half, a linear map is one - to - one if and only if it is an isomorphism between its domain and its range, that is, if and only if its domain has the same dimension as its range. The number of columns in $ H $ is the dimension of $ h $ 's domain and by the theorem the rank of $ H $ equals the dimension of $ h $ 's range. A linear map that is one - to - one and onto is nonsingular , otherwise it is singular . That is, a linear map is nonsingular if and only if it is an isomorphism. Some authors use `nonsingular' as a synonym for one - to - one while others use it the way that we have here. The difference is slight because any map is onto its range space, so a one - to - one map is an isomorphism with its range. In the first chapter we defined a matrix to be nonsingular if it is square and is the matrix of coefficients of a linear system with a unique solution. The next result justifies our dual use of the term. A nonsingular linear map is represented by a square matrix. A square matrix represents nonsingular maps if and only if it is a nonsingular matrix. Thus, a matrix represents isomorphisms if and only if it is square and nonsingular. Assume that the map $ map{h}{V}{W} $ is nonsingular. $ X $ says that for any matrix $ H $ representing that map, because $ h $ is onto the number of rows of $ H $ equals the rank of $ H $ , and because $ h $ is one - to - one the number of columns of  $ H $ is also equal to the rank of  $ H $ . Hence $ H $ is square. Next assume that $ H $ is square, $ nbyn{n} $ . The matrix  $ H $ is nonsingular if and only if its row rank is  $ n $ , which is true if and only if $ H $ 's rank is  $ n $ by Theorem Two.III., which is true if and only if $ h $ 's rank is  $ n $ by $ X $ , which is true if and only if $ h $ is an isomorphism by Theorem I.. (This last holds because the domain of $ h $ is $ n $ - dimensional as it is the number of columns in $ H $ .) Any map from $ Re^2 $ to $ polyspace_1 $ represented with respect to any pair of bases by $ [r] 1 &2  0 &3 $ . is nonsingular because this matrix has rank two. Any map $ map{g}{V}{W} $ represented by $ [r] 1 &2  3 &6 $ . is singular because this matrix is singular. We've now seen that the relationship between maps and matrices goes both ways: for a particular pair of bases, any linear map is represented by a matrix and any matrix describes a linear map. That is, by fixing spaces and bases we get a correspondence between maps and matrices. In the rest of this chapter we will explore this correspondence. For instance, we've defined for linear maps the operations of addition and scalar multiplication and we shall see what the corresponding matrix operations are. We shall also see the matrix operation that represent the map operation of composition. And, we shall see how to find the matrix that represents a map's inverse.
The definition of isomorphism has two conditions. In this section we will consider the second one. We will study maps that are required only to preserve structure, maps that are not also required to be correspondences. Experience shows that these maps are tremendously useful. For one thing we shall see in the second subsection below that while isomorphisms describe how spaces are the same, we can think of these maps as describing how spaces are alike. A function between vector spaces $ map{h}{V}{W} $ that preserves addition and scalar multiplication is a homomorphism } or linear map}. The projection map $ map{pi}{Re^3}{Re^2} $ $ colvec{x  y  z} mapsunder{pi} colvec{x  y} $ . is a homomorphism. It preserves addition $ pi(colvec{x_1  y_1  z_1}!+!colvec{x_2  y_2  z_2}) = pi(colvec{x_1+x_2  y_1+y_2  z_1+z_2}) = colvec{x_1+x_2  y_1+y_2} = pi(colvec{x_1  y_1  z_1}) + pi(colvec{x_2  y_2  z_2}) $ . and scalar multiplication. $ pi(rcdotcolvec{x_1  y_1  z_1}) = pi(colvec{rx_1  ry_1  rz_1}) = colvec{rx_1  ry_1} = rcdotpi(colvec{x_1  y_1  z_1}) $ . This is not an isomorphism since it is not one - to - one. For instance, both $ zero $ and $ vec{e}_3 $ in $ Re^3 $ map to the zero vector in $ Re^2 $ . The domain and codomain can be other than spaces of column vectors. Both of these are homomorphisms; the verifications are straightforward. $ map{f_1}{polyspace_2}{polyspace_3} $ given by $ a_0+a_1x+a_2x^2 ;mapsto; a_0x+(a_1/2)x^2+(a_2/3)x^3 $ . $ map{f_2}{M_{nbyn{2}}}{Re} $ given by $ a &b  c &d mapsto a+d $ . Between any two spaces there is a zero homomorphism, mapping every vector in the domain to the zero vector in the codomain. We shall use the two terms `homomorphism' and `linear map' interchangably. These two suggest why we say `linear map'. The map $ map{g}{Re^3}{Re} $ given by $ colvec{x  y  z} mapsunder{g} 3x+2y - (iv)5z $ . is linear, that is, is a homomorphism. The check is easy. In contrast, the map $ map{hat{g}}{Re^3}{Re} $ given by $ colvec{x  y  z} mapsunder{hat{g}} 3x+2y - (iv)5z+1 $ . is not linear. To show this we need only produce a single linear combination that the map does not preserve. Here is one. $ hat{g}(colvec[r]{0  0  0}+colvec[r]{1  0  0})=4 qquad hat{g}(colvec[r]{0  0  0}) +hat{g}(colvec[r]{1  0  0})=5 $ . The first of these two maps $ map{t_1, t_2}{Re^3}{Re^2} $ is linear while the second is not. $ colvec{x  y  z} mapsunder{t_1} colvec{5x - 2y  x+y} qquad colvec{x  y  z} mapsunder{t_2} colvec{5x - 2y  xy} $ . Finding a linear combination that the second map does not preserve is easy. So one way to think of `homomorphism' is that we are generalizing `isomorphism' (by dropping the condition that the map is a correspondence), motivated by the observation that many of the properties of isomorphisms have only to do with the map's structure - preservation property. The next two results are examples of this motivation. In the prior section we saw a proof for each that only uses preservation of addition and preservation of scalar multiplication, and therefore applies to homomorphisms. A linear map sends the zero vector to the zero vector. The following are equivalent for any map $ map{f}{V}{W} $ between vector spaces. $ f $ is a homomorphism $ f(c_1cdotvec{v}_1+c_2cdotvec{v}_2) =c_1cdot f(vec{v}_1)+c_2cdot f(vec{v}_2) $ for any $ c_1, c_2inRe $ and $ vec{v}_1, vec{v}_2in V $ $ f(c_1cdotvec{v}_1+dots+c_ncdotvec{v}_n) =c_1cdot f(vec{v}_1)+dots+c_ncdot f(vec{v}_n) $ for any $ c_1, dots, c_ninRe $ and $ vec{v}_1, ldots, vec{v}_nin V $ The function $ map{f}{Re^2}{Re^4} $ given by $ colvec{x  y} mapsunder{f} colvec{x/2  0  x+y  3y} $ . is linear since it satisfies item (2). $ colvec{r_1(x_1/2)+r_2(x_2/2)  0  r_1(x_1+y_1)+r_2(x_2+y_2)  r_1(3y_1)+r_2(3y_2)} = r_1colvec{x_1/2  0  x_1+y_1  3y_1} + r_2colvec{x_2/2  0  x_2+y_2  3y_2} $ . However, some things that hold for isomorphisms fail to hold for homomorphisms. One example is in the proof of Lemma I., which shows that an isomorphism between spaces gives a correspondence between their bases. Homomorphisms do not give any such correspondence; $ X $ shows this and another example is the zero map between two nontrivial spaces. Instead, for homomorphisms we have a weaker but still very useful result. A homomorphism is determined by its action on a basis: if $ V $ is a vector space with basis $ sequence{vec{beta}_1, dots, vec{beta}_n} $ , if $ W $ is a vector space, and if $ vec{w}_1, dots, vec{w}_nin W $ (these codomain elements need not be distinct) then there exists a homomorphism from $ V $ to $ W $ sending each $ vec{beta}_i $ to $ vec{w}_i $ , and that homomorphism is unique. For any input $ vec{v}in V $ let its expression with respect to the basis be $ vec{v}=c_1vec{beta}_1+dots+c_nvec{beta}_n $ . Define the associated output by using the same coordinates $ h(vec{v})=c_1vec{w}_1+dots+c_nvec{w}_n $ . This is well defined because, with respect to the basis, the representation of each domain vector $ vec{v} $ is unique. This map is a homomorphism because it preserves linear combinations: where $ vec{v_1}=c_1vec{beta}_1+cdots+c_nvec{beta}_n $ and $ vec{v_2}=d_1vec{beta}_1+cdots+d_nvec{beta}_n $ , here is the calculation. $ h(r_1vec{v}_1+r_2vec{v}_2) &=h(, (r_1c_1+r_2d_1)vec{beta}_1+dots+(r_1c_n+r_2d_n)vec{beta}_n, )  &=(r_1c_1+r_2d_1)vec{w}_1+dots+(r_1c_n+r_2d_n)vec{w}_n  &=r_1h(vec{v}_1)+r_2h(vec{v}_2) $ . This map is unique because if $ map{hat{h}}{V}{W} $ is another homomorphism satisfying that $ hat{h}(vec{beta}_i)=vec{w}_i $ for each $ i $ then $ h $ and $ hat{h} $ have the same effect on all of the vectors in the domain. hat{h}(vec{v}) =hat{h}(c_1vec{beta}_1+dots+c_nvec{beta}_n) =c_1 hat{h}(vec{beta}_1)+dots+c_n hat{h}(vec{beta}_n)  =c_1vec{w}_1+dots+c_nvec{w}_n =h(vec{v}) They have the same action so they are the same function. Let $ V $ and  $ W $ be vector spaces and let $ B=sequence{vec{beta}_1, ldots, vec{beta}_n} $ be a basis for  $ V $ . A function defined on that basis $ map{f}{B}{W} $ is extended linearly to a function $ map{hat{f}}{V}{W} $ if for all $ vec{v}in V $ such that $ vec{v}=c_1vec{beta}_1+cdots+c_nvec{beta}_n $ , the action of the map is $ hat{f}(vec{v})=c_1cdot f(vec{beta}_1) +cdots+c_ncdot f(vec{beta}_n) $ . If we specify a map $ map{h}{Re^2}{Re^2} $ that acts on the standard basis $ stdbasis_2 $ in this way $ h(colvec[r]{1  0})=colvec[r]{ - 1  1} qquad h(colvec[r]{0  1})=colvec[r]{ - 4  4} $ . then we have also specified the action of $ h $ on any other member of the domain. For instance, the value of $ h $ on this argument $ h(colvec[r]{3  - 2})=h(3cdot colvec[r]{1  0} - 2cdot colvec[r]{0  1}) =3cdot h(colvec[r]{1  0}) - 2cdot h(colvec[r]{0  1}) =colvec[r]{5  - 5} $ . is a direct consequence of the value of $ h $ on the basis vectors. Later in this chapter we shall develop a convenient scheme for computations like this one, using matrices. A linear map from a space into itself $ map{t}{V}{V} $ is a linear transformation}. In this book we use `linear transformation' only in the case where the codomain equals the domain. Be aware that some sources instead use it as a synonym for `linear map'. Still another synonym is `linear operator'. The map on $ Re^2 $ that projects all vectors down to the $ x $ - axis is a linear transformation. $ colvec{x  y}mapstocolvec{x  0} $ . The derivative map $ map{d/dx}{polyspace_n}{polyspace_n} $ $ a_0+a_1x+cdots+a_nx^n mapsunder{d/dx} a_1+2a_2x+3a_3x^2+cdots+na_nx^{n - 1} $ . is a linear transformation as this result from calculus shows: $ d(c_1f+c_2g)/dx=c_1, (df/dx)+c_2, (dg/dx) $ . The matrix transpose operation $ a &b  c &d ;mapsto; a &c  b &d $ . is a linear transformation of $ matspace_{nbyn{2}} $ . (Transpose is one - to - one and onto and so is in fact an automorphism.) We finish this subsection about maps by recalling that we can linearly combine maps. For instance, for these maps from $ Re^2 $ to itself $ colvec{x  y} mapsunder{f} colvec{2x  3x - 2y} quadtext{and}quad colvec{x  y} mapsunder{g} colvec{0  5x} $ . the linear combination $ 5f - 2g $ is also a transformation of  $ Re^2 $ . $ colvec{x  y} mapsunder{5f - 2g} colvec{10x  5x - 10y} $ . For vector spaces $ V $ and $ W $ , the set of linear functions from $ V $ to $ W $ is itself a vector space, a subspace of the space of all functions from $ V $ to $ W $ . noindent We denote the space of linear maps from $ V $ to  $ W $ by $ linmaps{V}{W} $ . This set is non - empty because it contains the zero homomorphism. So to show that it is a subspace we need only check that it is closed under the operations. Let $ map{f, g}{V}{W} $ be linear. Then the operation of function addition is preserved $ (f+g)(c_1vec{v}_1+c_2vec{v}_2) &=f(c_1vec{v}_1+c_2vec{v}_2) + g(c_1vec{v}_1+c_2vec{v}_2)  &=c_1f(vec{v}_1)+c_2f(vec{v}_2) +c_1g(vec{v}_1)+c_2g(vec{v}_2)  &=c_1bigl(f+gbigr)(vec{v}_1)+c_2bigl(f+gbigr)(vec{v}_2) $ . as is the operation of scalar multiplication of a function. $ (rcdot f)(c_1vec{v}_1+c_2vec{v}_2) &=r(c_1f(vec{v}_1)+c_2f(vec{v}_2))  &=c_1(rcdot f)(vec{v}_1)+c_2(rcdot f)(vec{v}_2) $ . Hence $ linmaps{V}{W} $ is a subspace. We started this section by defining `homomorphism' as a generalization of `isomorphism', by isolating the structure preservation property. Some of the points about isomorphisms carried over unchanged, while we adapted others. Note, however, that the idea of `homomorphism' is in no way somehow secondary to that of `isomorphism'. In the rest of this chapter we shall work mostly with homomorphisms. This is partly because any statement made about homomorphisms is automatically true about isomorphisms but more because, while the isomorphism concept is more natural, our experience will show that the homomorphism concept is more fruitful and more central to progress. Isomorphisms and homomorphisms both preserve structure. The difference is that homomorphisms have fewer restrictions, since they needn't be onto and needn't be one - to - one. We will examine what can happen with homomorphisms that cannot happen with isomorphisms. First consider the fact that homomorphisms need not be onto. Of course, each function is onto some set, namely its range. For example, the injection map $ map{iota}{Re^2}{Re^3} $ $ colvec{x  y} mapsto colvec{x  y  0} $ . is a homomorphism, and is not onto $ Re^3 $ . But it is onto the $ xy $ - plane. Under a homomorphism, the image of any subspace of the domain is a subspace of the codomain. In particular, the image of the entire space, the range of the homomorphism, is a subspace of the codomain. Let $ map{h}{V}{W} $ be linear and let $ S $ be a subspace of the domain $ V $ . The image $ h(S) $ is a subset of the codomain $ W $ , which is nonempty because $ S $ is nonempty. Thus, to show that $ h(S) $ is a subspace of $ W $ we need only show that it is closed under linear combinations of two vectors. If $ h(vec{s}_1) $ and $ h(vec{s}_2) $ are members of $ h(S) $ then $ c_1cdot h(vec{s}_1)+c_2cdot h(vec{s}_2) = h(c_1cdot vec{s}_1)+h(c_2cdot vec{s}_2) = h(c_1cdot vec{s}_1+c_2cdot vec{s}_2) $ is also a member of $ h(S) $ because it is the image of $ c_1cdot vec{s}_1+c_2cdot vec{s}_2 $ from $ S $ . The range space of a homomorphism $ map{h}{V}{W} $ is $ rangespace{h}=set{h(vec{v})suchthat vec{v}in V} $ . sometimes denoted $ h(V) $ . The dimension of the range space is the map's rank. noindent We shall soon see the connection between the rank of a map and the rank of a matrix. For the derivative map $ map{d/dx}{polyspace_3}{polyspace_3} $ given by $ a_0+a_1x+a_2x^2+a_3x^3 mapsto a_1+2a_2x+3a_3x^2 $ the range space $ rangespace{d/dx} $ is the set of quadratic polynomials $ set{r+sx+tx^2suchthat r, s, tinRe } $ . Thus, this map's rank is  $ 3 $ . With this homomorphism $ map{h}{M_{nbyn{2}}}{polyspace_3} $ $ a &b  c &d mapsto (a+b+2d)+cx^2+cx^3 $ . an image vector in the range can have any constant term, must have an $ x $ coefficient of zero, and must have the same coefficient of $ x^2 $ as of $ x^3 $ . That is, the range space is $ rangespace{h}=set{r+sx^2+sx^3suchthat r, sinRe} $ and so the rank is  $ 2 $ . The prior result shows that, in passing from the definition of isomorphism to the more general definition of homomorphism, omitting the onto requirement doesn't make an essential difference. Any homomorphism is onto some space, namely its range. However, omitting the one - to - one condition does make a difference. A homomorphism may have many elements of the domain that map to one element of the codomain. Below is a bean sketch of a many - to - one map between sets.appendrefs{many - to - one maps}spacefactor=1000 It shows three elements of the codomain that are each the image of many members of the domain. (Rather than picture lots of individual $ mapsto $ arrows, each association of many inputs with one output shows only one such arrow.) Recall that for any function $ map{h}{V}{W} $ , the set of elements of $ V $ that map to $ vec{w}in W $ is the inverse image/ $ h^{ - 1}(vec{w})=set{vec{v}in Vsuchthat h(vec{v})=vec{w}} $ . Above, the left side shows three inverse image sets. Consider the projection $ map{pi}{Re^3}{Re^2} $ $ colvec{x  y  z} mapsunder{pi} colvec{x  y} $ . which is a homomorphism that is many - to - one. An inverse image set is a vertical line of vectors in the domain. One example is this. $ pi^{ - 1}(colvec[r]{1  3})=set{colvec[r]{1  3  z}suchthat zinRe} $ . This homomorphism $ map{h}{Re^2}{Re^1} $ $ colvec{x  y} mapsunder{h} x+y $ . is also many - to - one. For a fixed $ winRe^1 $ the inverse image $ h^{ - 1}(w) $ is the set of plane vectors whose components add to $ w $ . In generalizing from isomorphisms to homomorphisms by dropping the one - to - one condition we lose the property that, intuitively, the domain is ``the same'' as the range. We lose, that is, that the domain corresponds perfectly to the range. The examples below illustrate that what we retain is that a homomorphism describes how the domain is ``analogous to'' or ``like'' the range. We think of $ Re^3 $ as like $ Re^2 $ except that vectors have an extra component. That is, we think of the vector with components $ x $ , $ y $ , and  $ z $ as like the vector with components $ x $ and  $ y $ . Defining the projection map $ pi $ makes precise which members of the domain we are thinking of as related to which members of the codomain. To understanding how the preservation conditions in the definition of homomorphism show that the domain elements are like the codomain elements, start by picturing $ Re^2 $ as the $ xy $ - plane inside of $ Re^3 $ (the $ xy $  plane inside of $ Re^3 $ is a set of three - tall vectors with a third component of zero and so does not precisely equal the set of two - tall vectors  $ Re^2 $ , but this embedding makes the picture much clearer). The preservation of addition property says that vectors in $ Re^3 $ act like their shadows in the plane. noindent Thinking of $ pi(vec{v}) $ as the ``shadow'' of $ vec{v} $ in the plane gives this restatement: the sum of the shadows $ pi(vec{v}_1)+pi(vec{v}_2) $ equals the shadow of the sum $ pi(vec{v}_1+vec{v}_2) $ . Preservation of scalar multiplication is similar. Drawing the codomain $ Re^2 $ on the right gives a picture that is uglier but is more faithful to the bean sketch above. Again, the domain vectors that map to $ vec{w}_1 $ lie in a vertical line; one is drawn, in gray. Call any member of this inverse image $ pi^{ - 1}(vec{w}_1) $ a `` $ vec{w}_1 $  vector.'' Similarly, there is a vertical line of `` $ vec{w}_2 $  vectors'' and a vertical line of `` $ vec{w}_1+vec{w}_2 $  vectors.'' Now, saying that $ pi $ is a homomorphism is recognizing that if $ pi(vec{v}_1)=vec{w}_1 $ and $ pi(vec{v}_2)=vec{w}_2 $ then $ pi(vec{v}_1+vec{v}_2)=pi(vec{v}_1)+pi(vec{v}_2) =vec{w}_1+vec{w}_2 $ . That is, the classes add: any $ vec{w}_1 $  vector plus any $ vec{w}_2 $  vector equals a $ vec{w}_1+vec{w}_2 $  vector. Scalar multiplication is similar. So although $ Re^3 $ and $ Re^2 $ are not isomorphic $ pi $ describes a way in which they are alike: vectors in $ Re^3 $ add as do the associated vectors in $ Re^2 $ Dash vectors add as their shadows add. A homomorphism can express an analogy between spaces that is more subtle than the prior one. For the map from $ X $ $ colvec{x  y} mapsunder{h} x+y $ . fix two numbers in the range $ w_1, w_2inRe $ . A $ vec{v}_1 $ that maps to $ w_1 $ has components that add to $ w_1 $ , so the inverse image $ h^{ - 1}(w_1) $ is the set of vectors with endpoint on the diagonal line $ x+y=w_1 $ . Think of these as `` $ w_1 $ vectors.'' Similarly we have `` $ w_2 $ vectors'' and `` $ w_1+w_2 $ vectors.'' The addition preservation property says this. Restated, if we add a $ w_1 $  vector to a $ w_2 $  vector then $ h $ maps the result to a $ w_1+w_2 $ vector. Briefly, the sum of the images is the image of the sum. Even more briefly, $ h(vec{v}_1)+h(vec{v}_2)=h(vec{v}_1+vec{v}_2) $ . The inverse images can be structures other than lines. For the linear map $ map{h}{Re^3}{Re^2} $ $ colvec{x  y  z} mapsto colvec{x  x} $ . the inverse image sets are planes $ x=0 $ , $ x=1 $ , etc., perpendicular to the $ x $ - axis. We won't describe how every homomorphism that we will use is an analogy because the formal sense that we make of ``alike in that $ X $ , , and $ X $ we draw two insights. The first insight is that in all three examples the inverse image of the range's zero vector is a line or plane through the origin. It is therefore a subspace of the domain. For any homomorphism the inverse image of a subspace of the range is a subspace of the domain. In particular, the inverse image of the trivial subspace of the range is a subspace of the domain. noindent (The examples above consider inverse images of single vectors but this result is about inverse images of sets $ h^{ - 1}(S)=set{vec{v}in Vsuchthat h(vec{v})in S} $ . We use the same term for both by taking the inverse image of a single element $ h^{ - 1}(vec{w}) $ to be the inverse image of the one - element set $ h^{ - 1}(set{vec{w}}) $ .) Let $ map{h}{V}{W} $ be a homomorphism and let $ S $ be a subspace of the range space of $ h $ . Consider the inverse image of $ S $ . It is nonempty because it contains $ zero_V $ , since $ h(zero_V)=zero_W $ and $ zero_W $ is an element of $ S $ as $ S $ is a subspace. To finish we show that $ h^{ - 1}(S) $ is closed under linear combinations. Let $ vec{v}_1 $ and $ vec{v}_2 $ be two of its elements, so that $ h(vec{v}_1) $ and $ h(vec{v}_2) $ are elements of $ S $ . Then $ c_1vec{v}_1+c_2vec{v}_2 $ is an element of the inverse image $ h^{ - 1}(S) $ because $ h(c_1vec{v}_1+c_2vec{v}_2) =c_1h(vec{v}_1)+c_2h(vec{v}_2) $ is a member of $ S $ . The null space or kernel of a linear map $ map{h}{V}{W} $ is the inverse image of $ zero_W $ . $ nullspace{h}=h^{ - 1}(zero_W)=set{vec{v}in Vsuchthat h(vec{v})=zero_W} $ . The dimension of the null space is the map's nullity. The map from $ X $ has this null space $ nullspace{d/dx}=set{a_0+0x+0x^2+0x^3suchthat a_0inRe} $ so its nullity is $ 1 $ . The map from $ X $ has this null space, and nullity $ 2 $ . $ nullspace{h}=set{ a &b  0 & - (a+b)/2 suchthat a, binRe} $ . Now for the second insight from the above examples. In $ X $ each of the vertical lines squashes down to a single pointDash in passing from the domain to the range, $ pi $ takes all of these one - dimensional vertical lines and maps them to a point, leaving the range smaller than the domain by one dimension. Similarly, in $ X $ the two - dimensional domain compresses to a one - dimensional range by breaking the domain into the diagonal lines and maps each of those to a single member of the range. Finally, in $ X $ the domain breaks into planes which get squashed to a point and so the map starts with a three - dimensional domain but ends two smaller, with a one - dimensional range. (The codomain is two - dimensional but the range is one - dimensional and the dimension of the range is what matters.) A linear map's rank plus its nullity equals the dimension of its domain. Let $ map{h}{V}{W} $ be linear and let $ B_N=sequence{vec{beta}_1, ldots, vec{beta}_k} $ be a basis for the null space. Expand that to a basis $ B_V=sequence{vec{beta}_1, dots, vec{beta}_k, vec{beta}_{k+1}, dots, vec{beta}_n} $ for the entire domain, using Corollary Two.III.. We shall show that $ B_R=sequence{ h(vec{beta}_{k+1}), dots, h(vec{beta}_n)} $ is a basis for the range space. Then counting the size of the bases gives the result. To see that $ B_R $ is linearly independent, consider $ zero_W=c_{k+1}h(vec{beta}_{k+1})+dots+c_nh(vec{beta}_n) $ . We have $ zero_W=h(c_{k+1}vec{beta}_{k+1}+dots+c_nvec{beta}_n) $ and so $ c_{k+1}vec{beta}_{k+1}+dots+c_nvec{beta}_n $ is in the null space of $ h $ . As $ B_N $ is a basis for the null space there are scalars $ c_1, dots, c_k $ satisfying this relationship. $ c_1vec{beta}_1+dots+c_kvec{beta}_k = c_{k+1}vec{beta}_{k+1}+dots+c_nvec{beta}_n $ . But this is an equation among members of $ B_V $ , which is a basis for $ V $ , so each $ c_i $ equals $ 0 $ . Therefore $ B_R $ is linearly independent. To show that $ B_R $ spans the range space consider a member of the range space $ h(vec{v}) $ . Express $ vec{v} $ as a linear combination $ vec{v}=c_1vec{beta}_1+dots+c_nvec{beta}_n $ of members of $ B_V $ . This gives $ h(vec{v})=h(c_1vec{beta}_1+dots+c_nvec{beta}_n) =c_1h(vec{beta}_1)+dots+c_kh(vec{beta}_k) +c_{k+1}h(vec{beta}_{k+1})+dots+c_nh(vec{beta}_n) $ and since $ vec{beta}_1 $ , ldots, $ vec{beta}_k $ are in the null space, we have that $ h(vec{v})=zero+dots+zero +c_{k+1}h(vec{beta}_{k+1})+dots+c_nh(vec{beta}_n) $ . Thus, $ h(vec{v}) $ is a linear combination of members of $ B_R $ , and so $ B_R $ spans the range space. Where $ map{h}{Re^3}{Re^4} $ is $ colvec{x  y  z} mapsunder{h} colvec{x  0  y  0} $ . the range space and null space are $ rangespace{h}= set{colvec{a  0  b  0}suchthat a, binRe } quadtext{and}quad nullspace{h}= set{colvec{0  0  z}suchthat zinRe } $ . and so the rank of $ h $ is $ 2 $ while the nullity is $ 1 $ . If $ map{t}{Re}{Re} $ is the linear transformation $ xmapsto - 4x, $ then the range is $ rangespace{t}=Re $ . The rank is $ 1 $ and the nullity is $ 0 $ . The rank of a linear map is less than or equal to the dimension of the domain. Equality holds if and only if the nullity of the map is $ 0 $ . We know that an isomorphism exists between two spaces if and only if the dimension of the range equals the dimension of the domain. We have now seen that for a homomorphism to exist a necessary condition is that the dimension of the range must be less than or equal to the dimension of the domain. For instance, there is no homomorphism from $ Re^2 $ onto $ Re^3 $ . There are many homomorphisms from $ Re^2 $ into $ Re^3 $ , but none onto. The range space of a linear map can be of dimension strictly less than the dimension of the domain and so linearly independent sets in the domain may map to linearly dependent sets in the range. ( $ X $ 's derivative transformation on $ polyspace_3 $ has a domain of dimension  $ 4 $ but a range of dimension  $ 3 $ and the derivative sends $ set{1, x, x^2, x^3} $ to $ set{0, 1, 2x, 3x^2} $ ). That is, under a homomorphism independence may be lost. In contrast, dependence stays. Under a linear map, the image of a linearly dependent set is linearly dependent. Suppose that $ c_1vec{v}_1+dots+c_nvec{v}_n=zero_V $ with some $ c_i $ nonzero. Apply $ h $ to both sides: $ h(c_1vec{v}_1+dots+c_nvec{v}_n)=c_1h(vec{v}_1)+dots+c_nh(vec{v}_n) $ and $ h(zero_V)=zero_W $ . Thus we have $ c_1h(vec{v}_1)+dots+c_nh(vec{v}_n)=zero_W $ with some $ c_i $ nonzero. When is independence not lost? The obvious sufficient condition is when the homomorphism is an isomorphism. This condition is also necessary; see $ X $ . We will finish this subsection comparing homomorphisms with isomorphisms by observing that a one - to - one homomorphism is an isomorphism from its domain onto its range. This one - to - one homomorphism $ map{iota}{Re^2}{Re^3} $ $ colvec{x  y} mapsunder{iota} colvec{x  y  0} $ . gives a correspondence between $ Re^2 $ and the $ xy $ - plane subset of $ Re^3 $ . Where $ V $ is an $ n $ - dimensional vector space, these are equivalent statements about a linear map $ map{h}{V}{W} $ . $ h $ is one - to - one $ h $ has an inverse from its range to its domain that is a linear map $ nullspace{h}=set{zero, } $ , that is, $ nullity(h)=0 $ $ rank (h)=n $ if $ sequence{vec{beta}_1, dots, vec{beta}_n} $ is a basis for $ V $ then $ sequence{h(vec{beta}_1), dots, h(vec{beta}_n)} $ is a basis for $ rangespace{h} $ We will first show that $ text{(1)} Longleftrightarrow text{(2)} $ . We will then show that $ text{(1)}implies text{(3)}implies text{(4)}implies text{(5)}implies text{(2)} $ . For $ text{(1)} Longrightarrow text{(2)} $ , suppose that the linear map $ h $ is one - to - one, and therefore has an inverse $ map{h^{ - 1}}{rangespace{h}}{V} $ . The domain of that inverse is the range of $ h $ and thus a linear combination of two members of it has the form $ c_1h(vec{v}_1)+c_2h(vec{v}_2) $ . On that combination, the inverse $ h^{ - 1} $ gives this. $ h^{ - 1}(c_1h(vec{v}_1)+c_2h(vec{v}_2)) &=h^{ - 1}(h(c_1vec{v}_1+c_2vec{v}_2))  &=composed{h^{ - 1}}{h};(c_1vec{v}_1+c_2vec{v}_2)  &=c_1vec{v}_1+c_2vec{v}_2  &=c_1cdot h^{ - 1}(h(vec{v}_1))+c_2cdot h^{ - 1}(h(vec{v}_2)) $ . Thus if a linear map has an inverse then the inverse must be linear. But this also gives the $ text{(2)} Longrightarrow text{(1)} $ implication, because the inverse itself must be one - to - one. Of the remaining implications, $ text{(1)}implies text{(3)} $ holds because any homomorphism maps $ zero_V $ to $ zero_W $ , but a one - to - one map sends at most one member of $ V $ to $ zero_W $ . Next, $ text{(3)} implies text{(4)} $ is true since rank plus nullity equals the dimension of the domain. For $ text{(4)} implies text{(5)} $ , to show that $ sequence{h(vec{beta}_1), dots, h(vec{beta}_n)} $ is a basis for the range space we need only show that it is a spanning set, because by assumption the range has dimension $ n $ . Consider $ h(vec{v})inrangespace{h} $ . Expressing $ vec{v} $ as a linear combination of basis elements produces $ h(vec{v})=h(lincombo{c}{vec{beta}}) $ , which gives that $ h(vec{v})=c_1h(vec{beta}_1)+dots+c_nh(vec{beta}_n) $ , as desired. Finally, for the $ text{(5)}implies text{(2)} $ implication, assume that $ sequence{vec{beta}_1, dots, vec{beta}_n} $ is a basis for $ V $ so that $ sequence{h(vec{beta}_1), dots, h(vec{beta}_n)} $ is a basis for $ rangespace{h} $ . Then every $ vec{w}inrangespace{h} $ has the unique representation $ vec{w}=c_1h(vec{beta}_1)+dots+c_nh(vec{beta}_n) $ . Define a map from $ rangespace{h} $ to $ V $ by $ vec{w} ;mapsto; lincombo{c}{vec{beta}} $ . (uniqueness of the representation makes this well - defined). Checking that it is linear and that it is the inverse of $ h $ are easy. We have seen that a linear map expresses how the structure of the domain is like that of the range. We can think of such a map as organizing the domain space into inverse images of points in the range. In the special case that the map is one - to - one, each inverse image is a single point and the map is an isomorphism between the domain and the range.
This is a PASCAL routine to do $ krho_i+rho_j $ to an augmented matrix. Of course this is only one part of a whole program, but it makes the point that Gaussian reduction is ideal for computer coding. There are pitfalls, however. For example, some arise from the computer's use of finite - precision approximations of real numbers. These systems provide a simple example. (The second two lines are hard to tell apart.) Both have $ (1, 1) $ as their unique solution. In the first system, some small change in the numbers will produce only a small change in the solution: $ {2} x &+ &2y &= &3  3x & - &2y &= &(i)008 $ . gives a solution of $ ((i)002, 0.999) $ . Geometrically, changing one of the lines by a small amount does not change the intersection point by very much. That's not true for the second system. A small change in the coefficients $ {2} x &+ &2y &= &3  (i)000, 000, 01x &+ &2y &= &(iii)000, 000, 03 $ . leads to a completely different answer: $ (3, 0) $ . The solution of the second example varies wildly, depending on a $ 9 $ - th digit. That's bad news for a machine using $ 8 $ digits to represent reals. In short, systems that are nearly singular may be hard to compute with. Another thing that can go wrong is error propagation. In a system with a large number of equations (say, 100 or more), small rounding errors early in the procedure can snowball to overwhelm the solution at the end. These issues, and many others like them, are outside the scope of this book, but remember that just because Gauss's Method always works in theory and just because a program correctly implements that method and just because the answer appears on green - bar paper, doesn't mean that answer is right. In practice, always use a package where experts have worked hard to counter what can go wrong.
noindenttextit{This section uses material from three optional subsections: Combining Subspaces, Determinants Exist, and Laplace's Expansion.} We began this chapter by recalling that every linear map $ map{h}{V}{W} $ can be represented with respect to some bases $ Bsubset V $ and $ Dsubset W $ by a partial identity matrix. Restated, the partial identity form is a canonical form for matrix equivalence. This chapter considers the case where the codomain equals the domain so we naturally ask what is possible when the two bases are equal, when we have $ rep{t}{B, B} $ . In short, we want a canonical form for matrix similarity. We noted that in the $ B, B $ case a partial identity matrix is not always possible. We therefore extended the matrix forms of interest to the natural generalization, diagonal matrices, and showed that a transformation or square matrix can be diagonalized if its eigenvalues are distinct. But we also gave an example of a square matrix that cannot be diagonalized because it is nilpotent, and thus diagonal form won't suffice as the canonical form for matrix similarity. The prior section developed that example to get a canonical form for nilpotent matrices, subdiagonal ones. This section finishes our program by showing that for any linear transformation there is a basis  $ B $ such that the matrix representation $ rep{t}{B, B} $ is the sum of a diagonal matrix and a nilpotent matrix. This is Jordan canonical form. Recall that the set of square matrices  $ matspace_{nbyn{n}} $ is a vector space under entry - by - entry addition and scalar multiplication, and that this space has dimension $ n^2 $ . Thus for any $ nbyn{n} $ matrix $ T $ the $ n^2+1 $ - member set $ set{I, T, T^2, dots, T^{n^2} } $ is linearly dependent and so there are scalars $ c_0, dots, c_{n^2} $ , not all zero, such that $ c_{n^2}T^{n^2}+dots+c_1T+c_0I $ . is the zero matrix. Therefore every transformation has a sort of generalized nilpotencyDash the powers of a square matrix cannot climb forever without a kind of repeat. Let $ t $ be a linear transformation of a vector space  $ V $ . Where $ f(x)=c_nx^n+dots+c_1x+c_0 $ is a polynomial, $ f(t) $ is the transformation $ c_nt^n+dots+c_1t+c_0(identity) $ on  $ V $ . In the same way, if $ T $ is a square matrix then $ f(T) $ is the matrix $ c_nT^n+dots+c_1T+c_0I $ . noindent The polynomial of the matrix represents the polynomial of the map: if $ T=rep{t}{B, B} $ then $ f(T)=rep{f(t)}{B, B} $ . This is because $ T^j=rep{t^j}{B, B} $ , and $ cT=rep{ct}{B, B} $ , and $ T_1+T_2 =rep{t_1+t_2}{B, B} $ . We shall write the matrix polynomial slightly differently than the map polynomial. For instance, if $ f(x)=x - 3 $ then we shall write the identity matrix, as in  $ f(T)=T - 3I $ , but not write the identity map, as in  $ f(t)=t - 3 $ . Rotation of plane vectors $ pi/3 $  radians counterclockwise is represented with respect to the standard basis by $ T= [r] cos(pi/3) & - sin(pi/3)  sin(pi/3) &cos(pi/3) = [r] 1/2 & - sqrt{3}/2  sqrt{3}/2 &1/2 $ . and verifying that $ T^2 - T+I=Z $ is routine. (Geometrically, $ T^2 $ rotates by $ 2pi/3 $ . On the standard basis vector  $ vec{e}_1 $ , the action of $ T^2 - T $ is to give the difference between the unit vector with angle $ 2pi/3 $ and the unit vector with angle $ pi/3 $ , which is $ binom{ - 1}{0} $ . On $ vec{e}_2 $ it gives $ binom{0}{ - 1} $ . So $ T^2 - T= - I $ .) The space $ matspace_{nbyn{2}} $ has dimension four so we know that for any $ nbyn{2} $ matrix  $ T $ there is a fourth degree polynomial $ f $ such that $ f(T)=Z $ . But in that example we exhibited a degree two polynomial that works. So while degree  $ n^2 $ always suffices, in some cases a smaller - degree polynomial is enough. The minimal polynomial $ m(x) $ of a transformation $ t $ or a square matrix $ T $ is the non - zero polynomial of least degree and with leading coefficient  $ 1 $ such that $ m(t) $ is the zero map or $ m(T) $ is the zero matrix. noindent A minimal polynomial cannot be a constant polynomial because of the restriction on the leading coefficient. So a minimal polynomial must have degree at least one. The zero matrix has minimal polynomial $ p(x)=x $ while the identity matrix has minimal polynomial $ hat{p}(x)=x - 1 $ . Any transformation or square matrix has a unique minimal polynomial. First we prove existence. By the earlier observation that degree  $ n^2 $ suffices, there is at least one nonzero polynomial $ p(x)=c_kx^k+cdots+c_0 $ that takes the map or matrix to zero. From among all such polynomials take one with the smallest degree. Divide this polynomial by its leading coefficient  $ c_k $ to get a leading  $ 1 $ . Hence any map or matrix has at least one minimal polynomial. Now for uniqueness. Suppose that $ m(x) $ and $ hat{m}(x) $ both take the map or matrix to zero, are both of minimal degree and are thus of equal degree, and both have a leading  $ 1 $ . Consider the difference, $ m(x) - hat{m}(x) $ . If it is not the zero polynomial then it has a nonzero leading coefficient. Dividing through by that leading coefficient would make it a polynomial that takes the map or matrix to zero, has leading coefficient  $ 1 $ , and is of smaller degree than $ m $ and $ hat{m} $ (because in the subtraction the leading $ 1 $ 's cancel). That would contradict the minimality of the degree of $ m $ and $ hat{m} $ . Thus $ m(x) - hat{m}(x) $ is the zero polynomial and the two are equal. One way to compute the minimal polynomial for the matrix of $ X $ is to find the powers of $ T $ up to $ n^2=4 $ . $ T^2= [r] - 1/2 & - sqrt{3}/2  sqrt{3}/2 & - 1/2 quad T^3= [r] - 1 &0  0 & - 1 quad T^4= [r] - 1/2 &sqrt{3}/2  - sqrt{3}/2 & - 1/2 $ . Put $ c_4T^4+c_3T^3+c_2T^2+c_1T+c_0I $ equal to the zero matrix. $ {5} - (1/2)c_4 & - &c_3 & - &(1/2)c_2 &+ &(1/2)c_1 &+ &c_0 &= &0  (sqrt{3}/2)c_4 & & & - &(sqrt{3}/2)c_2 & - &(sqrt{3}/2)c_1 & & &= &0  - (sqrt{3}/2)c_4 & & &+ &(sqrt{3}/2)c_2 &+ &(sqrt{3}/2)c_1 & & &= &0  - (1/2)c_4 & - &c_3 & - &(1/2)c_2 &+ &(1/2)c_1 &+ &c_0 &= &0 $ . Apply Gauss' Method. $ {5} - (1/2)c_4 & - &c_3 & - &(1/2)c_2 &+ &(1/2)c_1 &+ &c_0 &= &0  & & - (sqrt{3})c_3 & - &sqrt{3}c_2 & & &+ &sqrt{3}c_0 &= &0 $ . With an eye toward making the degree of the polynomial as small as possible, note that setting $ c_4 $ , $ c_3 $ , and $ c_2 $ to zero forces $ c_1 $ and $ c_0 $ to also come out as zero so the equations won't allow a degree one minimal polynomial. Instead, set $ c_4 $ and $ c_3 $ to zero. The system $ {3} - (1/2)c_2 &+ &(1/2)c_1 &+ &c_0 &= &0  - sqrt{3}c_2 & & &+ &sqrt{3}c_0 &= &0 $ . has the solution set $ c_1= - c_0 $ and $ c_2=c_0 $ . Taking the leading coefficient to be  $ c_2=1 $ gives the minimal polynomial $ x^2 - x+1 $ . That computation is ungainly. We shall develop an alternative. Suppose that the polynomial $ f(x)=c_nx^n+dots+c_1x+c_0 $ factors as $ k(x - lambda_1)^{q_1}cdots(x - lambda_z)^{q_z} $ . If $ t $ is a linear transformation then these two are equal maps. $ c_nt^n+dots+c_1t+c_0 = kcdotcomposed{composed{(t - lambda_1)^{q_1}}{cdots}}{ (t - lambda_z)^{q_z}} $ . Consequently, if $ T $ is a square matrix then $ f(T) $ and $ kcdot(T - lambda_1I)^{q_1}cdots(T - lambda_z I)^{q_z} $ are equal matrices. We use induction on the degree of the polynomial. The cases where the polynomial is of degree zero and degree one are clear. The full induction argument is $ X $ but we will give its sense with the degree two case. A quadratic polynomial factors into two linear terms $ f(x)=k(x - lambda_1)cdot(x - lambda_2) =k(x^2+( - lambda_1 - lambda_2)x+lambda_1lambda_2) $ . Substituting $ t $ for $ x $ in the factored and unfactored versions gives the same map. $ bigl(kcdotcomposed{(t - lambda_1)}{(t - lambda_2)}bigr)>(vec{v}) &=bigl(kcdot(t - lambda_1)bigr), (t(vec{v}) - lambda_2vec{v})  &=kcdotbigl(t(t(vec{v})) - t(lambda_2vec{v}) - lambda_1 t(vec{v})+lambda_1lambda_2vec{v}bigr)  &=kcdot bigl(composed{t}{t}, (vec{v}) - (lambda_1+lambda_2)t(vec{v}) +lambda_1lambda_2vec{v}bigr)  &=kcdot(t^2 - (lambda_1+lambda_2)t+lambda_1lambda_2)>(vec{v}) $ . The third equality uses linearity of $ t $ to bring $ lambda_2 $ out of the second term. The next result is that every root of the minimal polynomial is an eigenvalue and that every eigenvalue is a root of the minimal polynomial. (That is, the result says ` $ 1leq q_i $ ' and not just ` $ 0leq q_i $ '.) For that, recall that to find eigenvalues we solve $ deter{T - xI}=0 $ and this determinant gives a polynomial in $ x $ , the characteristic polynomial, whose roots are the eigenvalues. [Cayley - Hamilton] hspace*{0em plus2em} If the characteristic polynomial of a transformation or square matrix factors into $ kcdot (x - lambda_1)^{p_1}(x - lambda_2)^{p_2}cdots(x - lambda_z)^{p_z} $ . then its minimal polynomial factors into $ (x - lambda_1)^{q_1}(x - lambda_2)^{q_2}cdots(x - lambda_z)^{q_z} $ . where $ 1leq q_i leq p_i $ for each $ i $ between $ 1 $ and $ z $ . noindent The proof takes up the next three lemmas. We will state them in matrix terms because that version is convenient for the first proof but they apply equally well to maps. The first result is the key. For the proof, observe that we can view a matrix of polynomials as a polynomial with matrix coefficients. $ 2x^2+3x - 1 &x^2+2  3x^2+4x+1 &4x^2+x+1 = [r] 2 &1  3 &4 x^2 + [r] 3 &0  4 &1 x + [r] - 1 &2  1 &1 $ . If $ T $ is a square matrix with characteristic polynomial $ c(x) $ then $ c(T) $ is the zero matrix. Let $ C $ be $ T - xI $ , the matrix whose determinant is the characteristic polynomial $ c(x)=c_nx^n+dots+c_1x+c_0 $ . $ C= t_{1, 1} - x &t_{1, 2} &ldots  t_{2, 1} &t_{2, 2} - x  vdots & &ddots  & & &t_{n, n} - x $ . Recall Theorem Four.III., that the product of a matrix with its adjoint equals the determinant of the matrix times the identity. $ c(x)cdot I =adj (C)C =adj (C)(T - xI) =adj (C)T - adj(C)cdot x tag*{( $ * $ )} $ . The left side of ( $ * $ ) is $ c_nIx^n+c_{n - 1}Ix^{n - 1}+cdots+c_1Ix+c_0I $ . For the right side, the entries of $ adj (C) $ are polynomials, each of degree at most $ n - 1 $ since the minors of a matrix drop a row and column. As suggested before the proof, rewrite it as a polynomial with matrix coefficients: $ adj (C)=C_{n - 1}x^{n - 1}+cdots+C_1x+C_0 $ where each $ C_i $ is a matrix of scalars. Now this is the right side of ( $ * $ ). $ [(C_{n - 1}T)x^{n - 1}+cdots+(C_1T)x+C_0T] - [C_{n - 1}x^n+C_{n - 2}x^{n - 1}+cdots+C_0x] $ . Equate the left and right side of ( $ * $ )'s coefficients of $ x^n $ , of $ x^{n - 1} $ , etc. $ c_nI &= - C_{n - 1}  c_{n - 1}I &= - C_{n - 2}+C_{n - 1}T  &vdotswithin{=}  c_{1}I &= - C_{0}+C_{1}T  c_{0}I &=C_{0}T $ . Multiply, from the right, both sides of the first equation by $ T^n $ , both sides of the second equation by $ T^{n - 1} $ , etc. $ c_nT^n &= - C_{n - 1}T^n  c_{n - 1}T^{n - 1} &= - C_{n - 2}T^{n - 1}+C_{n - 1}T^n  &vdotswithin{=}  c_{1}T &= - C_{0}T+C_{1}T^2  c_{0}I &=C_{0}T $ . Add. The left is $ c_nT^n+c_{n - 1}T^{n - 1}+cdots+c_0I $ . The right telescopes; for instance, $ - C_{n - 1}T^n $ from the first line combines with the $ C_{n - 1}T^n $ half of the second line. The total on the right is the zero matrix. We refer to that result by saying that a matrix or map satisfies its characteristic polynomial. Any polynomial that is satisfied by $ T $ is divisible by $ T $ 's minimal polynomial. That is, for a polynomial $ f(x) $ , if $ f(T) $ is the zero matrix then $ f(x) $ is divisible by the minimal polynomial of $ T $ . Let $ m(x) $ be minimal for $ T $ . The Division Theorem for Polynomials gives $ f(x)=q(x)cdot m(x)+r(x) $ where the degree of $ r $ is strictly less than the degree of $ m $ . Because $ T $ satisfies both $ f $ and $ m $ , plugging $ T $ into that equation gives that $ r(T) $ is the zero matrix. That contradicts the minimality of $ m $ unless $ r $ is the zero polynomial. Combining the prior two lemmas shows that the minimal polynomial divides the characteristic polynomial. Thus any root of the minimal polynomial is also a root of the characteristic polynomial. Thus so far we have that if the minimal polynomial factors as $ m(x)=(x - lambda_1)^{q_1}cdots(x - lambda_i)^{q_i} $ then the characteristic polynomial also has the roots $ lambda_1 $ , ldots, $ lambda_i $ . But as far as what we have established to this point, the characteristic polynomial might have additional roots: $ c(x)=(x - lambda_1)^{p_1}cdots(x - lambda_i)^{p_i} (x - lambda_{i+1})^{p_{i+1}}cdots(x - lambda_z)^{p_z} $ , where $ 1leq q_jleq p_j $ for $ 1leq j leq i $ . We finish the proof of the Cayley - Hamilton Theorem by showing that the characteristic polynomial has no additional roots so that there are no $ lambda_{i+1} $ , $ lambda_{i+2} $ , etc. Each linear factor of the characteristic polynomial of a square matrix is also a linear factor of the minimal polynomial. Let $ T $ be a square matrix with minimal polynomial $ m(x) $ of degree  $ n $ and assume that $ x - lambda $ is a factor of the characteristic polynomial of $ T $ , so that $ lambda $ is an eigenvalue of $ T $ . We will show that $ m(lambda)=0 $ , i.e., that $ x - lambda $ is a factor of $ m $ . Assume that $ lambda $ is associated with the eigenvector  $ vec{v} $ and consider the powers $ T^2(vec{v}) $ , ldots, $ T^n(vec{v}) $ . We have $ T^2(vec{v}) =Tcdotlambdavec{v}=lambdacdot Tvec{v}=lambda^2vec{v} $ . The same happens for all of the powers: $ T^ivec{v}=lambda^ivec{v}, $ for $ 1leq i leq n $ . Thus for any polynomial function $ p(x) $ , application of the matrix $ p(T) $ to $ vec{v} $ equals the result of multiplying $ vec{v} $ by the scalar $ p(lambda) $ . p(T), vec{v} =(c_kT^k+dots+c_1T+c_0I), vec{v} =c_kT^kvec{v}+dots+c_1Tvec{v}+c_0vec{v}  =c_klambda^kvec{v}+dots+c_1lambdavec{v}+c_0vec{v} =p(lambda)cdotvec{v} Since $ m(T) $ is the zero matrix, $ zero=m(T)(vec{v})=m(lambda)cdotvec{v} $ and hence $ m(lambda)=0 $ , as $ vec{v}neqzero $ because it is an eigenvector. That concludes the proof of the Cayley - Hamilton Theorem. We can use the Cayley - Hamilton Theorem to find the minimal polynomial of this matrix. $ T= [r] 2 &0 &0 &1  1 &2 &0 &2  0 &0 &2 & - 1  0 &0 &0 &1 $ . First we find its characteristic polynomial $ c(x)=(x - 1)(x - 2)^3 $ with the usual determinant $ deter{T - xI} $ . With that, the Cayley - Hamilton Theorem says that $ T $ 's minimal polynomial is either $ (x - 1)(x - 2) $ or $ (x - 1)(x - 2)^2 $ or $ (x - 1)(x - 2)^3 $ . We can decide among the choices just by computing $ (T - 1I)(T - 2I)=! [r] 1 &0 &0 &1  1 &1 &0 &2  0 &0 &1 & - 1  0 &0 &0 &0 [r] 0 &0 &0 &1  1 &0 &0 &2  0 &0 &0 & - 1  0 &0 &0 & - 1 = [r] 0 &0 &0 &0  1 &0 &0 &1  0 &0 &0 &0  0 &0 &0 &0 $ . and $ (T - 1I)(T - 2I)^2= [r] 0 &0 &0 &0  1 &0 &0 &1  0 &0 &0 &0  0 &0 &0 &0 [r] 0 &0 &0 &1  1 &0 &0 &2  0 &0 &0 & - 1  0 &0 &0 & - 1 = [r] 0 &0 &0 &0  0 &0 &0 &0  0 &0 &0 &0  0 &0 &0 &0 $ . and so $ m(x)=(x - 1)(x - 2)^2 $ . We are looking for a canonical form for matrix similarity. This subsection completes this program by moving from the canonical form for the classes of nilpotent matrices to the canonical form for all classes. A linear transformation on a nontrivial vector space is nilpotent if and only if its only eigenvalue is zero. If the linear transformation $ t $ on a nontrivial vector space is nilpotent then there is an $ n $ such that $ t^n $ is the zero map, so $ t $ satisfies the polynomial $ p(x)=x^n=(x - 0)^n $ . By $ X $ the minimal polynomial of $ t $ divides $ p $ , so the minimal polynomial's only root is zero. By Cayley - Hamilton, $ X $ , the characteristic polynomial's only root is zero. Thus $ t $ 's only eigenvalue is zero. Conversely, if a transformation $ t $ on an $ n $ - dimensional space has only the single eigenvalue of zero then its characteristic polynomial is $ x^n $ . $ X $ says that a map satisfies its characteristic polynomial so $ t^n $ is the zero map. Thus $ t $ is nilpotent. noindent The `nontrivial vector space' is in the statement of that lemma because on a trivial space $ set{zero} $ the only transformation is the zero map, which has no eigenvalues because there are no associated nonzero eigenvectors. The transformation $ t - lambda $ is nilpotent if and only if $ t $ 's only eigenvalue is $ lambda $ . The transformation $ t - lambda $ is nilpotent if and only if $ t - lambda $ 's only eigenvalue is $ 0 $ . That holds if and only if $ t $ 's only eigenvalue is $ lambda $ , because $ t(vec{v})=lambdavec{v} , $ if and only if $ (t - lambda), (vec{v})=0cdotvec{v} $ . If the matrices $ T - lambda I $ and $ N $ are similar then $ T $ and $ N+lambda I $ are also similar, via the same change of basis matrices. With $ N=P(T - lambda I)P^{ - 1}=PTP^{ - 1} - P(lambda I)P^{ - 1} $ we have $ N=PTP^{ - 1} - PP^{ - 1}(lambda I) $ because the diagonal matrix $ lambda I $ commutes with anything. Thus $ N=PTP^{ - 1} - lambda I $ , and therefore $ N+lambda I=PTP^{ - 1} $ . We already have the canonical form for the case of nilpotent matrices, that is, for the case of matrices whose only eigenvalue is zero. By Corollary III., each such matrix is similar to one that is all zeroes except for blocks of subdiagonal ones. The prior results let us extend this to all matrices that have a single eigenvalue  $ lambda $ . In the new form, besides blocks of subdiagonal ones, all of the entries on the diagonal are  $ lambda $ . (See $ X $ below.) The characteristic polynomial of $ T=[r] 2 & - 1  1 &4 $ . is $ (x - 3)^2 $ and so $ T $ has a single eigenvalue, $ 3 $ . Because $ 3 $ is the only eigenvalue, $ T - 3I $ has the single eigenvalue of $ 0 $ and is therefore nilpotent. To get the canonical form of  $ T $ , start by finding a string basis for $ T - 3I $ as in the prior section, computing the powers of this matrix and the null spaces of those powers. The null space of $ t - 3 $ has dimension one and the null space of $ (t - 3)^2 $ has dimension two. So this is the canonical representation matrix and the form of the associated string basis  $ B $ for $ t - 3 $ . $ rep{t - 3}{B, B} =N= [r] 0 &0  1 &0 qquad {r} vec{beta}_1xmapsto{t - 3}vec{beta}_2xmapsto{t - 3}zero  $ . Find such a basis by first picking a $ vec{beta}_2 $ that is mapped to  $ zero $ by $ t - 3 $ (that is, pick an element of the null space $ nullspace{t - 3} $ given in the table). Then choose a $ vec{beta}_1 $ that is mapped to  $ zero $ by $ (t - 3)^2 $ . The only restriction on the choice, besides membership in the null space, is that $ vec{beta}_1 $ and $ vec{beta}_2 $ need to form a linearly independent set. Here is one possible choice. $ vec{beta}_1=colvec[r]{1  1} quad vec{beta}_2=colvec[r]{ - 2  2} $ . With that, $ X $ says that $ T $ is similar to this matrix. $ rep{t}{B, B}= N+3I= [r] 3 &0  1 &3 $ . We can exhibit the similarity computation. Take $ T $ to represent a transformation $ map{t}{C^2}{C^2} $ with respect to the standard basis (as we shall do for the rest of the chapter). The similarity diagram $ C^2_{wrt{stdbasis_2}} @>t - 3>T - 3I> C^2_{wrt{stdbasis_2}}  @Vscriptstyleidentity Vscriptstyle PV @Vscriptstyleidentity Vscriptstyle PV  C^2_{wrt{B}} @>t - 3>N> C^2_{wrt{B}} $ . describes that to move from the lower left to the upper left we multiply by $ P^{ - 1}=bigl(rep{identity}{stdbasis_2, B}bigr)^{ - 1} =rep{identity}{B, stdbasis_2} =[r] 1 & - 2  1 &2 $ . and to move from the upper right to the lower right we multiply by this matrix. $ P=[r] 1 & - 2  1 &2 ^{ - 1}!! =[r] 1/2 &1/2  - 1/4 &1/4 $ . So this equation shows the similarity. $ [r] 1/2 &1/2  - 1/4 &1/4 [r] 2 & - 1  1 &4 [r] 1 & - 2  1 &2 = [r] 3 &0  1 &3 $ . This matrix $ T= [r] 4 &1 &0 & - 1  0 &3 &0 &1  0 &0 &4 &0  1 &0 &0 &5 $ . has characteristic polynomial $ (x - 4)^4 $ and so has the single eigenvalue  $ 4 $ . The null space of $ t - 4 $ has dimension two, the null space of $ (t - 4)^2 $ has dimension three, and the null space of $ (t - 4)^3 $ has dimension four. This gives the canonical form for $ t - 4 $ . $ N=rep{t - 4}{B, B}= [r] 0 &0 &0 &0  1 &0 &0 &0  0 &1 &0 &0  0 &0 &0 &0 qquad {r} vec{beta}_{1}xmapsto{t - 4}vec{beta}_{2} xmapsto{t - 4}vec{beta}_{3}xmapsto{t - 4}zero  vec{beta}_{4}xmapsto{t - 4}zero  $ . To produce such a basis, choose a $ vec{beta}_3 $ and  $ vec{beta}_4 $ that are mapped to  $ zero $ by  $ t - 4 $ . Also choose a  $ vec{beta}_2 $ that is mapped to  $ zero $ by  $ (t - 4)^2 $ and a $ vec{beta}_1 $ is mapped to  $ zero $ by  $ (t - 4)^3 $ . $ vec{beta}_1=colvec{1  0  0  0} quad vec{beta}_2=colvec{ - 1  1  0  0} quad vec{beta}_3=colvec{ - 1  1  0  1} quad vec{beta}_4=colvec{0  0  1  0} $ . Note that these four were chosen to not have any linear dependences. This is the canonical form matrix similar to  $ T $ . $ rep{t}{B, B} =N+4I= [r] 4 &0 &0 &0  1 &4 &0 &0  0 &1 &4 &0  0 &0 &0 &4 $ . A Jordan block is a square matrix, or a square block of entries inside of a matrix, that is all zeroes except that there is a number $ lambdainC $ such that every diagonal entry is $ lambda $ , and that every subdiagonal entry is  $ 1 $ . (If the block is $ nbyn{1} $ then it has no subdiagonal  $ 1 $ 's.) The strings in the associated basis are Jordan strings or Jordan chains. The above examples illustrate that for single - eigenvalue matrices, the Jordan block matrices are canonical representatives of the similarity classes. We can make this matrix form unique by arranging the basis elements so that the blocks of subdiagonal ones go from longest to shortest, reading left to right. The $ nbyn{3} $ matrices whose only eigenvalue is $ 1/2 $ separate into three similarity classes. The three classes have these canonical representatives. $ [r] 1/2 &0 &0  0 &1/2 &0  0 &0 &1/2 qquad [r] 1/2 &0 &0  1 &1/2 &0  0 &0 &1/2 qquad [r] 1/2 &0 &0  1 &1/2 &0  0 &1 &1/2 $ . In particular, this matrix $ [r] 1/2 &0 &0  0 &1/2 &0  0 &1 &1/2 $ . belongs to the similarity class represented by the middle one, because of the convention of ordering the blocks of subdiagonal ones. We are now set up to finish the program of this chapter. First we review of what we have so far about transformations $ map{t}{C^n}{C^n} $ . The diagonalizable case is where $ t $ has  $ n $ distinct eigenvalues $ lambda_1 $ , ldots, $ lambda_n $ , that is, where the number of eigenvalues equals the dimension of the space. In this case there is a basis $ sequence{vec{beta}_1, ldots, vec{beta}_n} $ where $ vec{beta}_i $ is an eigenvector associated with the eigenvalue  $ lambda_i $ . The example below has $ n=3 $ and the three eigenvalues are $ lambda_1=1 $ , $ lambda_2=3 $ , and $ lambda_3= - 1 $ . It shows the canonical representative matrix and the associated basis. $ T_1= 1 &0 &0  0 &3 &0  0 &0 & - 1 qquad {r} vec{beta}_1mapsunder{t - 1}zero  vec{beta}_2mapsunder{t - 3}zero  vec{beta}_3mapsunder{t+1}zero $ . One diagonalization example is Five.II.. The case where $ t $ has a single eigenvalue leverages the results on nilpotency to get a basis of associated eigenvectors that form disjoint strings. This example has $ n=10 $ , a single eigenvalue  $ lambda=2 $ , and five Jordan blocks. $ T_2= 2 &0 &0 &0 &0 &0 &0 &0 &0 &0  1 &2 &0 &0 &0 &0 &0 &0 &0 &0  0 &1 &2 &0 &0 &0 &0 &0 &0 &0  0 &0 &0 &2 &0 &0 &0 &0 &0 &0  0 &0 &0 &1 &2 &0 &0 &0 &0 &0  0 &0 &0 &0 &1 &2 &0 &0 &0 &0  0 &0 &0 &0 &0 &0 &2 &0 &0 &0  0 &0 &0 &0 &0 &0 &1 &2 &0 &0  0 &0 &0 &0 &0 &0 &0 &0 &2 &0  0 &0 &0 &0 &0 &0 &0 &0 &0 &2  qquad {renewcommand{arraystretch}{(i)75} {r} vec{beta}_1mapsunder{t - 2}vec{beta}_2mapsunder{t - 2}vec{beta}_3mapsunder{t - 2}zero  vec{beta}_4mapsunder{t - 2}vec{beta}_5mapsunder{t - 2}vec{beta}_6mapsunder{t - 2}zero  vec{beta}_7mapsunder{t - 2}vec{beta}_8mapsunder{t - 2}zero  vec{beta}_9mapsunder{t - 2}zero  vec{beta}_{10}mapsunder{t - 2}zero } $ . We saw a full example as $ X $ . $ X $ below extends these two to any transformation $ map{t}{C^n}{C^n} $ . The canonical form consists of Jordan blocks containing the eigenvalues. This illustrates such a matrix for $ n=6 $ with eigenvalues $ 3 $ , $ 2 $ , and  $ - 1 $ (examples with full compuations are after the theorem). $ T_3= 3 &0 &0 &0 &0 &0  1 &3 &0 &0 &0 &0  0 &0 &3 &0 &0 &0  0 &0 &0 &2 &0 &0  0 &0 &0 &1 &2 &0  0 &0 &0 &0 &0 & - 1  qquad {renewcommand{arraystretch}{(i)5} {r} vec{beta}_1mapsunder{t - 3}vec{beta}_2mapsunder{t - 3}zero  vec{beta}_3mapsunder{t - 3}zero  vec{beta}_4mapsunder{t - 2}vec{beta}_5mapsunder{t - 2}zero  vec{beta}_6mapsunder{t+1}zero  } $ . It has four blocks, two associated with the eigenvalue  $ 3 $ , and one each with $ 2 $ and  $ - 1 $ . Any transformation $ map{t}{C^n}{C^n} $ can be represented in Jordan form, where each $ J_{lambda} $ is a Jordan block. $ J_{lambda_1} & &text{textit{ - - zeroes - - }}  &J_{lambda_2}  & &ddots  & &text{textit{ - - zeroes - - }} & &J_{lambda_{k}} $ . Restated, any $ nbyn{n} $ matrix is similar to one in this form. In the prior example, if we apply $ t - 3 $ to the basis then it will send two elements, $ vec{beta}_2 $ and  $ vec{beta}_3 $ , to  $ zero $ . This suggests doing the proof by induction since the dimension of the range space  $ rangespace{t - 3} $ is less than that of the starting space. We will do induction on  $ n $ . The $ n=1 $ base case is trivial since any $ nbyn{1} $ matrix is in Jordan form. For the inductive step assume that every transformation of $ C^1 $ , ldots, $ C^{n - 1} $ has a Jordan form representation and fix $ map{t}{C^n}{C^n} $ . Any transformation has at least one eigenvalue  $ lambda_1 $ because it has a minimal polynomial, which has at least one root over the complex numbers. That eigenvalue has a nonzero eigenvector  $ vec{v} $ , so that $ (t - lambda_1)(vec{v})=zero $ . Thus the dimension of $ nullspace{t - lambda_1} $ , the map's nullity, is greater than zero. Since the rank plus the nullity equals the dimension of the domain, the rank of $ t - lambda_1 $ is strictly less than  $ n $ . Write $ W $ for the range space $ rangespace{t - lambda_1} $ and write $ r $ for the rank, its dimension. If $ vec{w}in W=rangespace{t - lambda_1} $ then $ vec{w}inC^n $ and so $ (t - lambda_1)(vec{w})inrangespace{t - lambda_1} $ . Thus the restriction of $ t - lambda_1 $ to  $ W $ induces a transformation of  $ W $ , which we shall also call $ t - lambda_1 $ . The dimension of  $ W $ is less than  $ n $ and so the inductive hypothesis applies and we get a basis for  $ W $ consisting of disjoint strings that are associated with the eigenvalues of $ t - lambda_1 $ on  $ W $ . For any $ lambda $ that is an eigenvalue of  $ t - lambda_1 $ , the transformation of the basis elements will be $ (t - lambda_1) - lambda=t - (lambda_1 - lambda) $ , which we will write as $ t - lambda_s $ . This diagram shows some strings for $ t - lambda_1 $ , that is, where $ lambda=0 $ ; the argument remains valid if there are no such strings. $ {*{10}{l}} vec{w}_{1, n_1} &xmapsto{t - lambda_1} &cdots &xmapsto{t - lambda_1} &vec{w}_{1, 1} &xmapsto{t - lambda_1} &zero  vdotswithin{xmapsto{t - lambda_1}}  vec{w}_{q, n_q} &xmapsto{t - lambda_1} &cdots &xmapsto{t - lambda_1} &vec{w}_{q, 1} &xmapsto{t - lambda_1} &zero  vec{w}_{q+1, n_{q+1}} &xmapsto{t - lambda_2} &cdots &xmapsto{t - lambda_2} &vec{w}_{q+1, 1} &xmapsto{t - lambda_2} &zero  vdotswithin{xmapsto{t - lambda_1}}  vec{w}_{k, n_k} &xmapsto{t - lambda_i} &cdots &xmapsto{t - lambda_i} &vec{w}_{k, 1} &xmapsto{t - lambda_i} &zero  $ . These strings may have differing lengths; for instance, perhaps $ n_1neq n_k $ . The rightmost non - $ zero $ vectors $ vec{w}_{1, 1} $ , ldots, $ vec{w}_{k, 1} $ are in the null space of the associated maps,   $ (t - lambda_1)(vec{w}_{1, 1})=zero $ , ldots, $ (t - lambda_i)(vec{w}_{k, 1})=zero $ . Thus the number of strings associated with $ t - lambda_1 $ , denoted $ q $ in the above diagram, is equal to the dimension of $ Wcapnullspace{t - lambda_1} $ . The string basis above is for the space $ W=rangespace{t - lambda_1} $ . We now expand it to make it a basis for all of  $ C^n $ . First, because each of the vectors $ vec{w}_{1, n_1} $ , ldots,   $ vec{w}_{q, n_q} $ is an element of  $ rangespace{t - lambda_1} $ , each is the image of some vector from  $ C^n $ . Prefix each string with one such vector, $ vec{x}_1 $ , ldots, $ vec{x}_q $ , as shown below. Second, the dimension of $ Wcapnullspace{t - lambda_1} $ is  $ q $ , and the dimension of  $ W $ is  $ r $ , so the bookkeeping requires that there be a subspace $ Ysubseteqnullspace{t - lambda_1} $ with dimension $ r - q $ whose intersection with $ W $ is  $ set{zero} $ . Consequently, pick $ r - q $ linearly independent vectors $ vec{y}_1 $ , ldots, $ vec{y}_{r - q}innullspace{t - lambda_1} $ and incorporate them into the list of strings, again as shown below. $ {*{10}{c}} vec{x}_1 &xmapsto{t - lambda_1} &vec{w}_{1, n_1} &xmapsto{t - lambda_1} &cdots &xmapsto{t - lambda_1} &vec{w}_{1, 1} &xmapsto{t - lambda_1} &zero  vdotswithin{xmapsto{t - lambda_1}}  vec{x}_q &xmapsto{t - lambda_1} &vec{w}_{q, n_q} &xmapsto{t - lambda_1} &cdots &xmapsto{t - lambda_1} &vec{w}_{q, 1} &xmapsto{t - lambda_1} &zero  &&vec{w}_{q+1, n_{q+1}} &xmapsto{t - lambda_2} &cdots &xmapsto{t - lambda_2} &vec{w}_{q+1, 1} &xmapsto{t - lambda_2} &zero  &&vdotswithin{xmapsto{t - lambda_1}}  &&vec{w}_{k, n_k} &xmapsto{t - lambda_i} &cdots &xmapsto{t - lambda_i} &vec{w}_{k, 1} &xmapsto{t - lambda_i} &zero  &&&&&&vec{y}_1 &xmapsto{t - lambda_1} &zero  &&&&&&vdotswithin{xmapsto{t - lambda_1}}  &&&&&&vec{y}_{r - q} &xmapsto{t - lambda_1} &zero  $ . We will show that this is the desired basis for  $ C^n $ . Because of the bookkeeping the number of vectors in the set is equal to the dimension of the space, so we will be done if we verify that its elements are linearly independent. Assume that this is equal to  $ zero $ . $ a_1vec{x}_1+cdots+a_qvec{x}_q +b_{1, n_1}vec{w}_{1, n_1}+cdots+b_{k, 1}vec{w}_{k, 1} +c_1vec{y}_1+cdots+c_{r - q}vec{y}_{r - q} tag{ $ * $ } $ . We first show that that therefore all of the $ a $ 's are zero. Apply $ t - lambda_1 $ to ( $ * $ ). The vectors $ vec{y}_1 $ , ldots, $ vec{y}_{r - q} $ go to  $ zero $ . Each of the vectors $ vec{x}_i $ is sent to $ vec{w}_{i, n_i} $ . As to the $ vec{w} $ 's, they have the property that $ (t - lambda_k), vec{w}_{i, j}=vec{w}_{i, j - 1} $ for some  $ lambda_k $ . Write $ hat{lambda} $ for $ lambda_1 - lambda_k $ to get this. $ (t - lambda_1)(vec{w}_{i, j}) =(t - (lambda_1 - hat{lambda})+hat{lambda})(vec{w}_{i, j}) =vec{w}_{i, j - 1}+hat{lambda}cdotvec{w}_{i, j} tag{ $ ** $ } $ . Thus, applying $ t - lambda_1 $ to ( $ * $ ) ends with a linear combination of $ vec{w} $ 's. These vectors are linearly independent because they form a basis for  $ W $ and so in this linear combination the coefficients are  $ 0 $ . We will be done showing that $ a_i=0 $ for $ i=1, ldots, q $ are zero if we verify that after we apply $ t - lambda_1 $ to ( $ * $ ) then the coefficient of $ vec{w}_{i, n_i} $ is $ a_i $ . But that's true because for $ i=1, ldots, q $ , equation ( $ ** $ ) has $ hat{lambda}=0 $ and so $ vec{w}_{i, n_i} $ is the image of $ x_{i} $ alone. With the $ a $ 's equal to zero, what remains of ( $ * $ ) is this. $ zero = b_{1, n_1}vec{w}_{1, n_1}+cdots+b_{k, 1}vec{w}_{k, 1} +c_1vec{y}_1+cdots+c_{r - q}vec{y}_{r - q} $ . Rewrite as $ - (b_{1, n_1}vec{w}_{1, n_1}+cdots+b_{k, 1}vec{w}_{k, 1})= c_1vec{y}_1+cdots+c_{r - q}vec{y}_{r - q} $ . The $ vec{w} $ 's are from  $ W $ while the $ vec{y} $ 's are from  $ Y $ and the two spaces have only a trivial intersection. Consequently $ b_{1, n_1}vec{w}_{1, n_1}+cdots+b_{k, 1}vec{w}_{k, 1}=zero $ and $ c_1vec{y}_1+cdots+c_{r - q}vec{y}_{r - q}=zero $ , and the $ b $ 's and $ c $ 's are all zero. Technically, to be a canonical form for matrix similarity, Jordan form must be unique. That is, for any square matrix there should be one and only one matrix similar to it that is in Jordan form. We could make the theorem's form unique by arranging the Jordan blocks so the eigenvalues are in some specified order, and then arranging the blocks of subdiagonal ones from longest to shortest. In the examples below, we won't address that. The examples to follow start with a matrix and calculate a Jordan form matrix similar to it. Before those we will make one more point. For instance, in the first example below we find that the eigenvalues are $ 2 $ and  $ 6 $ . To pick the basis elements related to  $ 2 $ we find the null spaces of $ t - 2 $ , $ (t - 2)^2 $ and  $ (t - 2)^3 $ . That is, to find the elements of this example's basis we will look in the generalized null spaces $ gennullspace{t - 2} $ and  $ gennullspace{t - 6} $ . But we need to be sure that the calculation for each map does not affect the calculation for the other. Let $ map{t}{V}{V} $ be a transformation. A subspace $ Msubseteq V $ is $ t $ invariant if whenever $ vec{m}in M $ then $ t(vec{m})in M $ (shorter: $ t(M)subseteq M $ ). A subspace is $ t $ invariant if and only if it is $ t - lambda $ invariant for all scalars $ lambda $ . In particular, if $ lambda_i, lambda_jinC $ are unequal eigenvalues of $ t $ then the spaces $ gennullspace{t - lambda_i} $ and $ genrangespace{t - lambda_i} $ are $ t - lambda_j $ invariant. The right - to - left half of the first sentence is trivial: if the subspace is $ t - lambda $ invariant for all scalars $ lambda $ then taking $ lambda=0 $ shows that it is $ t $ invariant. For the other half suppose that the subspace is $ t $  invariant, so that if $ vec{m}in M $ then $ t(vec{m})in M $ , and let $ lambda $ be a scalar. The subspace $ M $ is closed under linear combinations and so if $ t(vec{m})in M $ then $ t(vec{m}) - lambdavec{m}in M $ . Thus if $ vec{m}in M $ then $ (t - lambda), (vec{m})in M $ . The second sentence follows from the first. The two spaces are $ t - lambda_i $  invariant, so they are $ t $  invariant. Applying the first sentence again gives that they are also $ t - lambda_j $ invariant. This matrix $ T= [r] 2 &0 &1  0 &6 &2  0 &0 &2 $ . has the characteristic polynomial $ (x - 2)^2(x - 6) $ . $ |T - xI|= 2 - x &0 &1  0 &6 - x &2  0 &0 &2 - x =(x - 2)^2(x - 6) $ . First we do the eigenvalue  $ 2 $ . Computation of the powers of $ T - 2I $ , and of the null spaces and nullities, is routine. (Recall our convention of taking $ T $ to represent a transformation $ map{t}{C^3}{C^3} $ with respect to the standard basis.) So the generalized null space $ gennullspace{t - 2} $ has dimension two. We know that the restriction of $ t - 2 $ is nilpotent on this subspace. From the way that the nullities grow we know that the action of $ t - 2 $ on a string basis is $ vec{beta}_1mapstovec{beta}_2mapstozero $ . Thus we can represent the restriction in the canonical form $ N_2= [r] 0 &0  1 &0 =rep{t - 2}{B, B} qquad B_2=sequence{colvec[r]{1  1  - 2}, colvec[r]{ - 2  0  0}} $ . (other choices of basis are possible). Consequently, the action of the restriction of $ t $ to $ gennullspace{t - 2} $ is represented by this Jordan block. $ J_2=N_2+2I=rep{t}{B_2, B_2}= [r] 2 &0  1 &2 $ . The second eigenvalue is $ 6 $ . Its computations are easier. Because the power of $ x - 6 $ in the characteristic polynomial is one, the restriction of $ t - 6 $ to $ gennullspace{t - 6} $ must be nilpotent, of index one (it can't be of index less than one and since $ x - 6 $ is a factor of the characteristic polynomial with the exponent one it can't be of index more than one either). Its action on a string basis must be $ vec{beta}_3mapstozero $ and since it is the zero map, its canonical form $ N_6 $ is the $ nbyn{1} $ zero matrix. Consequently, the canonical form $ J_6 $ for the action of $ t $ on $ gennullspace{t - 6} $ is the $ nbyn{1} $ Jordan block matrix with the single entry $ 6 $ . For the basis we can use any nonzero vector from the generalized null space. $ B_6=sequence{colvec[r]{0  1  0}} $ . Taken together, these two give that the Jordan form of $ T $ is $ rep{t}{B, B}= [r] 2 &0 &0  1 &2 &0  0 &0 &6 $ . where $ B $ is the concatenation of $ B_2 $ and $ B_6 $ . (If we want to be careful about getting a unique Jordan form then we could, for instance, rearrange the basis elements to have the eigenvalues in descending order. But, ascending order is fine also.) This matrix has the same characteristic polynomial as the prior example, $ (x - 2)^2(x - 6) $ . $ T= [r] 2 &2 &1  0 &6 &2  0 &0 &2 $ . But here the action of $ t - 2 $ is stable after only one applicationDash the restriction of $ t - 2 $ to $ gennullspace{t - 2} $ is nilpotent of index one. So the restriction of $ t - 2 $ to the generalized null space acts on a string basis via the two strings $ vec{beta}_1mapstozero $ and $ vec{beta}_2mapstozero $ . We have this Jordan block associated with the eigenvalue  $ 2 $ . $ J_2= [r] 2 &0  0 &2 $ . Thus, the contrast with the prior example is that while the characteristic polynomial tells us to look at the action of $ t - 2 $ on its generalized null space, the characteristic polynomial does not completely describe $ t - 2 $ 's action. We must do some computations to find that the minimal polynomial is $ (x - 2)(x - 6) $ . For the eigenvalue $ 6 $ , the arguments for the second eigenvalue of the prior example apply again: because the power of $ x - 6 $ in the characteristic polynomial is one, the restriction of $ t - 6 $ to $ gennullspace{t - 6} $ must be nilpotent of index one. Alternatively, we can just compute it. Either way, on $ gennullspace{t - 6} $ the restriction $ t - 6 $ 's canonical form $ N_6 $ is the $ nbyn{1} $ zero matrix. The Jordan block $ J_6 $ is the $ nbyn{1} $ matrix with entry $ 6 $ . Therefore the Jordan form for $ T $ is a diagonal matrix and so  $ t $ is a diagonalizable transformation. $ rep{t}{B, B}= [r] 2 &0 &0  0 &2 &0  0 &0 &6 qquad B=cat{B_2}{B_6} =sequence{colvec[r]{1  0  0}, colvec[r]{0  1  - 2}, colvec[r]{1  2  0}} $ . Of the three basis vectors, the first two come from the nullspace of $ t - 2 $ and the third is from the nullspace of  $ t - 6 $ . A bit of computing with $ T= [r] - 1 &4 &0 &0 &0  0 &3 &0 &0 &0  0 & - 4 & - 1 &0 &0  3 & - 9 & - 4 &2 & - 1  1 &5 &4 &1 &4 $ . shows that its characteristic polynomial is $ (x - 3)^3(x+1)^2 $ . This table shows that the restriction of $ t - 3 $ to $ gennullspace{t - 3} $ acts on a string basis via the two strings $ vec{beta}_1mapstovec{beta}_2mapstozero $ and $ vec{beta}_3mapstozero $ . A similar calculation for the other eigenvalue gives that the restriction of $ t+1 $ to its generalized null space acts on a string basis via the two separate strings $ vec{beta}_4mapstozero $ and $ vec{beta}_5mapstozero $ . Therefore $ T $ is similar to this Jordan form matrix. $ [r] - 1 &0 &0 &0 &0  0 & - 1 &0 &0 &0  0 &0 &3 &0 &0  0 &0 &1 &3 &0  0 &0 &0 &0 &3 $ .
noindenttextit{This section is optional. It is a prerequisite only for the final two sections of Chapter Five, and some Topics.} We have described projection from $ Re^3 $ into its $ xy $ - plane subspace as a shadow map. This shows why but it also shows that some shadows fall upward. vspace*{1ex} graphic is poorly drawn and sticks down into text So perhaps a better description is: the projection of $ vec{v} $ is the vector $ vec{p} $ in the plane with the property that someone standing on $ vec{p} $ and looking straight up or downDash that is, looking orthogonally to the planeDash sees the tip of $ vec{v} $ . In this section we will generalize this to other projections, orthogonal and non - orthogonal. We first consider orthogonal projection of a vector $ vec{v} $ into a line $ ell $ . This shows a figure walking out on the line to a point $ vec{p}/ $ such that the tip of $ vec{v} $ is directly above them, where ``above'' does not mean parallel to the $ y $ - axis but instead means orthogonal to the line. Since the line is the span of some vector $ ell=set{ccdotvec{s}suchthat cinRe} $ , we have a coefficient $ c_{vec{p}}, $ with the property that $ vec{v} - c_{vec{p}}vec{s}, $ is orthogonal to $ c_{vec{p}}vec{s} $ . To solve for this coefficient, observe that because $ vec{v} - c_{vec{p}}vec{s}, $ is orthogonal to a scalar multiple of $ vec{s} $ , it must be orthogonal to $ vec{s}, $ itself. Then $ (vec{v} - c_{vec{p}}vec{s})dotprod vec{s}=0 $ gives that $ c_{vec{p}}=vec{v}dotprod vec{s}/vec{s}dotprod vec{s} $ . The definend{orthogonal projection of $ vec{v} $ into the line spanned by a nonzero $ vec{s}, $ } is this vector. $ proj{vec{v}}{spanof{vec{s}, }}= frac{ vec{v}dotprodvec{s} }{ vec{s}dotprodvec{s} }cdotvec{s} $ . noindent (That says `spanned by $ vec{s}, $ ' instead the more formal `span of the set $ set{vec{s}, } $ '. This more casual phrase is common.) To orthogonally project the vector $ binom{2}{3} $ into the line $ y=2x $ , first pick a direction vector for the line. $ vec{s}=colvec[r]{1  2} $ . The calculation is easy. In $ Re^3 $ , the orthogonal projection of a general vector $ colvec{x  y  z} $ . into the $ y $ - axis is $ frac{ colvec{x  y  z}dotprodcolvec[r]{0  1  0} }{ colvec[r]{0  1  0}dotprodcolvec[r]{0  1  0} } cdotcolvec[r]{0  1  0}=colvec{0  y  0} $ . which matches our intuitive expectation. The picture above showing the figure walking out on the line until $ vec{v} $ 's tip is overhead is one way to think of the orthogonal projection of a vector into a line. We finish this subsection with two other ways. A railroad car left on an east - west track without its brake is pushed by a wind blowing toward the northeast at fifteen miles per hour; what speed will the car reach? For the wind we use a vector of length $ 15 $ that points toward the northeast. $ vec{v}=colvec[r]{15sqrt{1/2}  15sqrt{1/2}} $ . The car is only affected by the part of the wind blowing in the east - west directionDash the part of $ vec{v} $ in the direction of the $ x $ - axis is this (the picture has the same perspective as the railroad car picture above). So the car will reach a velocity of $ 15sqrt{1/2} $ miles per hour toward the east. Thus, another way to think of the picture that precedes the definition is that it shows $ vec{v} $ as decomposed into two parts, the part $ vec{p} $ with the line, and the part that is orthogonal to the line (shown above on the north - south axis). These two are non - interacting in the sense that the east - west car is not at all affected by the north - south part of the wind (see $ X $ ). So we can think of the orthogonal projection of $ vec{v} $ into the line spanned by $ vec{s}, $ as the part of $ vec{v}, $ that lies in the direction of $ vec{s} $ . Still another useful way to think of orthogonal projection into a line is to have the person stand on the vector, not the line. This person holds a rope looped over the line. As they pull, the loop slides on the line. When it is tight, the rope is orthogonal to the line. That is, we can think of the projection $ vec{p}, $ as being the vector in the line that is closest to $ vec{v} $ (see $ X $ ). A submarine is tracking a ship moving along the line $ y=3x+2 $ . Torpedo range is one - half mile. If the sub stays where it is, at the origin on the chart below, will the ship pass within range? The formula for projection into a line does not immediately apply because the line doesn't pass through the origin, and so isn't the span of any $ vec{s} $ . To adjust for this, we start by shifting the entire map down two units. Now the line is $ y=3x $ , a subspace. We project to get the point $ vec{p} $ on the line closest to $ vec{v}=colvec[r]{0  - 2} $ . the sub's shifted position. $ vec{p}=frac{colvec[r]{0  - 2}dotprodcolvec[r]{1  3}}{ colvec[r]{1  3}dotprodcolvec[r]{1  3}} cdot colvec[r]{1  3}=colvec[r]{ - 3/5  - 9/5} $ . The distance between $ vec{v} $ and $ vec{p} $ is about $ 0.63 $  miles. The ship will never be in range. The prior subsection suggests that projecting $ vec{v} $ into the line spanned by $ vec{s} $ decomposes that vector into two parts that are orthogonal and so are ``non - interacting.'' We now develop that suggestion. Vectors $ vec{v}_1, dots, vec{v}_kinRe^n $ are mutually orthogonal/ when any two are orthogonal: if $ ineq j $ then the dot product $ vec{v}_idotprodvec{v}_j $ is zero. If the vectors in a set $ set{vec{v}_1, dots, vec{v}_k}subsetRe^n $ are mutually orthogonal and nonzero then that set is linearly independent. Consider $ zero=c_1vec{v}_1+c_2vec{v}_2+dots+c_kvec{v}_k $ . For $ iin set{1, .., , k} $ , taking the dot product of $ vec{v}_i $ with both sides of the equation $ vec{v}_idotprod (c_1vec{v}_1+c_2vec{v}_2+dots+c_kvec{v}_k) =vec{v}_idotprodzero $ , which gives $ c_icdot(vec{v}_idotprodvec{v}_i)=0 $ , shows that $ c_i= 0 $ since $ vec{v}_ineqzero $ . In a $ k $  dimensional vector space, if the vectors in a size  $ k $ set are mutually orthogonal and nonzero then that set is a basis for the space. Any linearly independent size  $ k $ subset of a $ k $  dimensional space is a basis. Of course, the converse of $ X $ does not holdDash not every basis of every subspace of $ Re^n $ has mutually orthogonal vectors. However, we can get the partial converse that for every subspace of $ Re^n $ there is at least one basis consisting of mutually orthogonal vectors. The members $ vec{beta}_1 $ and $ vec{beta}_2 $ of this basis for $ Re^2 $ are not orthogonal. We will derive from $ B $ a new basis for the space $ sequence{vec{kappa}_1, vec{kappa}_2} $ consisting of mutually orthogonal vectors. The first member of the new basis is just $ vec{beta}_1 $ . $ vec{kappa}_1=colvec[r]{4  2} $ . For the second member of the new basis, we subtract from $ vec{beta}_2 $ the part in the direction of $ vec{kappa}_1 $ . This leaves the part of $ vec{beta}_2 $ that is orthogonal to $ vec{kappa}_1 $ . By the corollary $ sequence{vec{kappa}_1, vec{kappa}_2} $ is a basis for $ Re^2 $ . An orthogonal basis/ for a vector space is a basis of mutually orthogonal vectors. To produce from this basis for $ Re^3 $ $ B=sequence{ colvec[r]{1  1  1}, colvec[r]{0  2  0}, colvec[r]{1  0  3} } $ . an orthogonal basis, start by taking the first vector unchanged. $ vec{kappa}_1=colvec[r]{1  1  1} $ . Get $ vec{kappa}_2 $ by subtracting from $ vec{beta}_2 $ its part in the direction of $ vec{kappa}_1 $ . $ vec{kappa}_2=colvec[r]{0  2  0} - proj{colvec[r]{0  2  0}}{spanof{vec{kappa}_1}} =colvec[r]{0  2  0} - colvec[r]{2/3  2/3  2/3} =colvec[r]{ - 2/3  4/3  - 2/3} $ . Find $ vec{kappa}_3 $ by subtracting from $ vec{beta}_3 $ the part in the direction of $ vec{kappa}_1 $ and also the part in the direction of $ vec{kappa}_2 $ . $ vec{kappa}_3=colvec[r]{1  0  3} - proj{colvec[r]{1  0  3}}{spanof{vec{kappa}_1}} - proj{colvec[r]{1  0  3}}{spanof{vec{kappa}_2}} =colvec[r]{ - 1  0  1} $ . As above, the corollary gives that the result is a basis for $ Re^3 $ . $ sequence{ colvec[r]{1  1  1}, colvec[r]{ - 2/3  4/3  - 2/3}, colvec[r]{ - 1  0  1} } $ . [Gram - Schmidt orthogonalization] If $ sequence{vec{beta}_1, ldotsvec{beta}_k} $ is a basis for a subspace of $ Re^n $ then the vectors $ vec{kappa}_1 &=vec{beta}_1  vec{kappa}_2 &=vec{beta}_2 - proj{vec{beta}_2}{spanof{vec{kappa}_1}}  vec{kappa}_3 &=vec{beta}_3 - proj{vec{beta}_3}{spanof{vec{kappa}_1}} - proj{vec{beta}_3}{spanof{vec{kappa}_2}}  &vdotswithin{=}  vec{kappa}_k &=vec{beta}_k - proj{vec{beta}_k}{spanof{vec{kappa}_1}} - cdots - proj{vec{beta}_k}{spanof{vec{kappa}_{k - 1}}} $ . form an orthogonal basis for the same subspace. This is restricted to $ Re^n $ only because we have not given a definition of orthogonality for other spaces. We will use induction to check that each $ vec{kappa}_i $ is nonzero, is in the span of $ sequence{vec{beta}_1, ldots, vec{beta}_i} $ , and is orthogonal to all preceding vectors $ vec{kappa}_1dotprodvec{kappa}_i= cdots =vec{kappa}_{i - 1}dotprodvec{kappa}_{i}=0 $ . Then $ X $ gives that $ sequence{vec{kappa}_1, ldots, vec{kappa}_k} $ is a basis for the same space as is the starting basis. We shall only cover the cases up to $ i=3 $ , to give the sense of the argument. The full argument is $ X $ . The $ i=1 $ case is trivial; taking $ vec{kappa}_1 $ to be $ vec{beta}_1 $ makes it a nonzero vector since $ vec{beta}_1 $ is a member of a basis, it is obviously in the span of $ sequence{vec{beta}_1} $ , and the `orthogonal to all preceding vectors' condition is satisfied vacuously. In the $ i=2 $ case the expansion $ vec{kappa}_2=vec{beta}_2 - proj{vec{beta}_2}{spanof{vec{kappa}_1}}= vec{beta}_2 - frac{vec{beta}_2dotprodvec{kappa}_1}{ vec{kappa}_1dotprodvec{kappa}_1} cdotvec{kappa}_1 = vec{beta}_2 - frac{vec{beta}_2dotprodvec{kappa}_1}{ vec{kappa}_1dotprodvec{kappa}_1} cdotvec{beta}_1 $ . shows that $ vec{kappa}_2neq zero $ or else this would be a non - trivial linear dependence among the $ vec{beta} $ 's (it is nontrivial because the coefficient of $ vec{beta}_2 $ is $ 1 $ ). It also shows that $ vec{kappa}_2 $ is in the span of $ sequence{vec{beta}_1, vec{beta}_2} $ . And, $ vec{kappa}_2 $ is orthogonal to the only preceding vector $ vec{kappa}_1dotprodvec{kappa}_2= vec{kappa}_1dotprod(vec{beta}_2 - proj{vec{beta}_2}{spanof{vec{kappa}_1}})=0 $ . because this projection is orthogonal. The $ i=3 $ case is the same as the $ i=2 $ case except for one detail. As in the $ i=2 $ case, expand the definition. $ vec{kappa}_3 &=vec{beta}_3 - frac{vec{beta}_3dotprodvec{kappa}_1}{ vec{kappa}_1dotprodvec{kappa}_1} cdotvec{kappa}_1 - frac{vec{beta}_3dotprodvec{kappa}_2}{ vec{kappa}_2dotprodvec{kappa}_2} cdotvec{kappa}_2  &=vec{beta}_3 - frac{vec{beta}_3dotprodvec{kappa}_1}{ vec{kappa}_1dotprodvec{kappa}_1} cdotvec{beta}_1 - frac{vec{beta}_3dotprodvec{kappa}_2}{ vec{kappa}_2dotprodvec{kappa}_2} cdotbigl(vec{beta}_2 - frac{vec{beta}_2dotprodvec{kappa}_1}{ vec{kappa}_1dotprodvec{kappa}_1} cdotvec{beta}_1bigr) $ . By the first line $ vec{kappa}_3neqzero $ , since $ vec{beta}_3 $ isn't in the span  $ spanof{vec{beta}_1, vec{beta}_2} $ and therefore by the inductive hypothesis it isn't in the span  $ spanof{vec{kappa}_1, vec{kappa}_2} $ . By the second line $ vec{kappa}_3 $ is in the span of the first three $ vec{beta} $ 's. Finally, the calculation below shows that $ vec{kappa}_3 $ is orthogonal to $ vec{kappa}_1 $ . $ vec{kappa}_1dotprodvec{kappa}_3 &=vec{kappa}_1dotprodbigl(;vec{beta}_3 - proj{vec{beta}_3}{spanof{vec{kappa}_1}} - proj{vec{beta}_3}{spanof{vec{kappa}_2}};bigr)  &=vec{kappa}_1dotprodbigl(vec{beta}_3 - proj{vec{beta}_3}{spanof{vec{kappa}_1}}bigr) - vec{kappa}_1dotprodproj{vec{beta}_3}{spanof{vec{kappa}_2}}  &=0 $ . (Here is the difference with the $ i=2 $ case: as happened for $ i=2 $ the first term is  $ 0 $ because this projection is orthogonal, but here the second term in the second line is  $ 0 $ because $ vec{kappa}_1 $ is orthogonal to $ vec{kappa}_2 $ and so is orthogonal to any vector in the line spanned by $ vec{kappa}_2 $ .) A similar check shows that $ vec{kappa}_3 $ is also orthogonal to $ vec{kappa}_2 $ . In addition to having the vectors in the basis be orthogonal, we can also normalize each vector by dividing by its length, to end with an orthonormal basis. . From the orthogonal basis of $ X $ , normalizing produces this orthonormal basis. $ sequence{ colvec[r]{1/sqrt{3}  1/sqrt{3}  1/sqrt{3}}, colvec[r]{ - 1/sqrt{6}  2/sqrt{6}  - 1/sqrt{6}}, colvec[r]{ - 1/sqrt{2}  0  1/sqrt{2}} } $ . noindent Besides its intuitive appeal, and its analogy with the standard basis $ stdbasis_n $ for $ Re^n $ , an orthonormal basis also simplifies some computations. $ X $ is an example. noindenttextit{This subsection uses material from the optional earlier subsection on Combining Subspaces.} The prior subsections project a vector into a line by decomposing it into two parts: the part in the line $ proj{vec{v}, }{spanof{vec{s}, }} $ and the rest $ vec{v} - proj{vec{v}, }{spanof{vec{s}, }} $ . To generalize projection to arbitrary subspaces we will follow this decomposition idea. Let a vector space be a direct sum $ V=Mdirectsum N $ . Then for any $ vec{v}in V $ with $ vec{v}=vec{m}+vec{n} $ where $ vec{m}in M, , vec{n}in N $ , the definend{projection of $ vec{v} $ into $ M $ along $ N $ } is $ proj{vec{v}, }{M, N}=vec{m} $ . This definition applies in spaces where we don't have a ready definition of orthogonal. (Definitions of orthogonality for spaces other than the $ Re^n $ are perfectly possible but we haven't seen any in this book.) The space $ matspace_{nbyn{2}} $ of $ nbyn{2} $ matrices is the direct sum of these two. $ M=set{ a &b  0 &0 suchthat a, binRe } qquad N=set{ 0 &0  c &d suchthat c, dinRe } $ . To project $ A=[r] 3 &1  0 &4 $ . into $ M $ along $ N $ , we first fix bases for the two subspaces. $ B_{M}=sequence{ [r] 1 &0  0 &0 , [r] 0 &1  0 &0 } qquad B_{N}=sequence{ [r] 0 &0  1 &0 , [r] 0 &0  0 &1 } $ . Their concatenation $ B=cat{B_{M}}{B_{N}}=sequence{ [r] 1 &0  0 &0 , [r] 0 &1  0 &0 , [r] 0 &0  1 &0 , [r] 0 &0  0 &1 } $ . is a basis for the entire space because $ matspace_{nbyn{2}} $ is the direct sum. So we can use it to represent $ A $ . $ [r] 3 &1  0 &4 = 3cdot[r] 1 &0  0 &0 +1cdot[r] 0 &1  0 &0 +0cdot[r] 0 &0  1 &0 +4cdot[r] 0 &0  0 &1 $ . The projection of $ A $ into $ M $ along $ N $ keeps the $ M $ part and drops the $ N $ part. $ proj{[r] 3 &1  0 &4 }{M, N} = 3cdot[r] 1 &0  0 &0 +1cdot[r] 0 &1  0 &0 =[r] 3 &1  0 &0 $ . Both subscripts on $ proj{vec{v}, }{M, N} $ are significant. The first subscript $ M $ matters because the result of the projection is a member of $ M $ . For an example showing that the second one matters, fix this plane subspace of $ Re^3 $ and its basis. $ M=set{colvec{x  y  z}suchthat y - 2z=0} qquad B_M=sequence{ colvec[r]{1  0  0}, colvec[r]{0  2  1} } $ . We will compare the projections of this element of  $ Re^3 $ $ vec{v}=colvec[r]{2  2  5} $ . into $ M $ along these two subspaces (verification that $ Re^3=Mdirectsum N $ and $ Re^3=Mdirectsum hat{N} $ is routine). $ N=set{kcolvec[r]{0  0  1}suchthat kinRe} qquad hat{N}=set{kcolvec[r]{0  1  - 2}suchthat kinRe} $ . Here are natural bases for $ N $ and $ hat{N} $ . $ B_N=sequence{ colvec[r]{0  0  1} } qquad B_{hat{N}}=sequence{ colvec[r]{0  1  - 2} } $ . To project into $ M $ along  $ N $ , represent $ vec{v} $ with respect to the concatenation $ cat{B_M}{B_N} $ $ colvec[r]{2  2  5}= 2cdotcolvec[r]{1  0  0}+ 1cdotcolvec[r]{0  2  1}+ 4cdotcolvec[r]{0  0  1} $ . and drop the $ N $ term. $ proj{vec{v}, }{M, N} =2cdotcolvec[r]{1  0  0}+ 1cdotcolvec[r]{0  2  1} =colvec[r]{2  2  1} $ . To project into  $ M $ along  $ hat{N} $ represent $ vec{v} $ with respect to $ cat{B_M}{B_{hat{N}}} $ $ colvec[r]{2  2  5}= 2cdotcolvec[r]{1  0  0} +(9/5)cdotcolvec[r]{0  2  1} - (8/5)cdotcolvec[r]{0  1  - 2} $ . and omit the $ hat{N} $ part. $ proj{vec{v}, }{M, hat{N}} = 2cdotcolvec[r]{1  0  0} +(9/5)cdotcolvec[r]{0  2  1} =colvec[r]{2  18/5  9/5} $ . So projecting along different subspaces can give different results. These pictures compare the two maps. Both show that the projection is indeed `into' the plane and `along' the line. Notice that the projection along $ N $ is not orthogonal since there are members of the plane $ M $ that are not orthogonal to the dotted line. But the projection along $ hat{N} $ is orthogonal. We have seen two projection operations, orthogonal projection into a line as well as this subsections's projection into an  $ M $ and along an  $ N $ , and we naturally ask whether they are related. The right - hand picture above suggests the answerDash orthogonal projection into a line is a special case of this subsection's projection; it is projection along a subspace perpendicular to the line. The orthogonal complement/ of a subspace $ M $ of $ Re^n $ is $ M^perp=set{vec{v}inRe^nsuchthat vec{v} text{ is perpendicular to all vectors in }M} $ . (read `` $ M $ perp''). The orthogonal projection $ proj{vec{v}, }{M} $ of a vector is its projection into $ M $ along $ M^perp $ . In $ Re^3 $ , to find the orthogonal complement of the plane $ P=set{colvec{x  y  z}suchthat 3x+2y - z=0 } $ . we start with a basis for $ P $ . $ B=sequence{colvec[r]{1  0  3}, colvec[r]{0  1  2} } $ . Any $ vec{v} $ perpendicular to every vector in $ B $ is perpendicular to every vector in the span of $ B $ (the proof of this is $ X $ ). Therefore, the subspace $ P^perp $ consists of the vectors that satisfy these two conditions. $ colvec[r]{1  0  3}dotprodcolvec{v_1  v_2  v_3}=0 qquad colvec[r]{0  1  2}dotprodcolvec{v_1  v_2  v_3}=0 $ . Those conditions give a linear system. $ P^perp =set{colvec{v_1  v_2  v_3}suchthat [r] 1 &0 &3  0 &1 &2 colvec{v_1  v_2  v_3} =colvec[r]{0  0} } $ . We are thus left with finding the null space of the map represented by the matrix, that is, with calculating the solution set of the homogeneous linear system. $ {3} v_1 & & &+ &3v_3 &= &0  & &v_2 &+ &2v_3 &= &0 quadLongrightarrowquad P^perp=set{kcolvec[r]{ - 3  - 2  1}suchthat kinRe} $ . Where $ M $ is the $ xy $ - plane subspace of $ Re^3 $ , what is $ M^perp $ ? A common first reaction is that $ M^perp $ is the $ yz $ - plane but that's not right because some vectors from the $ yz $ - plane are not perpendicular to every vector in the $ xy $ - plane. Instead $ M^perp $ is the $ z $ - axis, since proceeding as in the prior example and taking the natural basis for the $ xy $ - plane gives this. $ M^perp = set{colvec{x  y  z}suchthat [r] 1 &0 &0  0 &1 &0 colvec{x  y  z}= colvec[r]{0  0} ;} = set{colvec{x  y  z}suchthat x=0 text{ and }y=0 } $ . If $ M $ is a subspace of $ Re^n $ then its orthogonal complement $ M^perp $ is also a subspace. The space is the direct sum of the two $ Re^n=Mdirectsum M^perp $ . For any $ vec{v}inRe^n $ the vector $ vec{v} - proj{vec{v}, }{M} $ is perpendicular to every vector in $ M $ . First, the orthogonal complement $ M^perp $ is a subspace of $ Re^n $ because it is a null space, namely the null space of the orthogonal projection map. To show that the space  $ R^n $ is the direct sum of the two, start with any basis $ B_M=sequence{vec{mu}_1, dots, vec{mu}_k} $ for $ M $ . Expand it to a basis for the entire space and then apply the Gram - Schmidt process to get an orthogonal basis $ K=sequence{vec{kappa}_1, dots, vec{kappa}_n} $ for $ Re^n $ . This $ K $ is the concatenation of two bases: $ sequence{vec{kappa}_1, dots, vec{kappa}_k} $ with the same number of members,   $ k $ , as $ B_M $ , and $ D=sequence{vec{kappa}_{k+1}, dots, vec{kappa}_n} $ . The first is a basis for $ M $ so if we show that the second is a basis for $ M^perp $ then we will have that the entire space is the direct sum. $ X $ from the prior subsection proves this about any orthogonal basis: each vector $ vec{v} $ in the space is the sum of its orthogonal projections into the lines spanned by the basis vectors. $ vec{v}=proj{vec{v}, }{spanof{vec{kappa}_1}} +dots+proj{vec{v}, }{spanof{vec{kappa}_n}} tag*{( $ * $ )} $ . To check this, represent the vector as $ vec{v}=r_1vec{kappa}_1+dots+r_nvec{kappa}_n $ , apply $ vec{kappa}_i $ to both sides $ vec{v}dotprodvec{kappa}_i =left(r_1vec{kappa}_1+dots+r_nvec{kappa}_nright) dotprodvec{kappa}_i =r_1cdot 0+dots+r_icdot(vec{kappa}_idotprodvec{kappa}_i) +dots+r_ncdot 0 $ , and solve to get $ r_i=(vec{v}dotprodvec{kappa}_i)/(vec{kappa}_idotprodvec{kappa}_i) $ , as desired. Any member of the span of $ D $ is orthogonal to any vector in $ M $ so the span of $ D $ is a subset of $ M^perp $ . To show that $ D $ is a basis for $ M^perp $ we need only show the other containment, that any $ vec{w}in M^perp $ is an element of the span of  $ D $ . The prior paragraph works for this. Any $ vec{w}in M^perp $ gives this on projections into basis vectors from $ M $ : $ proj{vec{w}, }{spanof{vec{kappa}_1}}=zero, dots, , proj{vec{w}, }{spanof{vec{kappa}_k}}=zero $ . Therefore equation ( $ * $ ) gives that $ vec{w} $ is a linear combination of $ vec{kappa}_{k+1}, dots, vec{kappa}_n $ . Thus $ D $ is a basis for $ M^perp $ and $ Re^n $ is the direct sum of the two. The final sentence of the lemma is proved in much the same way. Write $ vec{v}=proj{vec{v}, }{spanof{vec{kappa}_1}} +dots+proj{vec{v}, }{spanof{vec{kappa}_n}} $ . Then $ proj{vec{v}, }{M} $ keeps only the $ M $ part and drops the $ M^perp $ part: $ proj{vec{v}, }{M}=proj{vec{v}, }{spanof{vec{kappa}_{1}}} +dots+proj{vec{v}, }{spanof{vec{kappa}_k}} $ . Therefore $ vec{v} - proj{vec{v}, }{M} $ consists of a linear combination of elements of $ M^perp $ and so is perpendicular to every vector in $ M $ . Given a subspace, we could compute the orthogonal projection into that subspace by following the steps of that proof: finding a basis, expanding it to a basis for the entire space, applying Gram - Schmidt to get an orthogonal basis, and projecting into each linear subspace. However we will instead use a convenient formula. Let $ M $ be a subspace of $ Re^n $ with basis $ sequence{vec{beta}_1, dots, vec{beta}_k} $ and let $ A $ be the matrix whose columns are the $ vec{beta} $ 's. Then for any $ vec{v}inRe^n $ the orthogonal projection is $ proj{vec{v}, }{M}=c_1vec{beta}_1+dots+c_kvec{beta}_k $ , where the coefficients $ c_i $ are the entries of the vector $ (trans{A}A)^{ - 1}trans{A}cdotvec{v} $ . That is, $ proj{vec{v}, }{M}=A(trans{A}A)^{ - 1}trans{A}cdotvec{v} $ . The vector $ proj{vec{v}}{M} $ is a member of $ M $ and so is a linear combination of basis vectors $ c_1cdotvec{beta}_1+dots+c_kcdotvec{beta}_k $ . Since $ A $ 's columns are the $ vec{beta} $ 's, there is a $ vec{c}inRe^k $ such that $ proj{vec{v}, }{M}=Avec{c} $ . To find $ vec{c} $ note that the vector $ vec{v} - proj{vec{v}, }{M} $ is perpendicular to each member of the basis so $ zero =trans{A}bigl( vec{v} - Avec{c} bigr) =trans{A}vec{v} - trans{A}Avec{c} $ . and solving gives this (showing that $ trans{A}A $ is invertible is an exercise). $ vec{c}=bigl( trans{A}Abigr)^{ - 1}trans{A}cdotvec{v} $ . Therefore $ proj{vec{v}, }{M}=Acdotvec{c}=A(trans{A}A)^{ - 1}trans{A}cdotvec{v} $ , as required. To orthogonally project this vector into this subspace $ vec{v}=colvec[r]{1  - 1  1} qquad P=set{colvec{x  y  z}suchthat x+z=0} $ . first make a matrix whose columns are a basis for the subspace $ A=[r] 0 &1  1 &0  0 & - 1 $ . and then compute. $ Abigl(trans{A}Abigr)^{ - 1}trans{A} &=[r] 0 &1  1 &0  0 & - 1 [r] 1 &0  0 &1/2 [r] 0 &1 &0  1 &0 & - 1  &= [r] 1/2 &0 & - 1/2  0 &1 &0  - 1/2 &0 &1/2 $ . With the matrix, calculating the orthogonal projection of any vector into $ P $ is easy. $ proj{vec{v}}{P}= [r] 1/2 &0 & - 1/2  0 &1 &0  - 1/2 &0 &1/2 colvec[r]{1  - 1  1} =colvec[r]{0  - 1  0} $ . Note, as a check, that this result is indeed in $ P $ .
Systems of linear equations are common in science and mathematics. These two examples from high school science give a sense of how they arise. The first example is from hypertarget{ex:Statics}{Statics}. Suppose that we have three objects, we know that one has a mass of 2 kg, and we want to find the two unknown masses. Experimentation with a meter stick produces these two balances. For the masses to balance we must have that the sum of moments on the left equals the sum of moments on the right, where the moment of an object is its mass times its distance from the balance point. That gives a system of two linear equations. $ odd formatting to match chem problem below 40h + 15c &= 100  25c &= 50+50h $ . The second example is from Chemistry. We can mix, under controlled conditions, toluene $ hbox{C}_7hbox{H}_8 $ and nitric acid $ hbox{H}hbox{N}hbox{O}_3 $ to produce trinitrotoluene $ hbox{C}_7hbox{H}_5hbox{O}_6hbox{N}_3 $ along with the byproduct water (conditions have to be very well controlledDash trinitrotoluene is better known as TNT). In what proportion should we mix them? The number of atoms of each element present before the reaction $ x, {rm C}_7{rm H}_8 + y, {rm H}{rm N}{rm O}_3 quadlongrightarrowquad z, {rm C}_7{rm H}_5{rm O}_6{rm N}_3 + w, {rm H}_2{rm O} tag*{} $ . must equal the number present afterward. Applying that in turn to the elements C, H, N, and O gives this system. $ odd formatting to state more naturally 7x &= 7z  8x +1y &= 5z+2w  1y &= 3z  3y &= 6z+1w $ . Both examples come down to solving a system of equations. In each system, the equations involve only the first power of each variable. This chapter shows how to solve any such system of equations. A linear combination of $ x_1 $ , ldots, $ x_n $ has the form $ a_1x_1+a_2x_2+a_3x_3+cdots+a_nx_n $ . where the numbers $ a_1, ldots , a_ninRe $ are the combination's coefficients. A linear equation in the variables $ x_1 $ , ldots, $ x_n $ has the form $ a_1x_1+a_2x_2+a_3x_3+cdots+a_nx_n=d $ where $ dinRe $ is the constant. An $ n $ - tuple $ (s_1, s_2, ldots , s_n)inRe^n $ is a solution of, or satisfies, that equation if substituting the numbers $ s_1 $ , ldots, $ s_n $ for the variables gives a true statement: $ a_1s_1+a_2s_2+cdots+a_ns_n=d $ . A system of linear equations $ {4} a_{1, 1}x_1 &+ &a_{1, 2}x_2 &+ &cdots &+ &a_{1, n}x_n &= &d_1  a_{2, 1}x_1 &+ &a_{2, 2}x_2 &+ &cdots &+ &a_{2, n}x_n &= &d_2  & & & & & & &vdotswithin{=}  a_{m, 1}x_1 &+ &a_{m, 2}x_2 &+ &cdots &+ &a_{m, n}x_n &= &d_m $ . has the solution $ (s_1, s_2, ldots , s_n) $ if that $ n $ - tuple is a solution of all of the equations. The combination $ 3x_1 + 2x_2 $ of $ x_1 $ and $ x_2 $ is linear. The combination $ 3x_1^2 + 2x_2 $ is not a linear function of $ x_1 $ and $ x_2 $ , nor is $ 3x_1 + 2sin(x_2) $ . We usually take $ x_1 $ , ldots, $ x_n $ to be unequal to each other because in a sum with repeats we can rearrange to make the elements unique, as with $ 2x+3y+4x=6x+3y $ . We sometimes include terms with a zero coefficient, as in $ x - 2y+0z $ , and at other times omit them, depending on what is convenient. The ordered pair $ ( - 1, 5) $ is a solution of this system. $ {2} 3x_1 &+ &2x_2 &= &7  - x_1 &+ &x_2 &= &6 $ . In contrast, $ (5, - 1) $ is not a solution. Finding the set of all solutions is solving the system. We don't need guesswork or good luck; there is an algorithm that always works. This algorithm is Gauss's Method (or Gaussian elimination or linear elimination ). To solve this system $ {3} & & & &3x_3 &= &9  x_1 &+ &5x_2 & - &2x_3 &= &2  frac{1}{3}x_1 &+ &2x_2 & & &= &3 $ . we transform it, step by step, until it is in a form that we can easily solve. The first transformation rewrites the system by interchanging the first and third row. $ quad &grstep{ text{swap row 1 with row 3} } {3} frac{1}{3}x_1 &+ &2x_2 & & &= &3  x_1 &+ &5x_2 & - &2x_3 &= &2  & & & &3x_3 &= &9 $ . The second transformation rescales the first row by a factor of  $ 3 $ . $ quad &grstep{ text{multiply row 1 by 3} } {3} x_1 &+ &6x_2 & & &= &9  x_1 &+ &5x_2 & - &2x_3 &= &2  & & & &3x_3 &= &9 $ . The third transformation is the only nontrivial one in this example. We mentally multiply both sides of the first row by $ - 1 $ , mentally add that to the second row, and write the result in as the new second row. $ &grstep{ text{add $ - 1 $ times row 1 to row 2} } {3} x_1 &+ &6x_2 & & &= &9  & & - x_2 & - &2x_3 &= & - 7  & & & &3x_3 &= &9 $ . These steps have brought the system to a form where we can easily find the value of each variable. The bottom equation shows that $ x_3=3 $ . Substituting $ 3 $ for $ x_3 $ in the middle equation shows that $ x_2=1 $ . Substituting those two into the top equation gives that $ x_1=3 $ . Thus the system has a unique solution; the solution set is set{(3, 1, 3)}. We will use Gauss's Method throughout the book. It is fast and easy. We will now show that it is also safe: Gauss's Method never loses solutions nor does it ever pick up extraneous solutions, so that a tuple is a solution to the system before we apply the method if and only if it is a solution after. [Gauss's Method] If a linear system is changed to another by one of these operations setlength{sep}{0ex} an equation is swapped with another an equation has both sides multiplied by a nonzero constant an equation is replaced by the sum of itself and a multiple of another then the two systems have the same set of solutions. Each of the three operations has a restriction. Multiplying a row by  $ 0 $ is not allowed because obviously that can change the solution set. Similarly, adding a multiple of a row to itself is not allowed because adding  $ - 1 $ times the row to itself has the effect of multiplying the row by  $ 0 $ . And we disallow swapping a row with itself, to make some results in the fourth chapter easier. Besides, it's pointless. We will cover the equation swap operation here. The other two cases are similar and are $ X $ . Consider a linear system. $ {4} a_{1, 1}x_1 &+ &a_{1, 2}x_2 &+ &cdots &+ &a_{1, n}x_n &= &d_1hfillhbox{}  & & & & & & &vdotswithin{=}  a_{i, 1}x_1 &+ &a_{i, 2}x_2 &+ &cdots &+ &a_{i, n}x_n &= &d_ihfillhbox{}  & & & & & & &vdotswithin{=}  a_{j, 1}x_1 &+ &a_{j, 2}x_2 &+ &cdots &+ &a_{j, n}x_n &= &d_jhfillhbox{}  & & & & & & &vdotswithin{=}  a_{m, 1}x_1 &+ &a_{m, 2}x_2 &+ &cdots &+ &a_{m, n}x_n &= &d_m  $ . The tuple $ (s_1, ldots, , s_n) $ satisfies this system if and only if substituting the values for the variables, the $ s $ 's for the $ x $ 's, gives a conjunction of true statements: $ a_{1, 1}s_1+a_{1, 2}s_2+cdots+a_{1, n}s_n=d_1 $ and ldots $ a_{i, 1}s_1+a_{i, 2}s_2+cdots+a_{i, n}s_n=d_i $ and ldots $ a_{j, 1}s_1+a_{j, 2}s_2+cdots+a_{j, n}s_n=d_j $ and ldots $ a_{m, 1}s_1+a_{m, 2}s_2+cdots+a_{m, n}s_n=d_m $ . In a list of statements joined with `and' we can rearrange the order of the statements. Thus this requirement is met if and only if $ a_{1, 1}s_1+a_{1, 2}s_2+cdots+a_{1, n}s_n=d_1 $ and ldots $ a_{j, 1}s_1+a_{j, 2}s_2+cdots+a_{j, n}s_n=d_j $ and ldots $ a_{i, 1}s_1+a_{i, 2}s_2+cdots+a_{i, n}s_n=d_i $ and ldots $ a_{m, 1}s_1+a_{m, 2}s_2+cdots+a_{m, n}s_n=d_m $ . This is exactly the requirement that $ (s_1, ldots, , s_n) $ solves the system after the row swap. The three operations from $ X $ are the elementary reduction operations, or row operations, or Gaussian operations. They are swapping , multiplying by a scalar (or rescaling ), and row combination . When writing out the calculations, we will abbreviate `row $ i $ ' by ` $ rho_i $ ' (this is the Greek letter rho, pronounced aloud as ``row''). For instance, we will denote a row combination operation by $ krho_i+rho_j $ , with the row that changes written second. To save writing we will often combine addition steps when they use the same $ rho_i $ , as in the next example. Gauss's Method systematically applies the row operations to solve a system. Here is a typical case. $ {3} x &+ &y & & &= &0  2x & - &y &+ &3z &= &3  x & - &2y & - &z &= &3 $ . We begin by using the first row to eliminate the $ 2x $ in the second row and the $ x $ in the third. To get rid of the $ 2x $ we mentally multiply the entire first row by $ - 2 $ , add that to the second row, and write the result in as the new second row. To eliminate the  $ x $ in the third row we multiply the first row by $ - 1 $ , add that to the third row, and write the result in as the new third row. $ &grstep[ - rho_1 +rho_3]{ - 2rho_1 +rho_2} {3} x &+ &y & & &= &0  & & - 3y&+ &3z &= &3  & & - 3y& - &z &= &3 $ . We finish by transforming the second system into a third, where the bottom equation involves only one unknown. We do that by using the second row to eliminate the $ y $  term from the third row. $ grstep{ - rho_2 +rho_3} {3} x &+ &y & & &= &0  & & - 3y&+ &3z &= &3  & & & & - 4z&= &0 $ . Now finding the system's solution is easy. The third row gives $ z=0 $ . Substitute that back into the second row to get $ y= - 1 $ . Then substitute back into the first row to get $ x=1 $ . For the Physics problem from the start of this chapter, Gauss's Method gives this. $ {2} 40h &+ &15c &= &100  - 50h &+ &25c &= &50 grstep{5/4rho_1 +rho_2} {2} 40h &+ &15c &= &100  & &(175/4)c &= &175 $ . So $ c=4 $ , and back - substitution gives that $ h=1 $ . (We will solve the Chemistry problem later.) The reduction $ {3} x &+ &y &+ &z &= &9  2x &+ &4y & - &3z &= &1  3x &+ &6y & - &5z &= &0 &grstep[ - 3rho_1 +rho_3]{ - 2rho_1 +rho_2} {3} x &+ &y &+ &z &= &9  & &2y & - &5z &= & - 17  & &3y & - &8z&= & - 27  &grstep{ - (3/2)rho_2+rho_3} {3} x &+ &y &+ &z &= &9  & &2y & - &5z &= & - 17  & & & & - (1/2)z &= & - (3/2) $ . shows that $ z=3 $ , $ y= - 1 $ , and $ x=7 $ . As illustrated above, the point of Gauss's Method is to use the elementary reduction operations to set up back - substitution. In each row of a system, the first variable with a nonzero coefficient is the row's leading variable . A system is in echelon form if each leading variable is to the right of the leading variable in the row above it, except for the leading variable in the first row, and any rows with all - zero coefficients are at the bottom. The prior three examples only used the operation of row combination. This linear system requires the swap operation to get it into echelon form because after the first combination $ {4} x & - &y & & & & &= &0  2x & - &2y &+ &z &+ &2w &= &4  & &y & & &+ &w &= &0  & & & &2z &+ &w &= &5 &grstep{ - 2rho_1 +rho_2} {4} x & - &y &spaceforemptycolumn & & & &= &0  & & & &z &+ &2w &= &4  & &y & & &+ &w &= &0  & & & &2z &+ &w &= &5 $ . the second equation has no leading $ y $ . We exchange it for a lower - down row that has a leading $ y $ . $ &grstep{rho_2 leftrightarrowrho_3} {4} x & - &y &spaceforemptycolumn & & & &= &0  & &y & & &+ &w &= &0  & & & &z &+ &2w &= &4  & & & &2z &+ &w &= &5 $ . (Had there been more than one suitable row below the second then we could have used any one.) With that, Gauss's Method proceeds as before. $ &grstep{ - 2rho_3 +rho_4} {4} x & - &y &spaceforemptycolumn & & & &= &0  & &y & & &+ &w &= &0  & & & &z &+ &2w &= &4  & & & & & & - 3w&= & - 3 $ . Back - substitution gives $ w=1 $ , $ z=2 $ , $ y= - 1 $ , and $ x= - 1 $ . Strictly speaking, to solve linear systems we don't need the row rescaling operation. We have introduced it here because it is convenient and because we will use it later in this chapter as part of a variation of Gauss's Method, the Gauss - Jordan Method. All of the systems so far have the same number of equations as unknowns. All of them have a solution and for all of them there is only one solution. We finish this subsection by seeing other things that can happen. This system has more equations than variables. $ {2} x &+ &3y &= &1  2x &+ &y &= & - 3  2x &+ &2y &= & - 2 $ . Gauss's Method helps us understand this system also, since this $ &grstep[ - 2rho_1 +rho_3]{ - 2rho_1 +rho_2} {2} x &+ &3y &= &1  & & - 5y &= & - 5  & & - 4y &= & - 4 $ . shows that one of the equations is redundant. Echelon form $ grstep{ - (4/5)rho_2 +rho_3} {2} x &+ &3y &= &1  & & - 5y &= & - 5  & &0 &= &0 $ . gives that $ y=1 $ and $ x= - 2 $ . The ` $ 0=0 $ ' reflects the redundancy. Gauss's Method is also useful on systems with more variables than equations. The next subsection has many examples. Another way that linear systems can differ from the examples shown above is that some linear systems do not have a unique solution. This can happen in two ways. The first is that a system can fail to have any solution at all. Contrast the system in the last example with this one. $ {2} x &+ &3y &= &1  2x &+ &y &= & - 3  2x &+ &2y &= &0 grstep[ - 2rho_1 +rho_3]{ - 2rho_1 +rho_2} {2} x &+ &3y &= &1  & & - 5y &= & - 5  & & - 4y &= & - 2 $ . Here the system is inconsistent: no pair of numbers $ (s_1, s_2) $ satisfies all three equations simultaneously. Echelon form makes the inconsistency obvious. $ grstep{ - (4/5)rho_2 +rho_3} {2} x &+ &3y &= &1  & & - 5y &= & - 5  & &0 &= &2 $ . The solution set is empty. The prior system has more equations than unknowns but that is not what causes the inconsistencyDash $ X $ has more equations than unknowns and yet is consistent. Nor is having more equations than unknowns necessary for inconsistency, as we see with this inconsistent system that has the same number of equations as unknowns. $ {2} x &+ &2y &= &8  2x &+ &4y &= &8 grstep{ - 2rho_1 + rho_2} {2} x &+ &2y &= &8  & &0 &= & - 8 $ . Instead, inconsistency has to do with the interaction of the left and right sides; in the first system above the left side's second equation is twice the first but the right side's second constant is not twice the first. Later we will have more to say about dependencies between a system's parts. The other way that a linear system can fail to have a unique solution, besides having no solutions, is to have many solutions. In this system $ {2} x &+ &y &= &4  2x &+ &2y &= &8 $ . any pair of numbers satisfying the first equation also satisfies the second. The solution set $ set{ (x, y)suchthat x+y=4} $ is infinite; some example member pairs are  $ (0, 4) $ , $ ( - 1, 5) $ , and $ ((ii)5, (i)5) $ . The result of applying Gauss's Method here contrasts with the prior example because we do not get a contradictory equation. $ grstep{ - 2rho_1 + rho_2} {2} x &+ &y &= &4  & &0 &= &0 $ . Don't be fooled by that example: a $ 0=0 $ equation is not the signal that a system has many solutions. The absence of a $ 0=0 $ equation does not keep a system from having many different solutions. This system is in echelon form, has no $ 0=0 $ , but has infinitely many solutions, including $ (0, 1, - 1) $ , $ (0, 1/2, - 1/2) $ , $ (0, 0, 0) $ , and $ (0, - pi, pi) $ (any triple whose first component is $ 0 $ and whose second component is the negative of the third is a solution). $ {3} x &+ &y &+ &z &= &0  & &y &+ &z &= &0 $ . Nor does the presence of $ 0=0 $ mean that the system must have many solutions. $ X $ shows that. So does this system, which does not have any solutions at all despite that in echelon form it has a $ 0=0 $ row. $ {3} 2x & & & - &2z &= &6  & &y &+ &z &= &1  2x &+ &y & - &z &= &7  & &3y &+ &3z &= &0 &grstep{ - rho_1 +rho_3} {3} 2x &spaceforemptycolumn & & - &2z &= &6  & &y &+ &z &= &1  & &y &+ &z &= &1  & &3y &+ &3z &= &0  &grstep[ - 3rho_2 +rho_4]{ - rho_2 +rho_3} {3} 2x &spaceforemptycolumn & & - &2z &= &6  & &y &+ &z &= &1  & & & &0 &= &0  & & & &0 &= & - 3 $ . In summary, Gauss's Method uses the row operations to set a system up for back substitution. If any step shows a contradictory equation then we can stop with the conclusion that the system has no solutions. If we reach echelon form without a contradictory equation, and each variable is a leading variable in its row, then the system has a unique solution and we find it by back substitution. Finally, if we reach echelon form without a contradictory equation, and there is not a unique solutionDash that is, at least one variable is not a leading variableDash then the system has many solutions. The next subsection explores the third case. We will see that such a system must have infinitely many solutions and we will describe the solution set. medskip noindenttextbf{Note.}hspace*{.2em} textit{In the exercises here, and in the rest of the book, you must justify all of your answers. For instance, if a question asks whether a system has a solution then you must justify a yes response by producing the solution and must justify a no response by showing that no solution exists.} A linear system with a unique solution has a solution set with one element. A linear system with no solution has a solution set that is empty. In these cases the solution set is easy to describe. Solution sets are a challenge to describe only when they contain many elements. This system has many solutions because in echelon form $ {3} 2x & & &+ &z &= &3  x & - &y & - &z &= &1  3x & - &y & & &= &4 &grstep[ - (3/2)rho_1 +rho_3]{ - (1/2)rho_1+rho_2} {3} 2x & & &+ &z &= &3  & & - y & - &(3/2)z &= & - 1/2  & & - y & - &(3/2)z &= & - 1/2  &grstep{ - rho_2+rho_3} {3} 2x & & &+ &z &= &3  & & - y & - &(3/2)z &= & - 1/2  & & & &0 &= &0 $ . not all of the variables are leading variables. $ X $ shows that an $ (x, y, z) $ satisfies the first system if and only if it satisfies the third. So we can describe the solution set $ set{(x, y, z)suchthattext{ $ 2x+z=3 $ and $ x - y - z=1 $ and $ 3x - y=4 $ }} $ in this way. $ set{(x, y, z)suchthattext{ $ 2x+z=3 $ and $ - y - 3z/2= - 1/2 $ }} tag{ $ * $ } $ . This description is better because it has two equations instead of three but it is not optimal because it still has some hard to understand interactions among the variables. To improve it, use the variable that does not lead any equation, $ z $ , to describe the variables that do lead, $ x $ and $ y $ . The second equation gives $ y=(1/2) - (3/2)z $ and the first equation gives $ x=(3/2) - (1/2)z $ . Thus we can describe the solution set as this set of triples. $ set{((3/2) - (1/2)z, , (1/2) - (3/2)z, , z)suchthat zinRe} tag{ $ ** $ } $ . Compared with ( $ * $ ), the advantage of ( $ ** $ ) is that $ z $ can be any real number. This makes the job of deciding which tuples are in the solution set much easier. For instance, taking $ z=2 $ shows that $ (1/2, - 5/2, 2) $ is a solution. In an echelon form linear system the variables that are not leading are free. Reduction of a linear system can end with more than one variable free. Gauss's Method on this system $ {4} x &+ &y &+ &z & - &w &= &1  & &y & - &z &+ &w &= & - 1  3x & & &+ &6z & - &6w &= &6  & & - y &+ &z & - &w &= &1 &grstep{ - 3rho_1 +rho_3} {4} x &+ &y &+ &z & - &w &= &1  & &y & - &z &+ &w &= & - 1  & & - 3y &+ &3z & - &3w &= &3  & & - y &+ &z & - &w &= &1  &grstep[rho_2 +rho_4]{3rho_2 +rho_3} {4} x &+ &y &+ &z & - &w &= &1  & &y & - &z &+ &w &= & - 1  & & & & & &0 &= &0  & & & & & &0 &= &0 $ . leaves $ x $ and $ y $ leading and both $ z $ and  $ w $ free. To get the description that we prefer, we work from the bottom. We first express the leading variable $ y $ in terms of $ z $ and $ w $ , as $ y= - 1+z - w $ . Moving up to the top equation, substituting for $ y $ gives $ x+( - 1+z - w)+z - w=1 $ and solving for $ x $ leaves $ x=2 - 2z+2w $ . The solution set $ set{(2 - 2z+2w, - 1+z - w, z, w)suchthat z, winRe} tag{ $ ** $ } $ . has the leading variables expressed in terms of the variables that are free. The list of leading variables may skip over some columns. After this reduction $ {4} 2x & - &2y & & & & &= &0  & & & &z &+ &3w &= &2  3x & - &3y & & & & &= &0  x & - &y &+ &2z &+ &6w &= &4 &grstep[ - (1/2)rho_1+ rho_4]{ - (3/2)rho_1 +rho_3} {4} 2x & - &2y &spaceforemptycolumn & & & &= &0  & & & &z &+ &3w &= &2  & & & & & &0 &= &0  & & & &2z &+ &6w &= &4  &grstep{ - 2rho_2 +rho_4} {4} 2x & - &2y &spaceforemptycolumn & & & &= &0  & & & &z &+ &3w &= &2  & & & & & &0 &= &0  & & & & & &0 &= &0 $ . $ x $ and $ z $ are the leading variables, not $ x $ and  $ y $ . The free variables are $ y $ and  $ w $ and so we can describe the solution set as $ set{ (y, y, 2 - 3w, w)suchthat y, winRe } $ . For instance, $ (1, 1, 2, 0) $ satisfies the systemDash take $ y=1 $ and $ w=0 $ . The four - tuple $ (1, 0, 5, 4) $ is not a solution since its first coordinate does not equal its second. A variable that we use to describe a family of solutions is a parameter. We say that the solution set in the prior example is parametrized/ with $ y $ and $ w $ . The terms `parameter' and `free variable' do not mean the same thing. In the prior example $ y $ and  $ w $ are free because in the echelon form system they do not lead. They are parameters because we used them to describe the set of solutions. Had we instead rewritten the second equation as $ w=2/3 - (1/3)z $ then the free variables would still be $ y $ and  $ w $ but the parameters would be $ y $ and  $ z $ . In the rest of this book we will solve linear systems by bringing them to echelon form and then parametrizing with the free variables. This is another system with infinitely many solutions. $ {4} x &+ &2y & & & & &= &1  2x & & &+ &z & & &= &2  3x &+ &2y &+ &z & - &w &= &4 &grstep[ - 3rho_1 +rho_3]{ - 2rho_1+rho_2} {4} x &+ &2y & & & & &= &1  & & - 4y &+ &z & & &= &0  & & - 4y &+ &z & - &w &= &1  &grstep{ - rho_2+rho_3} {4} x &+ &2y & & & & &= &1  & & - 4y &+ &z & & &= &0  & & & & & & - w &= &1 $ . The leading variables are $ x $ , $ y $ , and $ w $ . The variable $ z $ is free. Notice that, although there are infinitely many solutions, the value of $ w $ doesn't vary but is constant $ w= - 1 $ . To parametrize, write $ w $ in terms of $ z $ with $ w= - 1+0z $ . Then $ y=(1/4)z $ . Substitute for $ y $ in the first equation to get $ x=1 - (1/2)z $ . The solution set is $ set{(1 - (1/2)z, (1/4)z, z, - 1)suchthat zinRe} $ . Parametrizing solution sets shows that systems with free variables have infinitely many solutions. For instance, above $ z $ takes on all of infinitely many real number values, each associated with a different solution. We finish this subsection by developing a streamlined notation for linear systems and their solution sets. An $ nbym{m}{n} $ matrix is a rectangular array of numbers with $ m $  rows and $ n $  columns. Each number in the matrix is an entry. We usually denote a matrix with an upper case roman letter. For instance, $ A= [r] 1 &(ii)2 &5  3 &4 & - 7 $ . has $ 2 $  rows and $ 3 $  columns and so is a $ nbym{2}{3} $ matrix. Read that aloud as ``two - by - three''; the number of rows is always stated first. (The matrix has parentheses around it so that when two matrices are adjacent we can tell where one ends and the other begins.) We name matrix entries with the corresponding lower - case letter, so that the entry in the second row and first column of the above array is $ a_{2, 1}=3 $ . Note that the order of the subscripts matters: $ a_{1, 2}neq a_{2, 1} $ since $ a_{1, 2}=(ii)2 $ . We denote the set of all $ nbym{m}{n} $ matrices by $ matspace_{nbym{m}{n}} $ . We do Gauss's Method using matrices in essentially the same way that we did it for systems of equations: a matrix row's leading entry is its first nonzero entry (if it has one) and we perform row operations to arrive at matrix echelon form, where the leading entry in lower rows are to the right of those in the rows above. We like matrix notation because it lightens the clerical load, the copying of variables and the writing of $ + $ 's and $ = $ 's. We can abbreviate this linear system $ {3} x &+ &2y & & &= &4  & &y & - &z &= &0  x & & &+ &2z &= &4 $ . with this matrix. $ [r]{3} 1 &2 &0 &4  0 &1 & - 1 &0  1 &0 &2 &4 $ . The vertical bar reminds a reader of the difference between the coefficients on the system's left hand side and the constants on the right. With a bar, this is an augmented/ matrix. $ [r]{3} 1 &2 &0 &4  0 &1 & - 1 &0  1 &0 &2 &4 grstep{ - rho_1 +rho_3} [r]{3} 1 &2 &0 &4  0 &1 & - 1 &0  0 & - 2 &2 &0 grstep{2rho_2 +rho_3} [r]{3} 1 &2 &0 &4  0 &1 & - 1 &0  0 &0 &0 &0 $ . The second row stands for $ y - z=0 $ and the first row stands for $ x+2y=4 $ so the solution set is $ set{(4 - 2z, z, z)suchthat zinRe} $ . Matrix notation also clarifies the descriptions of solution sets. $ X $ 's $ set{(2 - 2z+2w, - 1+z - w, z, w)suchthat z, winRe} $ is hard to read. We will rewrite it to group all of the constants together, all of the coefficients of  $ z $ together, and all of the coefficients of  $ w $ together. We write them vertically, in one - column matrices. $ set{colvec[r]{2  - 1  0  0} +colvec[r]{ - 2  1  1  0}cdot z +colvec[r]{2  - 1  0  1}cdot w suchthat z, winRe} $ . For instance, the top line says that $ x=2 - 2z+2w $ and the second line says that $ y= - 1+z - w $ . (Our next section gives a geometric interpretation that will help us picture the solution sets.) A column vector, often just called a vector, is a matrix with a single column. A matrix with a single row is a row vector. The entries of a vector are sometimes called components. A column or row vector whose components are all zeros is a zero vector. Vectors are an exception to the convention of representing matrices with capital roman letters. We use lower - case roman or greek letters overlined with an arrow: $ vec{a} $ , $ vec{b} $ , ldots, or $ vec{alpha} $ , $ vec{beta} $ , ldots (boldface is also common: {boldmath $ a $ } or {boldmath $ alpha $ }). For instance, this is a column vector with a third component of $ 7 $ . $ vec{v}= colvec[r]{ 1  3  7} $ . A zero vector is denoted $ zero $ . There are many different zero vectorsDash the one - tall zero vector, the two - tall zero vector, etc.Dash but nonetheless we will often say ``the'' zero vector, expecting that the size will be clear from the context. The linear equation $ a_1x_1+a_2x_2+, cdots, +a_nx_n=d $ with unknowns $ x_1, ldots, , x_n $ is satisfied by $ vec{s}=colvec{s_1  vdotswithin{s_1}  s_n} $ . if $ a_1s_1+a_2s_2+, cdots, +a_ns_n=d $ . A vector satisfies a linear system if it satisfies each equation in the system. The style of description of solution sets that we use involves adding the vectors, and also multiplying them by real numbers. Before we give the examples showing the style we first need to define these operations. The vector sum of $ vec{u} $ and $ vec{v} $ is the vector of the sums. $ vec{u}+vec{v}= colvec{u_1  vdotswithin{u_1}  u_n} + colvec{v_1  vdotswithin{v_1}  v_n} = colvec{u_1+v_1  vdotswithin{u_1+v_1}  u_n+v_n} $ . Note that for the addition to be defined the vectors must have the same number of entries. This entry - by - entry addition works for any pair of matrices, not just vectors, provided that they have the same number of rows and columns. The scalar multiplication/ of the real number $ r $ and the vector $ vec{v} $ is the vector of the multiples. $ rcdotvec{v}= rcdotcolvec{v_1  vdotswithin{v_1}  v_n} = colvec{rv_1  vdotswithin{rv_1}  rv_n} $ . As with the addition operation, the entry - by - entry scalar multiplication operation extends beyond vectors to apply to any matrix. We write scalar multiplication either as $ rcdotvec{v} $ or $ vec{v}cdot r $ , and sometimes even omit the ` $ cdot $ ' symbol:  $ rvec{v} $ . (Do not refer to scalar multiplication as `scalar product' because that name is for a different operation.) $ colvec[r]{2  3  1} + colvec[r]{3  - 1  4} = colvec{2+3  3 - 1  1+4} = colvec[r]{5  2  5} qquad 7cdotcolvec[r]{1  4  - 1  - 3} = colvec[r]{7  28  - 7  - 21} $ . Observe that the definitions of addition and scalar multiplication agree where they overlap; for instance, $ vec{v} +vec{v} = 2vec{v} $ . With these definitions, we are set to use matrix and vector notation to both solve systems and express the solution. This system $ {5} 2x &+ &y & & & - &w & & &= &4  & &y & & &+ &w &+ &u &= &4  x & & & - &z &+ &2w & & &= &0 $ . reduces in this way. $ [r]{5} 2 &1 &0 & - 1 &0 &4  0 &1 &0 &1 &1 &4  1 &0 & - 1 &2 &0 &0 &grstep{ - (1/2)rho_1+rho_3} [r]{5} 2 &1 &0 & - 1 &0 &4  0 &1 &0 &1 &1 &4  0 & - 1/2 & - 1 &5/2 &0 & - 2  &grstep{(1/2)rho_2+rho_3} [r]{5} 2 &1 &0 & - 1 &0 &4  0 &1 &0 &1 &1 &4  0 &0 & - 1 &3 &1/2 &0 $ . The solution set is $ set{(w+(1/2)u, 4 - w - u, 3w+(1/2)u, w, u)suchthat w, uinRe} $ . We write that in vector form. $ set{colvec{x  y  z  w  u}= colvec[r]{0  4  0  0  0}+ colvec[r]{1  - 1  3  1  0}w+ colvec[r]{1/2  - 1  1/2  0  1}u suchthat w, uinRe} $ . Note how well vector notation sets off the coefficients of each parameter. For instance, the third row of the vector form shows plainly that if $ u $ is fixed then $ z $ increases three times as fast as $ w $ . Another thing shown plainly is that setting both $ w $ and $ u $ to zero gives that $ colvec{x  y  z  w  u} =colvec[r]{0  4  0  0  0} $ . is a particular solution of the linear system. In the same way, the system $ {3} x & - &y &+ &z &= &1  3x & & &+ &z &= &3  5x & - &2y &+ &3z &= &5 $ . reduces $ [r]{3} 1 & - 1 &1 &1  3 &0 &1 &3  5 & - 2 &3 &5 &grstep[ - 5rho_1+rho_3]{ - 3rho_1+rho_2} [r]{3} 1 & - 1 &1 &1  0 &3 & - 2 &0  0 &3 & - 2 &0  &grstep{ - rho_2+rho_3} [r]{3} 1 & - 1 &1 &1  0 &3 & - 2 &0  0 &0 &0 &0 $ . to give a one - parameter solution set. $ set{colvec[r]{1  0  0} +colvec[r]{ - 1/3  2/3  1}z suchthat zinRe} $ . As in the prior example, the vector not associated with the parameter $ colvec[r]{1  0  0} $ . is a particular solution of the system. Before the exercises, we will consider what we have accomplished and what we will do in the remainder of the chapter. So far we have done the mechanics of Gauss's Method. We have not stopped to consider any of the questions that arise, except for proving $ X $ Dash which justifies the method by showing that it gives the right answers. For example, can we always describe solution sets as above, with a particular solution vector added to an unrestricted linear combination of some other vectors? We've noted that the solution sets described in this way have infinitely many members so answering this question would tell us about the size of solution sets. The following subsection shows that the answer is ``yes.'' This chapter's second section then uses that answer to describe the geometry of solution sets. Other questions arise from the observation that we can do Gauss's Method in more than one way (for instance, when swapping rows we may have a choice of rows to swap with). $ X $ says that we must get the same solution set no matter how we proceed but if we do Gauss's Method in two ways must we get the same number of free variables in each echelon form system? Must those be the same variables, that is, is it impossible to solve a problem one way to get $ y $ and  $ w $ free and solve it another way to get $ y $ and  $ z $ free? The third section of this chapter answers ``yes, '' that from any starting linear system, all derived echelon form versions have the same free variables. Thus, by the end of the chapter we will not only have a solid grounding in the practice of Gauss's Method but we will also have a solid grounding in the theory. We will know exactly what can and cannot happen in a reduction. In the prior subsection the descriptions of solution sets all fit a pattern. They have a vector that is a particular solution of the system added to an unrestricted combination of some other vectors. The solution set from $ X $ illustrates. $ set{ underbracket[.7pt]{ colvec[r]{0  4  0  0  0}}_{text{shortstack{rule{0pt}{2ex}particular  solution}}}+ underbracket[.7pt]{wcolvec[r]{1  - 1  3  1  0}+ ucolvec[r]{1/2  - 1  1/2  0  1}}_{text{shortstack{rule{0pt}{2ex}unrestricted  combination}}} suchthat w, uinRe} $ . The combination is unrestricted in that $ w $ and $ u $ can be any real numbersDash there is no condition like ``such that $ 2w - u=0mspace{1mu} $ '' to restrict which pairs $ w, u $ we can use. That example shows an infinite solution set fitting the pattern. The other two kinds of solution sets also fit. A one - element solution set fits because it has a particular solution and the unrestricted combination part is trivial. That is, instead of being a combination of two vectors or of one vector, it is a combination of no vectors. (By convention the sum of an empty set of vectors is the zero vector.) An empty solution set fits the pattern because there is no particular solution and thus there are no sums of that form. Any linear system's solution set has the form $ set{vec{p}+c_1vec{beta}_1+, cdots, +c_kvec{beta}_k suchthat c_1, , ldots, , c_kinRe} $ . where $ vec{p} $ is any particular solution and where the number of vectors $ vec{beta}_1 $ , ldots, $ vec{beta}_k $ equals the number of free variables that the system has after a Gaussian reduction. The solution description has two parts, the particular solution $ vec{p} $ and the unrestricted linear combination of the $ vec{beta} $ 's. We shall prove the theorem with two corresponding lemmas. We will focus first on the unrestricted combination. For that we consider systems that have the vector of zeroes as a particular solution so that we can shorten $ vec{p}+c_1vec{beta}_1+dots+c_kvec{beta}_k $ to $ c_1vec{beta}_1+dots+c_kvec{beta}_k $ . A linear equation is homogeneous if it has a constant of zero, so that it can be written as $ a_1x_1+a_2x_2+, cdots, +a_nx_n=0 $ . With any linear system like $ {2} 3x &+ &4y &= 3  2x & - &y &= 1 $ . we associate a system of homogeneous equations by setting the right side to zeros. $ {2} 3x &+ &4y &= 0  2x & - &y &= 0 $ . Compare the reduction of the original system $ {2} 3x &+ &4y &= 3  2x & - &y &= 1 grstep{ - (2/3)rho_1+rho_2} {2} 3x &+ &4y &= 3  & & - (11/3)y &= - 1 $ . with the reduction of the associated homogeneous system. $ {2} 3x &+ &4y &= 0  2x & - &y &= 0 grstep{ - (2/3)rho_1+rho_2} {2} 3x &+ &4y &= 0  & & - (11/3)y &= 0 $ . Obviously the two reductions go in the same way. We can study how to reduce a linear system by instead studying how to reduce the associated homogeneous system. Studying the associated homogeneous system has a great advantage over studying the original system. Nonhomogeneous systems can be inconsistent. But a homogeneous system must be consistent since there is always at least one solution, the zero vector. Some homogeneous systems have the zero vector as their only solution. $ {3} 3x &+ &2y &+ &z &= &0  6x &+ &4y & & &= &0  & &y &+ &z &= &0 grstep{ - 2rho_1 +rho_2} {3} 3x &+ &2y &+ &z &= &0  & & & & - 2z&= &0  & &y &+ &z &= &0 grstep{rho_2 leftrightarrowrho_3} {3} 3x &+ &2y &+ &z &= &0  & &y &+ &z &= &0  & & & & - 2z&= &0 $ . Some homogeneous systems have many solutions. One is the Chemistry problem from the first page of the first subsection. $ {4} 7x & & & - &7z & & &= &0  8x &+ &y & - &5z & - &2w &= &0  & &y & - &3z & & &= &0  & &3y & - &6z & - &w &= &0 &grstep{ - (8/7)rho_1+rho_2} {4} 7x & & & - &7z & & &= &0  & &y &+ &3z & - &2w &= &0  & &y & - &3z & & &= &0  & &3y & - &6z & - &w &= &0  &grstep[ - 3rho_2+rho_4]{ - rho_2+rho_3} {4} 7x & & & - &7z & & &= &0  & &y &+ &3z & - &2w &= &0  & & & & - 6z &+ &2w &= &0  & & & & - 15z&+ &5w &= &0  &grstep{ - (5/2)rho_3+rho_4} {4} 7x & & & - &7z & & &= &0  & &y &+ &3z & - &2w &= &0  & & & & - 6z &+ &2w &= &0  & & & & & &0 &= &0 $ . The solution set $ set{colvec[r]{1/3  1  1/3  1}w suchthat winRe} $ . has many vectors besides the zero vector (if we take $ w $ to be a number of molecules then solutions make sense only when $ w $ is a nonnegative multiple of $ 3 $ ). For any homogeneous linear system there exist vectors $ vec{beta}_1 $ , ldots, $ vec{beta}_k $ such that the solution set of the system is $ set{c_1vec{beta}_1+cdots+c_kvec{beta}_k suchthat c_1, ldots, c_kinRe} $ . where $ k $ is the number of free variables in an echelon form version of the system. Before the proof, we discuss a couple of points about it. First, the proof verifies that, given a homogeneous system in echelon form, we can use back substitution to express all of the leading variables in terms of the free variables. To see why this suffices to establish the lemma, we give an example (we will also use this walk - through as a chance to illustrate the proof's notation). Consider this system of homogeneous equations in echelon form. $ {5} x &+ &y &+ &2z &+ &u &+ &v &= &0  & &y &+ &z &+ &u & - &v &= &0  & & & & & &u &+ &v &= &0 $ . It has five variables: $ x_1=x $ , ldots{}, $ x_5=v $ . To do back substitution, we start with the bottom row, equation  $ m=3 $ . We solve for its leading variable, $ x_{ell_3}=x_4=u $ , using its leading term $ a_{m, ell_m}x_m=a_{3, 4}x_4=1cdot u $ (the notation's ` $ ell $ ' stands for ``leading''). We get $ u= - v $ . Now back substitution enters its iteration phase. We will use the variable  $ t $ to count how many rows up from the bottom the current row is, so we take $ t=1 $ and look at row $ m - t=3 - 1=2 $ . We come to this row having so far expressed the leading variable in terms of free variables for row  $ 3 $ . We substitute for $ x_{ell_3}=x_4=u $ its expression in terms of free variables, getting $ y+z+( - v) - v=0 $ . Then solving for this row's leading variable gives $ x_{ell_{m - t}}=x_2=y= - z+2v $ . Next is the row that is $ t=2 $ up from the bottom, row $ m - t=1 $ . We have so far expressed the leading variable in terms of free variables for rows $ 3 $ and  $ 2 $ . Substitute for $ x_{ell_3}=u $ and $ x_{ell_2}=y $ , giving $ x+( - z+2v)+2z+( - v)+v=0 $ . Then solve for the leading variable in terms of the free variables as $ x_{ell_1}=x= - z - 2v $ . The point of this example is that now we are done. Just write the solution in vector notation $ colvec{x  y  z  u  v} =colvec{ - 1  - 1  1  0  0}z +colvec{ - 2  2  0  - 1  1}v qquad text{where $ z, vinRe $ } $ . to recognize that $ vec{beta}_1 $ and $ vec{beta}_2 $ of the lemma's statement are the vectors associated with the free variables $ z $ and  $ v $ . The second point about the proof follows from that example also. Back substitution moves up the system, using the conclusions from lower rows to do the next row. This suggests the proof strategy of mathematical induction. Induction is an important and non - obvious proof technique that will appear a number of times in this book. Our proofs by induction will come in two steps, a base step and an inductive step. In the base step, we verify that the statement is true for some first instance, here that for an echelon form linear system we can write the bottom equation's leading variable in terms of free variables. In the inductive step, we must establish an implication, namely that if the statement is true for all prior cases then it follows that the statement holds for the present case also. Specifically, here the inductive hypothesis is that if for the lower - down rows we can express the leading variables in terms of the free variables, then for the next row up we can express the leading variable in those terms also. Those two steps together prove the statement for all the rows because by the base step it is true for the bottom equation, and by the inductive step the fact that it is true for the bottom equation shows that it is true for the next one up. Then, another application of the inductive step implies that it is true for the third equation up, etc.appendrefs{mathematical induction} Apply Gauss's Method to get to echelon form. There may be some $ 0=0 $ equations; we ignore these (if the system consists only of $ 0=0 $  equations then the lemma is trivially true because there are no leading variables). But because the system is homogeneous there are no contradictory equations. We will use induction to verify that each leading variable can be expressed in terms of free variables. That will finish the proof because we can use the free variables as parameters and the $ vec{beta} $ 's are the vectors of coefficients of those free variables. For the base step, consider the bottom - most equation, $ a_{m, ell_m}x_{ell_m}+a_{m, 1+ell_{m}}x_{1+ell_{m}}+cdots+a_{m, n}x_n=0 $ . where $ a_{m, ell_m}neq 0 $ . This is the bottom row, so any variables $ x_{ell_{m}+1} $ , ldots{} after the leading one must be free. Move these to the right hand side and divide by $ a_{m, ell_m} $ $ x_{ell_m} =( - a_{m, 1+ell_{m}}/a_{m, ell_m})x_{1+ell_{m}}+cdots+( - a_{m, n}/a_{m, ell_m})x_n $ . to express the leading variable in terms of free variables. (A technical point: if in the bottom equation there are no variables to the right of  $ x_{ell_m} $ then $ x_{ell_m}=0 $ . This satisfies the statement that we are verifying because, as alluded to at the start of this subsection, it has $ x_{ell_m} $ written as a sum of a number of the free variables, namely as the sum of zero many, under the convention that a trivial sum totals to  $ 0 $ .) For the inductive step, assume that assume that for the $ m $ - th equation, and the text{ $ (m - 1) $ - th} equation, etc., up to and including the mbox{ $ m - (t - 1) $ - th} equation, we can express the leading variable of that equation in terms of free variables. That is, assume that the statement that we can express the leading variable of the row in terms of the free variables holds for the bottom - most $ t $  rows, with $ 1leq t<m $ . We must verify that the statement therefore also holds for the next equation up, the $ (m - t) $ - th equation. For each of $ x_{ell_m} $ , ldots, $ x_{ell_{m - (t - 1)}} $ , substitute its expression in terms of free variables. The result has a leading term of $ a_{m - t, ell_{m - t}}x_{ell_{m - t}} $ with $ a_{m - t, ell_{m - t}}neq 0 $ , and the rest of the left hand side is a combination of free variables. Move the free variables to the right side and divide by $ a_{m - t, ell_{m - t}} $ , to end with $ x_{ell_{m - t}} $ expressed in terms of free variables. We have done both the base step and the inductive step, so by the principle of mathematical induction the proposition is true. This shows, as discussed between the lemma and its proof, that we can parametrize solution sets using the free variables. We say that the set of vectors $ set{c_1vec{beta}_1+cdots+c_kvec{beta}_k suchthat c_1, ldots, c_kinRe} $ is generated by or spanned by the set $ set{smash{vec{beta}_1}, ldots, smash{vec{beta}_k}} $ . To finish the proof of $ X $ the next lemma considers the particular solution part of the solution set's description. For a linear system and for any particular solution $ vec{p}/ $ , the solution set equals $ set{vec{p}+vec{h} suchthat text{ $ vec{h} $ satisfies the associated homogeneous system} } $ . We will show mutual set inclusion, that any solution to the system is in the above set and that anything in the set is a solution of the system.appendrefs{set equality} For set inclusion the first way, that if a vector solves the system then it is in the set described above, assume that $ vec{s} $ solves the system. Then $ vec{s} - vec{p} $ solves the associated homogeneous system since for each equation index $ i $ , a_{i, 1}(s_1 - p_1)+cdots+a_{i, n}(s_n - p_n)  =(a_{i, 1}s_1+cdots+a_{i, n}s_n) - (a_{i, 1}p_1+cdots+a_{i, n}p_n) =d_i - d_i =0 where $ p_j $ and $ s_j $ are the $ j $ - th components of $ vec{p} $ and $ vec{s} $ . Express $ vec{s} $ in the required $ vec{p}+vec{h} $ form by writing $ vec{s} - vec{p} $ as $ vec{h} $ . For set inclusion the other way, take a vector of the form $ vec{p}+vec{h} $ , where $ vec{p} $ solves the system and $ vec{h} $ solves the associated homogeneous system and note that $ vec{p}+vec{h} $ solves the given system since for any equation index  $ i $ , a_{i, 1}(p_1+h_1)+cdots+a_{i, n}(p_n+h_n)  =(a_{i, 1}p_1+cdots+a_{i, n}p_n) +(a_{i, 1}h_1+cdots+a_{i, n}h_n) =d_i+0 =d_i where as earlier $ p_j $ and $ h_j $ are the $ j $ - th components of $ vec{p} $ and $ vec{h} $ . The two lemmas together establish $ X $ . Remember that theorem with the slogan, `` $ text{General} = text{Particular} + text{Homogeneous} $ ''. This system illustrates $ X $ . $ {3} x &+ &2y & - &z &= &1  2x &+ &4y & & &= &2  & &y & - &3z &= &0 $ . Gauss's Method $ grstep{ - 2rho_1+rho_2} {3} x &+ &2y & - &z &= &1  & & & &2z &= &0  & &y & - &3z &= &0 grstep{rho_2leftrightarrowrho_3} {3} x &+ &2y & - &z &= &1  & &y & - &3z &= &0  & & & &2z &= &0 $ . shows that the general solution is a singleton set. $ set{colvec[r]{1  0  0} } $ . That single vector is obviously a particular solution. The associated homogeneous system reduces via the same row operations $ {3} x &+ &2y & - &z &= &0  2x &+ &4y & & &= &0  & &y & - &3z &= &0 grstep{ - 2rho_1+rho_2} repeatedgrstep{rho_2swaprho_3} {3} x &+ &2y & - &z &= &0  & &y & - &3z &= &0  & & & &2z &= &0 $ . to also give a singleton set. $ set{colvec[r]{0  0  0} } $ . So, as discussed at the start of this subsection, in this single - solution case the general solution results from taking the particular solution and adding to it the unique solution of the associated homogeneous system. The start of this subsection also discusses that the case where the general solution set is empty fits the $ text{General}=text{Particular}+text{Homogeneous} $ pattern too. This system illustrates. $ {4} x & & &+ &z &+ &w &= & - 1  2x & - &y & & &+ &w &= &3  x &+ &y &+ &3z &+ &2w &= &1 grstep[ - rho_1+rho_3]{ - 2rho_1+rho_2} {4} x & & &+ &z &+ &w &= & - 1  & & - y& - &2z & - &w &= &5  & &y &+ &2z &+ &w &= &2 $ . It has no solutions because the final two equations conflict. But the associated homogeneous system does have a solution, as do all homogeneous systems. $ {4} x & & &+ &z &+ &w &= &0  2x & - &y & & &+ &w &= &0  x &+ &y &+ &3z &+ &2w &= &0 grstep[ - rho_1+rho_3]{ - 2rho_1+rho_2} grstep{rho_2+rho_3} {4} x & & &+ &z &+ &w &= &0  & & - y& - &2z & - &w &= &0  & & & & & &0 &= &0 $ . In fact, the solution set is infinite. $ set{colvec[r]{ - 1  - 2  1  0}z+colvec[r]{ - 1  - 1  0  1}w suchthat z, winRe} $ . Nonetheless, because the original system has no particular solution, its general solution set is emptyDash there are no vectors of the form $ vec{p}+vec{h} $ because there are no $ vec{p}: $ 's. Solution sets of linear systems are either empty, have one element, or have infinitely many elements. We've seen examples of all three happening so we need only prove that there are no other possibilities. First observe a homogeneous system with at least one non - $ zero $ solution $ vec{v} $ has infinitely many solutions. This is because any scalar multiple of  $ vec{v} $ also solves the homogeneous system and there are infinitely many vectors in the set of scalar multiples of $ vec{v} $ : if $ s, tinRe $ are unequal then $ svec{v}neq tvec{v} $ , since $ svec{v} - tvec{v}=(s - t)vec{v} $ is non - $ zero $ as any non - $ 0 $ component of $ vec{v} $ , when rescaled by the non - $ 0 $ factor $ s - t $ , will give a non - $ 0 $ value. Now apply $ X $ to conclude that a solution set $ set{vec{p}+vec{h}suchthat text{ $ vec{h} $ solves the associated homogeneous system}} $ . is either empty (if there is no particular solution $ vec{p} $ ), or has one element (if there is a $ vec{p} $ and the homogeneous system has the unique solution $ zero $ ), or is infinite (if there is a $ vec{p} $ and the homogeneous system has a non - $ zero $ solution, and thus by the prior paragraph has infinitely many solutions). This table summarizes the factors affecting the size of a general solution. smallskip smallskip The dimension on the top of the table is the simpler one. When we perform Gauss's Method on a linear system, ignoring the constants on the right side and so paying attention only to the coefficients on the left - hand side, we either end with every variable leading some row or else we find some variable that does not lead a row, that is, we find some variable that is free. (We formalize ``ignoring the constants on the right'' by considering the associated homogeneous system.) A notable special case is systems having the same number of equations as unknowns. Such a system will have a solution, and that solution will be unique, if and only if it reduces to an echelon form system where every variable leads its row (since there are the same number of variables as rows), which will happen if and only if the associated homogeneous system has a unique solution. A square matrix is nonsingular if it is the matrix of coefficients of a homogeneous system with a unique solution. It is singular otherwise, that is, if it is the matrix of coefficients of a homogeneous system with infinitely many solutions. The first of these matrices is nonsingular while the second is singular $ [r] 1 &2  3 &4 qquad [r] 1 &2  3 &6 $ . because the first of these homogeneous systems has a unique solution while the second has infinitely many solutions. $ [b]{2} x &+ &2y &= &0  3x &+ &4y &= &0 qquad [b]{2} x &+ &2y &= &0  3x &+ &6y &= &0 $ . We have made the distinction in the definition because a system with the same number of equations as variables behaves in one of two ways, depending on whether its matrix of coefficients is nonsingular or singular. Where the matrix of coefficients is nonsingular the system has a unique solution for any constants on the right side: for instance, Gauss's Method shows that this system $ {2} x &+ &2y &= &a  3x &+ &4y &= &b $ . has the unique solution $ x=b - 2a $ and $ y=(3a - b)/2 $ . On the other hand, where the matrix of coefficients is singular the system never has a unique solutionDash it has either no solutions or else has infinitely many, as with these. $ [b]{2} x &+ &2y &= &1  3x &+ &6y &= &2 qquad [b]{2} x &+ &2y &= &1  3x &+ &6y &= &3 $ . The definition uses the word `singular' because it means ``departing from general expectation.'' People often, naively, expect that systems with the same number of variables as equations will have a unique solution. Thus, we can think of the word as connoting ``troublesome, '' or at least ``not ideal.'' (That `singular' applies to those systems that never have exactly one solution is ironic but it is the standard term.) The systems from $ X $ , $ X $ , and $ X $ each have an associated homogeneous system with a unique solution. Thus these matrices are nonsingular. $ [r] 3 &4  2 & - 1 qquad [r] 3 &2 &1  6 & - 4 &0  0 &1 &1 qquad [r] 1 &2 & - 1  2 &4 &0  0 &1 & - 3 $ . The Chemistry problem from $ X $ is a homogeneous system with more than one solution so its matrix is singular. $ [r] 7 &0 & - 7 &0  8 &1 & - 5 & - 2  0 &1 & - 3 &0  0 &3 & - 6 & - 1 $ . The table above has two dimensions. We have considered the one on top: we can tell into which column a given linear system goes solely by considering the system's left - hand side; the constants on the right - hand side play no role in this. The table's other dimension, determining whether a particular solution exists, is tougher. Consider these two systems with the same left side but different right sides. $ [b]{2} 3x &+ &2y &= &5  3x &+ &2y &= &5 qquad [b]{2} 3x &+ &2y &= &5  3x &+ &2y &= &4 $ . The first has a solution while the second does not, so here the constants on the right side decide if the system has a solution. We could conjecture that the left side of a linear system determines the number of solutions while the right side determines if solutions exist but that guess is not correct. Compare these two, with the same right sides but different left sides. $ [b]{2} 3x &+ &2y &= &5  4x &+ &2y &= &4 qquad [b]{2} 3x &+ &2y &= &5  3x &+ &2y &= &4 $ . The first has a solution but the second does not. Thus the constants on the right side of a system don't alone determine whether a solution exists. Rather, that depends on some interaction between the left and right. For some intuition about that interaction, consider this system with one of the coefficients left unspecified, as the variable  $ c $ . $ {3} x &+ &2y &+ &3z &= &1  x &+ &y &+ &z &= &1  cx &+ &3y &+ &4z &= &0 $ . If $ c=2 $ then this system has no solution because the left - hand side has the third row as the sum of the first two, while the right - hand does not. If $ cneq 2 $ then this system has a unique solution (try it with $ c=1 $ ). For a system to have a solution, if one row of the matrix of coefficients on the left is a linear combination of other rows then on the right the constant from that row must be the same combination of constants from the same rows. More intuition about the interaction comes from studying linear combinations. That will be our focus in the second chapter, after we finish the study of Gauss's Method itself in the rest of this chapter. endinput This subsection is optional. Later material will not require the work here. A set can be described in many different ways. Here are two different descriptions of a single set: $ set{colvec[r]{1  2  3}zsuchthat zinRe} quadtext{and}quad set{colvec[r]{2  4  6}wsuchthat winRe}. $ . For instance, this set contains $ colvec[r]{5  10  15} $ . (take $ z=5 $ and $ w=5/2 $ ) but does not contain $ colvec[r]{4  8  11} $ . (the first component gives $ z=4 $ but that clashes with the third component, similarly the first component gives $ w=4/5 $ but the third component gives something different). Here is a third description of the same set: $ set{colvec[r]{3  6  9}+colvec[r]{ - 1  - 2  - 3}ysuchthat yinRe}. $ . We need to decide when two descriptions are describing the same set. More pragmatically stated, how can a person tell when an answer to a homework question describes the same set as the one described in the back of the book? Sets are equal if and only if they have the same members. A common way to show that two sets, $ S_1 $ and $ S_2 $ , are equal is to show mutual inclusion: any member of $ S_1 $ is also in $ S_2 $ , and any member of $ S_2 $ is also in $ S_1 $ .appendrefs{set equality} To show that $ S_1= set{colvec[r]{1  - 1  0}c+colvec[r]{1  1  0}dsuchthat c, dinRe} $ . equals $ S_2= set{colvec[r]{4  1  0}m+colvec[r]{ - 1  - 3  0}nsuchthat m, ninRe} $ . we show first that $ S_1subseteq S_2 $ and then that $ S_2subseteq S_1 $ . For the first half we must check that any vector from $ S_1 $ is also in $ S_2 $ . We first consider two examples to use them as models for the general argument. If we make up a member of $ S_1 $ by trying $ c=1 $ and $ d=1 $ , then to show that it is in $ S_2 $ we need $ m $ and $ n $ such that $ colvec[r]{4  1  0}m +colvec[r]{ - 1  - 3  0}n =colvec[r]{2  0  0} $ . that is, this relation holds between $ m $ and $ n $ . $ {2} 4m & - &n &= &2  1m & - &3n &= &0  & &0 &= &0 $ . Similarly, if we try $ c=2 $ and $ d= - 1 $ , then to show that the resulting member of $ S_1 $ is in $ S_2 $ we need $ m $ and $ n $ such that such that $ colvec[r]{4  1  0}m +colvec[r]{ - 1  - 3  0}n =colvec[r]{3  - 3  0} $ . that is, this holds. $ {2} 4m & - &n &= &3  1m & - &3n &= & - 3  & &0 &= &0 $ . In the general case, to show that any vector from $ S_1 $ is a member of $ S_2 $ we must show that for any $ c $ and $ d $ there are appropriate $ m $ and $ n $ . We follow the pattern of the examples; fix $ colvec{c+d  - c+d  0}in S_1 $ . and look for $ m $ and $ n $ such that $ colvec[r]{4  1  0}m +colvec[r]{ - 1  - 3  0}n =colvec{c+d  - c+d  0} $ . that is, this is true. $ {2} 4m & - &n &= &c+dhfill  m & - &3n &= & - c+dhfill  & &0 &= &0hfill $ . Applying Gauss's Method $ {2} 4m & - &n &= &c+dhfill  m & - &3n &= & - c+dhfill grstep{ - (1/4)rho_1+rho_2} {2} 4m & - &n &= &c+dhfill  & & - (11/4)n &= & - (5/4)c+(3/4)dhfill $ . gives $ n=(5/11)c - (3/11)d $ and $ m=(4/11)c+(2/11)d $ . This shows that for any choice of $ c $ and $ d $ there are appropriate $ m $ and $ n $ . We conclude any member of $ S_1 $ is a member of $ S_2 $ because it can be rewritten in this way: $ colvec{c+d  - c+d  0} =colvec[r]{4  1  0}((4/11)c+(2/11)d)+ colvec[r]{ - 1  - 3  0}((5/11)c - (3/11)d). $ . For the other inclusion, $ S_2subseteq S_1 $ , we want to do the opposite. We want to show that for any choice of $ m $ and $ n $ there are appropriate $ c $ and $ d $ . So fix $ m $ and $ n $ and solve for $ c $ and $ d $ : $ {2} c &+ &d &= &4m - nhfill  - c &+ &d &= &m - 3nhfill grstep{rho_1+rho_2} {2} c &+ &d &= &4m - nhfill  & &2d &= &5m - 4nhfill $ . shows that $ d=(5/2)m - 2n $ and $ c=(3/2)m+n $ . Thus any vector from $ S_2 $ $ colvec[r]{4  1  0}m+colvec[r]{ - 1  - 3  0}n $ . is also of the right form for $ S_1 $ $ colvec[r]{1  - 1  0}((3/2)m+n) +colvec[r]{1  1  0}((5/2)m - 2n). $ . Of course, sometimes sets are not equal. The method of the prior example will help us see the relationship between the two sets. These $ P= set{colvec{x+y  2x  y}suchthat x, yinRe} quadtext{and}quad R= set{colvec{m+p  n  p}suchthat m, n, pinRe} $ . are not equal sets. While $ P $ is a subset of $ R $ , it is a proper subset of $ R $ because $ R $ is not a subset of $ P $ . To see that, observe first that given a vector from $ P $ we can express it in the form for $ R $ Dash if we fix $ x $ and $ y $ , we can solve for appropriate $ m $ , $ n $ , and $ p $ : $ {3} m & & &+ &p &= &x+yhfill  & &n & & &= &2xhfill  & & & &p &= &yhfill $ . shows that we can express any $ vec{v}= colvec[r]{1  2  0}x+ colvec[r]{1  0  1}y $ . as a member of $ R $ with $ m=x $ , $ n=2x $ , and $ p=y $ : $ vec{v}= colvec[r]{1  0  0}x+ colvec[r]{0  1  0}2x+ colvec[r]{1  0  1}y. $ . Thus $ Psubseteq R $ . But, for the other direction, the reduction resulting from fixing $ m $ , $ n $ , and $ p $ and looking for $ x $ and $ y $ $ {2} x &+ &y &= &m+phfill  2x & & &= &nhfill  & &y &= &phfill &grstep{ - 2rho_1+rho_2} {2} x &+ &y &= &m+phfill  & & - 2y&= & - 2m+n - 2phfill  & &y &= &phfill  &grstep{(1/2)rho_2+rho_3} {2} x &+ &y &= &m+phfill  & & - 2y&= & - 2m+n - 2phfill  & &0 &= &m+(1/2)nhfill $ . shows that the only vectors $ colvec{m+p  n  p}in R $ . representable in the form $ colvec{x+y  2x  y} $ . are those where $ 0=m+(1/2)n $ . For instance, $ colvec[r]{0  1  0} $ . is in $ R $ but not in $ P $ .
After developing the mechanics of Gauss's Method, we observed that it can be done in more than one way. For example, from this matrix $ [r] 2 &2  4 &3 $ . we could derive any of these three echelon form matrices. $ [r] 2 &2  0 & - 1 qquad [r] 1 &1  0 & - 1 qquad [r] 2 &0  0 & - 1 $ . The first results from $ - 2rho_1+rho_2 $ . The second comes from doing $ (1/2)rho_1 $ and then $ - 4rho_1+rho_2 $ . The third comes from $ - 2rho_1+rho_2 $ followed by $ 2rho_2+rho_1 $ (after the first row combination the matrix is already in echelon form but it is nonetheless a legal row operation). In this chapter's first section we noted that this raises questions. Will any two echelon form versions of a linear system have the same number of free variables? If yes, will the two have exactly the same free variables? In this section we will give a way to decide if one linear system can be derived from another by row operations. The answers to both questions, both ``yes, '' will follow from that. Here is an extension of Gauss's Method that has some advantages. To solve $ {3} x &+ &y & - &2z &= & - 2  & &y &+ &3z &= &7  x & & & - &z &= & - 1 $ . we can start as usual by reducing it to echelon form. $ grstep{ - rho_1+rho_3} [r]{3} 1 &1 & - 2 & - 2  0 &1 &3 &7  0 & - 1 &1 &1 grstep{rho_2+rho_3} [r]{3} 1 &1 & - 2 & - 2  0 &1 &3 &7  0 &0 &4 &8 $ . We can keep going to a second stage by making the leading entries into $ 1 $ 's $ grstep{(1/4)rho_3} [r]{3} 1 &1 & - 2 & - 2  0 &1 &3 &7  0 &0 &1 &2 $ . and then to a third stage that uses the leading entries to eliminate all of the other entries in each column by combining upwards. $ grstep[2rho_3+rho_1]{ - 3rho_3+rho_2} [r]{3} 1 &1 &0 &2  0 &1 &0 &1  0 &0 &1 &2 grstep{ - rho_2+rho_1} [r]{3} 1 &0 &0 &1  0 &1 &0 &1  0 &0 &1 &2 $ . The answer is $ x=1 $ , $ y=1 $ , and $ z=2 $ . Using one entry to clear out the rest of a column is pivoting on that entry. Notice that the row combination operations in the first stage move left to right while the combination operations in the third stage move right to left. The middle stage operations that turn the leading entries into $ 1 $ 's don't interact so we can combine multiple ones into a single step. $ [r]{2} 2 &1 &7  4 & - 2 &6 &grstep{ - 2rho_1+rho_2} [r]{2} 2 &1 &7  0 & - 4 & - 8  &grstep[( - 1/4)rho_2]{(1/2)rho_1} [r]{2} 1 &1/2 &7/2  0 &1 &2  &grstep{ - (1/2)rho_2+rho_1} [r]{2} 1 &0 &5/2  0 &1 &2 $ . The answer is $ x=5/2 $ and $ y=2 $ . This extension of Gauss's Method is the Gauss - Jordan Method or Gauss - Jordan reduction. A matrix or linear system is in reduced echelon form/ if, in addition to being in echelon form, each leading entry is a  $ 1 $ and is the only nonzero entry in its column. noindent The cost of using Gauss - Jordan reduction to solve a system is the additional arithmetic. The benefit is that we can just read off the solution set description. In any echelon form system, reduced or not, we can read off when the system has an empty solution set because there is a contradictory equation. We can read off when the system has a one - element solution set because there is no contradiction and every variable is the leading variable in some row. And, we can read off when the system has an infinite solution set because there is no contradiction and at least one variable is free. However, in reduced echelon form we can read off not just the size of the solution set but also its description. We have no trouble describing the solution set when it is empty, of course. $ X $ and $ X $ show how in a single element solution set case the single element is in the column of constants. The next example shows how to read the parametrization of an infinite solution set. [r]{4} 2 &6 &1 &2 &5  0 &3 &1 &4 &1  0 &3 &1 &2 &5 grstep{ - rho_2+rho_3} [r]{4} 2 &6 &1 &2 &5  0 &3 &1 &4 &1  0 &0 &0 & - 2 &4  grstep[(1/3)rho_2  - (1/2)rho_3]{(1/2)rho_1} repeatedgrstep[ - rho_3+rho_1]{ - (4/3)rho_3+rho_2} repeatedgrstep{ - 3rho_2+rho_1} [r]{4} 1 &0 & - 1/2 &0 & - 9/2  0 &1 &1/3 &0 &3  0 &0 &0 &1 & - 2 As a linear system this is $ {4} x_1 && & - &1/2x_3 && &= & - 9/2  &&x_2 &+&1/3x_3 && &= &3  && && &{}hspace{.5em}{}&x_4 &= & - 2 $ . so a solution set description is this. $ S=set{colvec{x_1  x_2  x_3  x_4} =colvec[r]{ - 9/2  3  0  - 2} +colvec[r]{1/2  - 1/3  1  0}x_3 suchthat x_3inRe} $ . Thus, echelon form isn't some kind of one best form for systems. Other forms, such as reduced echelon form, have advantages and disadvantages. Instead of picturing linear systems (and the associated matrices) as things we operate on, always directed toward the goal of echelon form, we can think of them as interrelated, where we can get from one to another by row operations. The rest of this subsection develops this thought. Elementary row operations are reversible. For any matrix $ A $ , the effect of swapping rows is reversed by swapping them back, multiplying a row by a nonzero $ k $ is undone by multiplying by $ 1/k $ , and adding a multiple of row $ i $ to row $ j $ (with $ ineq j $ ) is undone by subtracting the same multiple of row $ i $ from row $ j $ . $ A grstep{rho_ileftrightarrowrho_j} repeatedgrstep{rho_jleftrightarrowrho_i} A qquad A grstep{krho_i} repeatedgrstep{(1/k)rho_i} A qquad A grstep{krho_i+rho_j} repeatedgrstep{ - krho_i+rho_j} A $ . (We need the $ ineq j $ condition; see $ X $ .) Again, the point of view that we are developing, supported now by the lemma, is that the term `reduces to' is misleading: where $ Alongrightarrow B $ , we shouldn't think of $ B $ as after  $ A $ or simpler than  $ A $ . Instead we should think of the two matrices as interrelated. Below is a picture. It shows the matrices from the start of this section and their reduced echelon form version in a cluster, as interreducible. We say that matrices that reduce to each other are equivalent with respect to the relationship of row reducibility. The next result justifies this, using the definition of an equivalence.appendrefs{equivalence relations} Between matrices, `reduces to' is an equivalence re - la - tion. We must check the conditions (i) reflexivity, that any matrix reduces to itself, (ii) symmetry, that if $ A $ reduces to $ B $ then $ B $ reduces to $ A $ , and (iii) transitivity, that if $ A $ reduces to $ B $ and $ B $ reduces to $ C $ then $ A $ reduces to $ C $ . Reflexivity is easy; any matrix reduces to itself in zero - many operations. The relationship is symmetric by the prior lemmaDash if $ A $ reduces to $ B $ by some row operations then also $ B $ reduces to $ A $ by reversing those operations. For transitivity, suppose that $ A $ reduces to $ B $ and that $ B $ reduces to $ C $ . Following the reduction steps from $ A rightarrowcdotsrightarrow B $ with those from $ B rightarrowcdotsrightarrow C $ gives a reduction from $ A $ to $ C $ . Two matrices that are interreducible by elementary row operations are row equivalent. The diagram below shows the collection of all matrices as a box. Inside that box each matrix lies in a class. Matrices are in the same class if and only if they are interreducible. The classes are disjointDash no matrix is in two distinct classes. We have partitioned the collection of matrices into row equivalence classes.appendrefs{partitions and class representatives} noindent One of the classes is the cluster of interrelated matrices from the start of this section sketched above (it includes all of the nonsingular $ nbyn{2} $ matrices). The next subsection proves that the reduced echelon form of a matrix is unique. Rephrased in terms of the row - equivalence relationship, we shall prove that every matrix is row equivalent to one and only one reduced echelon form matrix. In terms of the partition what we shall prove is: every equivalence class contains one and only one reduced echelon form matrix. So each reduced echelon form matrix serves as a representative of its class. We will close this chapter by proving that every matrix is row equivalent to one and only one reduced echelon form matrix. The ideas here will reappear, and be further developed, in the next chapter. The crucial observation concerns how row operations act to transform one matrix into another: the new rows are linear combinations of the old. Consider this Gauss - Jordan reduction. $ {2} 2 &1 &0  1 &3 &5 &grstep{ - (1/2)rho_1+rho_2} {2} 2 &1 &0  0 &5/2 &5  &grstep[(2/5)rho_2]{(1/2)rho_1} {2} 1 &1/2 &0  0 &1 &2  &grstep{ - (1/2)rho_2+rho_1}; {2} 1 &0 & - 1  0 &1 &2 $ . Denoting those matrices $ Arightarrow Drightarrow Grightarrow B $ and writing the rows of $ A $ as $ alpha_1 $ and $ alpha_2 $ , etc., we have this. $ multline looks worse left({l} alpha_1  alpha_2 right) &grstep{ - (1/2)rho_1+rho_2} left({l} delta_1=alpha_1  delta_2= - (1/2)alpha_1+alpha_2 right)  &grstep[(2/5)rho_2]{(1/2)rho_1} left({l} gamma_1=(1/2)alpha_1  gamma_2= - (1/5)alpha_1+(2/5)alpha_2 right)  &grstep{ - (1/2)rho_2+rho_1} left({l} beta_1=(3/5)alpha_1 - (1/5)alpha_2  beta_2= - (1/5)alpha_1+(2/5)alpha_2 right) $ . The fact that Gaussian operations combine rows linearly also holds if there is a row swap. With this $ A $ , $ D $ , $ G $ , and $ B $ $ [r] 0 &2  1 &1 grstep{rho_1leftrightarrowrho_2} [r] 1 &1  0 &2 grstep{(1/2)rho_2} [r] 1 &1  0 &1 grstep{ - rho_2+rho_1} [r] 1 &0  0 &1 $ . we get these linear relationships. left({l} vec{alpha}_1  vec{alpha}_2 right) , grstep{rho_1leftrightarrowrho_2}, left({l} vec{delta}_1 =vec{alpha}_2  vec{delta}_2 =vec{alpha}_1 right) , grstep{(1/2)rho_2}, left({l} vec{gamma}_1 =vec{alpha}_2  vec{gamma}_2 =(1/2)vec{alpha}_1 right)  , grstep{ - rho_2+rho_1}, left({l} vec{beta}_1 =( - 1/2)vec{alpha}_1+1cdotvec{alpha}_2  vec{beta}_2 =(1/2)vec{alpha}_1 right) In summary, Gauss's Method systematically finds a suitable sequence of linear combinations of the rows. [Linear Combination Lemma] A linear combination of linear combinations is a linear combination. Given the set $ c_{1, 1}x_1+dots+c_{1, n}x_n $ through $ c_{m, 1}x_1+dots+c_{m, n}x_n $ of linear combinations of the $ x $ 's, consider a combination of those $ d_1(c_{1, 1}x_1+dots+c_{1, n}x_n), +dots+, d_m(c_{m, 1}x_1+dots+c_{m, n}x_n) $ . where the $ d $ 's are scalars along with the $ c $ 's. Distributing those $ d $ 's and regrouping gives $ =(d_1c_{1, 1}+dots+d_mc_{m, 1})x_1, +dots+, (d_1c_{1, n}+dots+d_mc_{m, n})x_n $ . which is also a linear combination of the $ x $ 's. Where one matrix reduces to another, each row of the second is a linear combination of the rows of the first. For any two interreducible matrices $ A $ and  $ B $ there is some minimum number of row operations that will take one to the other. We proceed by induction on that number. In the base step, that we can go from one matrix to another using zero reduction operations, the two are equal. Then each row of $ B $ is trivially a combination of $ A $ 's rows $ vec{beta}_i =0cdotvec{alpha}_1+cdots+1cdotvec{alpha}_i+cdots+0cdotvec{alpha}_m $ . For the inductive step assume the inductive hypothesis: with $ kgeq 0 $ , any matrix that can be derived from $ A $ in $ k $ or fewer operations has rows that are linear combinations of $ A $ 's rows. Consider a matrix  $ B $ such that reducing $ A $ to  $ B $ requires $ k+1 $ operations. In that reduction there is a next - to - last matrix  $ G $ , so that $ Alongrightarrowcdotslongrightarrow Glongrightarrow B $ . The inductive hypothesis applies to this $ G $ because it is only $ k $ steps away from $ A $ . That is, each row of $ G $ is a linear combination of the rows of $ A $ . We will verify that the rows of  $ B $ are linear combinations of the rows of  $ G $ . Then the Linear Combination Lemma, $ X $ , applies to show that the rows of  $ B $ are linear combinations of the rows of  $ A $ . If the row operation taking $ G $ to  $ B $ is a swap then the rows of $ B $ are just the rows of $ G $ reordered and each row of $ B $ is a linear combination of the rows of $ G $ . If the operation taking $ G $ to  $ B $ is multiplication of a row by a scalar  $ crho_i $ then $ vec{beta}_i=cvec{gamma}_i $ and the other rows are unchanged. Finally, if the row operation is adding a multiple of one row to another $ rrho_i+rho_j $ then only row  $ j $ of $ B $ differs from the matching row of  $ G $ , and $ vec{beta}_j=rgamma_i+gamma_j $ , which is indeed a linear combinations of the rows of $ G $ . Because we have proved both a base step and an inductive step, the proposition follows by the principle of mathematical induction. We now have the insight that Gauss's Method builds linear combinations of the rows. But of course its goal is to end in echelon form, since that is a particularly basic version of a linear system, as it has isolated the variables. For instance, in this matrix $ R=[r] 2 &3 &7 &8 &0 &0  0 &0 &1 &5 &1 &1  0 &0 &0 &3 &3 &0  0 &0 &0 &0 &2 &1 $ . $ x_1 $ has been removed from $ x_5 $ 's equation. That is, Gauss's Method has made $ x_5 $ 's row in some way independent of $ x_1 $ 's row. The following result makes this intuition precise. We sometimes refer to Gauss's Method as Gaussian elimination. What it eliminates is linear relationships among the rows. In an echelon form matrix, no nonzero row is a linear combination of the other nonzero rows. Let $ R $ be an echelon form matrix and consider its non - $ vec{0} $ rows. First observe that if we have a row written as a combination of the others $ vec{rho}_i=c_1vec{rho}_1+cdots+c_{i - 1}vec{rho}_{i - 1}+ c_{i+1}vec{rho}_{i+1}+cdots+c_mvec{rho}_m $ then we can rewrite that equation as $ vec{0}=c_1vec{rho}_1+cdots+c_{i - 1}vec{rho}_{i - 1}+c_ivec{rho}_i+ c_{i+1}vec{rho}_{i+1}+cdots+c_mvec{rho}_m tag{ $ * $ } $ . where not all the coefficients are zero; specifically, $ c_i= - 1 $ . The converse holds also: given equation ( $ * $ ) where some $ c_ineq 0 $ we could express $ vec{rho}_i $ as a combination of the other rows by moving $ c_ivec{rho}_i $ to the left and dividing by $ - c_i $ . Therefore we will have proved the theorem if we show that in ( $ * $ ) all of the coefficients are  $ 0 $ . For that we use induction on the row number  $ i $ . The base case is the first row  $ i=1 $ (if there is no such nonzero row, so that $ R $ is the zero matrix, then the lemma holds vacuously). Let $ ell_i $ be the column number of the leading entry in row  $ i $ . Consider the entry of each row that is in column  $ ell_1 $ . Equation ( $ * $ ) gives this. $ 0=c_1r_{1, ell_1}+c_2r_{2, ell_1}+cdots+c_mr_{m, ell_1} tag{ $ ** $ } $ . The matrix is in echelon form so every row after the first has a zero entry in that column $ r_{2, ell_1}=cdots=r_{m, ell_1}=0 $ . Thus equation ( $ ** $ ) shows that $ c_1=0 $ , because $ r_{1, ell_1}neq 0 $ as it leads the row. The inductive step is much the same as the base step. Again consider equation ( $ * $ ). We will prove that if the coefficient $ c_i $ is $ 0 $ for each row index $ iinset{1, ldots, k} $ then $ c_{k+1} $ is also $ 0 $ . We focus on the entries from column  $ ell_{k+1} $ . $ 0=c_1r_{1, ell_{k+1}}+cdots+c_{k+1}r_{k+1, ell_{k+1}}+cdots+c_mr_{m, ell_{k+1}} $ . By the inductive hypothesis $ c_1 $ , ldots $ c_k $ are all $ 0 $ so this reduces to the equation $ 0=c_{k+1}r_{k+1, ell_{k+1}}+cdots+c_mr_{m, ell_{k+1}} $ . The matrix is in echelon form so the entries $ r_{k+2, ell_{k+1}} $ , ldots, $ r_{m, ell_{k+1}} $ are all  $ 0 $ . Thus $ c_{k+1}=0 $ , because $ r_{k+1, ell_{k+1}}neq 0 $ as it is the leading entry. With that, we are ready to show that the end product of Gauss - Jordan reduction is unique. Each matrix is row equivalent to a unique reduced echelon form matrix. Fix a number of rows $ m $ . We will proceed by induction on the number of columns $ n $ . The base case is that the matrix has $ n=1 $ column. If this is the zero matrix then its echelon form is the zero matrix. If instead it has any nonzero entries then when the matrix is brought to reduced echelon form it must have at least one nonzero entry, which must be a $ 1 $ in the first row. Either way, its reduced echelon form is unique. For the inductive step we assume that $ n>1 $ and that all $ m $  row matrices having fewer than  $ n $ columns have a unique reduced echelon form. Consider an $ nbym{m}{n} $ matrix $ A $ and suppose that $ B $ and $ C $ are two reduced echelon form matrices derived from $ A $ . We will show that these two must be equal. Let $ hat{A} $ be the matrix consisting of the first $ n - 1 $ columns of $ A $ . Observe that any sequence of row operations that bring $ A $ to reduced echelon form will also bring $ hat{A} $ to reduced echelon form. By the inductive hypothesis this reduced echelon form of $ hat{A} $ is unique, so if $ B $ and $ C $ differ then the difference must occur in column  $ n $ . We finish the inductive step and the argument by showing that the two cannot differ only in that column. Consider a homogeneous system of equations for which $ A $ is the matrix of coefficients. $ {4} a_{1, 1}x_1 &+ &a_{1, 2}x_2 &+ &cdots &+ &a_{1, n}x_n &= &0  a_{2, 1}x_1 &+ &a_{2, 2}x_2 &+ &cdots &+ &a_{2, n}x_n &= &0  &&&&&&&vdotswithin{=}  a_{m, 1}x_1 &+ &a_{m, 2}x_2 &+ &cdots &+ &a_{m, n}x_n &= &0 tag{ $ * $ } $ . By Theorem One.I. the set of solutions to that system is the same as the set of solutions to $ B $ 's system $ {4} b_{1, 1}x_1 &+ &b_{1, 2}x_2 &+ &cdots &+ &b_{1, n}x_n &= &0  b_{2, 1}x_1 &+ &b_{2, 2}x_2 &+ &cdots &+ &b_{2, n}x_n &= &0  &&&&&&&vdotswithin{=}  b_{m, 1}x_1 &+ &b_{m, 2}x_2 &+ &cdots &+ &b_{m, n}x_n &= &0 tag{ $ ** $ } $ . and to $ C $ 's. $ quad {4} c_{1, 1}x_1 &+ &c_{1, 2}x_2 &+ &cdots &+ &c_{1, n}x_n &= &0  c_{2, 1}x_1 &+ &c_{2, 2}x_2 &+ &cdots &+ &c_{2, n}x_n &= &0  &&&&&&&vdotswithin{=}  c_{m, 1}x_1 &+ &c_{m, 2}x_2 &+ &cdots &+ &c_{m, n}x_n &= &0 tag{ $ mathord{*}mathord{*}mathord{*} $ } $ . With $ B $ and $ C $ different only in column  $ n $ , suppose that they differ in row  $ i $ . Subtract row  $ i $ of ( $ mathord{*}mathord{*}mathord{*} $ ) from row  $ i $ of ( $ ** $ ) to get the equation $ (b_{i, n} - c_{i, n})cdot x_n=0 $ . We've assumed that $ b_{i, n}neq c_{i, n} $ and so we get $ x_n=0 $ . Thus $ x_n $ is not a free variable and so in ( $ ** $ ) and ( $ mathord{*}mathord{*}mathord{*} $ ) the $ n $ - th column contains the leading entry of some row, since in an echelon form matrix any column that does not contain a leading entry is associated with a free variable. But now, with $ B $ and $ C $ equal on the first $ n - 1 $  columns, by the definition of reduced echeleon form their leading entries in the $ n $ - th column are in the same row. And, both leading entries would have to be $ 1 $ , and would have to be the only nonzero entries in that column. Therefore $ B=C $ . We have asked whether any two echelon form versions of a linear system have the same number of free variables, and if so are they exactly the same variables? With the prior result we can answer both questions ``yes.'' There is no linear system such that, say, we could apply Gauss's Method one way and get $ y $ and $ z $ free but apply it another way and get $ y $ and $ w $ free. Before the proof, recall the distinction between free variables and parameters. This system $ {3} x &+ &y & & &= &1  & &y &+ &z &= &2 $ . has one free variable,   $ z $ , because it is the only variable not leading a row. We have the habit of parametrizing using the free variable $ y=2 - z $ , $ x= - 1+z $ , but we could also parametrize using another variable, such as $ z=2 - y $ , $ x=1 - y $ . So the set of parameters is not unique, it is the set of free variables that is unique. If from a starting linear systems we derive by Gauss's Method two different echelon form systems, then the two have the same free variables. The prior result says that the reduced echelon form is unique. We get from any echelon form version to the reduced echelon form by eliminating up, so any echelon form version of a system has the same free variables as the reduced echelon form version. We close with a recap. In Gauss's Method we start with a matrix and then derive a sequence of other matrices. We defined two matrices to be related if we can derive one from the other. That relation is an equivalence relation, appendrefs{equivalence relation} called row equivalence, and so partitions the set of all matrices into row equivalence classes. (There are infinitely many matrices in the pictured class, but we've only got room to show two.) We have proved there is one and only one reduced echelon form matrix in each row equivalence class. So the reduced echelon form is a canonical formappendrefs{canonical representatives}spacefactor=1000 for row equivalence: the reduced echelon form matrices are representatives of the classes. The idea here is that one way to understand a mathematical situation is by being able to classify the cases that can happen. This is a theme in this book and we have seen this several times already. We classified solution sets of linear systems into the no - elements, one - element, and infinitely - many elements cases. We also classified linear systems with the same number of equations as unknowns into the nonsingular and singular cases. Here, where we are investigating row equivalence, we know that the set of all matrices breaks into the row equivalence classes and we now have a way to put our finger on each of those classesDash we can think of the matrices in a class as derived by row operations from the unique reduced echelon form matrix in that class. Put in more operational terms, uniqueness of reduced echelon form lets us answer questions about the classes by translating them into questions about the representatives. For instance, as promised in this section's opening, we now can decide whether one matrix can be derived from another by row reduction. We apply the Gauss - Jordan procedure to both and see if they yield the same reduced echelon form. These matrices are not row equivalent $ [r] 1 & - 3  - 2 &6 qquad [r] 1 & - 3  - 2 &5 $ . because their reduced echelon forms are not equal. $ [r] 1 & - 3  0 &0 qquad [r] 1 &0  0 &1 $ . Any nonsingular $ nbyn{3} $ matrix Gauss - Jordan reduces to this. $ [r] 1 &0 &0  0 &1 &0  0 &0 &1 $ . We can describe all the classes by listing all possible reduced echelon form matrices. Any $ nbyn{2} $ matrix lies in one of these: the class of matrices row equivalent to this, $ [r] 0 &0  0 &0 $ . the infinitely many classes of matrices row equivalent to one of this type $ 1 &a  0 &0 $ . where $ ainRe $ (including $ a=0 $ ), the class of matrices row equivalent to this, $ [r] 0 &1  0 &0 $ . and the class of matrices row equivalent to this $ [r] 1 &0  0 &1 $ . (this is the class of nonsingular $ nbyn{2} $ matrices).
Representations vary with the bases. For instance, with respect to the bases $ stdbasis_2 $ and $ B=sequence{colvec{1  1}, colvec{1  - 1}} $ . $ vec{e}_1inRe^2 $ has these different representations. $ rep{vec{e}_1}{stdbasis_2}=colvec{1  0} qquad rep{vec{e}_1}{B}=colvec{1/2  1/2} $ . The same holds for maps: with respect to the basis pairs $ stdbasis_2, stdbasis_2 $ and $ stdbasis_2, B $ , the identity map has these representations. $ rep{text{id}}{stdbasis_2, stdbasis_2}= [r] 1 &0  0 &1 qquad rep{text{id}}{stdbasis_2, B}= [r] 1/2 &1/2  1/2 & - 1/2 $ . This section shows how to translate among the representations. That is, we will compute how the representations vary as the bases vary. In converting $ rep{vec{v}}{B} $ to $ rep{vec{v}}{D} $ the underlying vector $ vec{v} $ doesn't change. Thus, the translation between these two ways of expressing the vector is accomplished by the identity map on the space, described so that the domain space vectors are represented with respect to $ B $ and the codomain space vectors are represented with respect to $ D $ . $ V_{wrt{B}}  @V{text{scriptsize $ identity $ }} VV  V_{wrt{D}} $ . (This diagram is vertical to fit with the ones in the next subsection.) The change of basis matrix for bases $ B, Dsubset V $ is the representation of the identity map $ map{identity}{V}{V} $ with respect to those bases. $ rep{identity}{B, D}= {c@{hspace{1em}}c@{hspace{1em}}c} vdots & &vdots  rep{vec{beta}_1}{D} &;cdots; &rep{vec{beta}_n}{D}  vdots & &vdots $ . A better name would be `change of representation matrix' but the above name is standard. The next result supports the definition. To convert from the representation of a vector  $ vec{v} $ with respect to $ B $ to its representation with respect to $ D $ use the change of basis matrix. $ rep{identity}{B, D}, rep{vec{v}}{B}=rep{vec{v}}{D} $ . Conversely, if left - multiplication by a matrix changes bases $ Mcdotrep{vec{v}}{B}=rep{vec{v}}{D} $ then $ M $ is a change of basis matrix. The first sentence holds because matrix - vector multiplication represents a map application and so $ rep{identity}{B, D}cdotrep{vec{v}}{B}=rep{, identity(vec{v}), }{D} =rep{vec{v}}{D} $ for each $ vec{v} $ . For the second sentence, with respect to $ B, D $ the matrix $ M $ represents a linear map whose action is to map each vector to itself, and is therefore the identity map. With these bases for $ Re^2 $ , $ B= sequence{ colvec[r]{2  1}, colvec[r]{1  0} } qquad D= sequence{ colvec[r]{ - 1  1}, colvec[r]{1  1} } $ . because $ rep{, identity(colvec[r]{2  1})}{D} =colvec[r]{ - 1/2  3/2}_D qquad rep{, identity(colvec[r]{1  0})}{D} =colvec[r]{ - 1/2  1/2}_D $ . the change of basis matrix is this. $ rep{rm id}{B, D} = [r] - 1/2 & - 1/2  3/2 &1/2 $ . For instance, this is the representation of $ vec{e}_2 $ $ rep{colvec[r]{0  1} }{B} =colvec[r]{1  - 2} $ . and the matrix does the conversion. $ [r] - 1/2 & - 1/2  3/2 &1/2 colvec[r]{1  - 2} = colvec[r]{1/2  1/2} $ . Checking that vector on the right is $ rep{vec{e}_2 }{D} $ is easy. We finish this subsection by recognizing the change of basis matrices as a familiar set. A matrix changes bases if and only if it is nonsingular. For the `only if' direction, if left - multiplication by a matrix changes bases then the matrix represents an invertible function, simply because we can invert the function by changing the bases back. Because it represents a function that is invertible, the matrix itself is invertible, and so is nonsingular. For `if' we will show that any nonsingular matrix $ M $ performs a change of basis operation from any given starting basis $ B $ (having  $ n $ vectors, where the matrix is $ nbyn{n} $ ) to some ending basis. If the matrix is the identity  $ I $ then the statement is obvious. Otherwise because the matrix is nonsingular Corollary IV. says there are elementary reduction matrices such that $ R_rcdots R_1cdot M=I $ with $ rgeq 1 $ . Elementary matrices are invertible and their inverses are also elementary so multiplying both sides of that equation from the left by $ {R_r}^{ - 1} $ , then by $ {R_{r - 1}}^{ - 1} $ , etc., gives $ M $ as a product of elementary matrices $ M={R_1}^{ - 1}cdots {R_r}^{ - 1} $ . We will be done if we show that elementary matrices change a given basis to another basis, since then $ {R_r}^{ - 1} $ changes $ B $ to some other basis $ B_r $ and $ {R_{r - 1}}^{ - 1} $ changes $ B_r $ to some $ B_{r - 1} $ , etc. We will cover the three types of elementary matrices separately; recall the notation for the three. $ M_{i}(k) colvec{ c_1  vdots  c_i  vdots  c_n } = colvec{ c_1  vdots  kc_i  vdots  c_n } quad P_{i, j} colvec{ c_1  vdots  c_i  vdots  c_j  vdots  c_n } = colvec{ c_1  vdots  c_j  vdots  c_i  vdots  c_n } quad C_{i, j}(k) colvec{ c_1  vdots  c_i  vdots  c_j  vdots  c_n } = colvec{ c_1  vdots  c_i  vdots  kc_i+c_j  vdots  c_n } $ . Applying a row - multiplication matrix  $ M_{i}(k) $ changes a representation with respect to $ sequence{vec{beta}_1, dots, vec{beta}_i, dots, vec{beta}_n } $ to one with respect to $ sequence{vec{beta}_1, dots, (1/k)vec{beta}_i, dots, vec{beta}_n } $ . vec{v}= c_1cdotvec{beta}_1+dots+c_icdotvec{beta}_i +dots+c_ncdotvec{beta}_n  mapsto; c_1cdotvec{beta}_1+dots+kc_icdot(1/k)vec{beta}_i+dots +c_ncdotvec{beta}_n=vec{v} The second one is a basis because the first is a basis and because of the $ kneq 0 $ restriction in the definition of a row - multiplication matrix. Similarly, left - multiplication by a row - swap matrix $ P_{i, j} $ changes a representation with respect to the basis $ sequence{vec{beta}_1, dots, vec{beta}_i, dots, vec{beta}_j, dots, vec{beta}_n } $ into one with respect to this basis $ sequence{vec{beta}_1, dots, vec{beta}_j, dots, vec{beta}_i, dots, vec{beta}_n } $ . vec{v}= c_1cdotvec{beta}_1+dots+c_icdotvec{beta}_i +dots+c_jvec{beta}_j+dots+c_ncdotvec{beta}_n  mapsto; c_1cdotvec{beta}_1+dots+c_jcdotvec{beta}_j+dots +c_icdotvec{beta}_i+dots+c_ncdotvec{beta}_n=vec{v} And, a representation with respect to $ sequence{vec{beta}_1, dots, vec{beta}_i, dots, vec{beta}_j, dots, vec{beta}_n } $ changes via left - multiplication by a row - combination matrix $ C_{i, j}(k) $ into a representation with respect to $ sequence{vec{beta}_1, dots, vec{beta}_i - kvec{beta}_j, dots, vec{beta}_j, dots, vec{beta}_n } $ vec{v}= c_1cdotvec{beta}_1+dots+c_icdotvec{beta}_i +c_jvec{beta}_j+dots+c_ncdotvec{beta}_n  mapsto; c_1cdotvec{beta}_1+dots+c_icdot(vec{beta}_i - kvec{beta}_j)+dots +(kc_i+c_j)cdotvec{beta}_j+dots+c_ncdotvec{beta}_n=vec{v} (the definition of $ C_{i, j}(k) $ specifies that $ ineq j $ and $ kneq 0 $ ). A matrix is nonsingular if and only if it represents the identity map with respect to some pair of bases. The first subsection shows how to convert the representation of a vector with respect to one basis to the representation of that same vector with respect to another basis. We next convert the representation of a map with respect to one pair of bases to the representation with respect to a different pairDash we convert from $ rep{h}{B, D} $ to $ rep{h}{hat{B}, hat{D}} $ . Here is the arrow diagram. $ V_{wrt{B}} @>h>H> W_{wrt{D}}  @V{text{scriptsize $ identity $ }} VV @V{text{scriptsize $ identity $ }} VV  V_{wrt{hat{B}}} @>h>hat{H}> W_{wrt{hat{D}}} $ . To move from the lower - left to the lower - right we can either go straight over, or else up to $ V_B $ then over to $ W_D $ and then down. So we can calculate $ hat{H}=rep{h}{hat{B}, hat{D}} $ either by directly using $ hat{B} $ and $ hat{D} $ , or else by first changing bases with $ rep{identity}{hat{B}, B} $ then multiplying by $ H=rep{h}{B, D} $ and then changing bases with $ rep{identity}{D, hat{D}} $ . To convert from the matrix  $ H $ representing a map  $ h $ with respect to $ B, D $ to the matrix  $ hat{H} $ representing it with respect to  $ hat{B}, hat{D} $ use this formula. $ hat{H}= rep{identity}{D, hat{D}}cdot Hcdot rep{identity}{hat{B}, B} tag*{text{( $ * $ })} $ . This is evident from the diagram. The matrix $ T=[r] cos(pi/6) & - sin(pi/6)  sin(pi/6) &cos(pi/6) = [r] sqrt{3}/2 & - 1/2  1/2 &sqrt{3}/2 $ . represents, with respect to $ stdbasis_2, stdbasis_2 $ , the transformation $ map{t}{Re^2}{Re^2} $ that rotates vectors through the counterclockwise angle of $ pi/6 $ radians. We can translate $ T $ to a representation with respect to these $ hat{B}=sequence{ colvec[r]{1  1} colvec[r]{0  2} } qquad hat{D}=sequence{ colvec[r]{ - 1  0} colvec[r]{2  3} } $ . by using the arrow diagram above. $ Re^2_{wrt{stdbasis_2}} @>t>T> Re^2_{wrt{stdbasis_2}}  @Vtext{scriptsize $ identity $ } VV @Vtext{scriptsize $ identity $ } VV  Re^2_{wrt{hat{B}}} @>t>hat{T}> Re^2_{wrt{hat{D}}} $ . The picture illustrates that we can compute  $ hat{T} $ either directly by going along the square's bottom, or as in formula ( $ * $ ) by going up on the left, then across the top, and then down on the right, with $ hat{T}= rep{identity}{stdbasis_2, hat{D}}cdot Tcdot rep{identity}{hat{B}, stdbasis_2} $ . (Note again that the matrix multiplication reads right to left, as the three functions are composed and function composition reads right to left.) Find the matrix for the left - hand side, the matrix  $ rep{identity}{hat{B}, stdbasis_2} $ , in the usual way: find the effect of the identity matrix on the starting basis  $ hat{B} $ Dash which is no effect at allDash and then represent those basis elements with respect to the ending basis  $ stdbasis_2 $ . $ rep{identity}{hat{B}, stdbasis_2} = 1 &0  1 &2 $ . This calculation is easy when the ending basis is the standard one. There are two ways to compute the matrix for going down the square's right side, $ rep{identity}{stdbasis_2, hat{D}} $ . We could calculate it directly as we did for the other change of basis matrix. Or, we could instead calculate it as the inverse of the matrix for going up $ rep{identity}{hat{D}, stdbasis_2} $ . That matrix is easy to find and we have a formula for the $ nbyn{2} $ inverse, so that's what is in the equation below. $ rep{t}{hat{B}, hat{D}} &=[r] - 1 &2  0 &3 ^{ - 1} [r] sqrt{3}/2 & - 1/2  1/2 &sqrt{3}/2 [r] 1 &0  1 &2  &=[r] (5 - sqrt{3})/6 &(3+2sqrt{3})/3  (1+sqrt{3})/6 &sqrt{3}/3 $ . The matrix is messier but the map that it represents is the same. For instance, to replicate the effect of $ t $ in the picture, start with $ hat{B} $ , $ rep{colvec[r]{1  3}}{hat{B}}=colvec[r]{1  1}_{hat{B}} $ . apply $ hat{T} $ , $ [r] (5 - sqrt{3})/6 &(3+2sqrt{3})/3  (1+sqrt{3})/6 &sqrt{3}/3 _{hat{B}, hat{D}} colvec[r]{1  1}_{hat{B}} = colvec[r]{(11+3sqrt{3})/6  (1+3sqrt{3})/6}_{hat{D}} $ . and check it against $ hat{D} $ . $ frac{11+3sqrt{3}}{6}cdotcolvec[r]{ - 1  0} +frac{1+3sqrt{3}}{6}cdotcolvec[r]{2  3} =colvec[r]{( - 3+sqrt{3})/2  (1+3sqrt{3})/2} $ . Changing bases can make the matrix simpler. On $ Re^3 $ the map $ colvec{x  y  z}mapsunder{t}colvec{y+z  x+z  x+y} $ . is represented with respect to the standard basis in this way. $ rep{t}{stdbasis_3, stdbasis_3}= [r] 0 &1 &1  1 &0 &1  1 &1 &0 $ . Representing it with respect to $ B=sequence{colvec[r]{1  - 1  0}, colvec[r]{1  1  - 2}, colvec[r]{1  1  1}} $ . gives a matrix that is diagonal. $ rep{t}{B, B}= [r] - 1 &0 &0  0 & - 1 &0  0 &0 &2 $ . Naturally we usually prefer representations that are easier to understand. We say that a map or matrix has been diagonalized when we find a basis  $ B $ such that the representation is diagonal with respect to $ B, B $ , that is, with respect to the same starting basis as ending basis. Chapter Five finds which maps and matrices are diagonalizable. The rest of this subsection develops the easier case of finding two bases $ B, D $ such that a representation is simple. Recall that the prior subsection shows that a matrix is a change of basis matrix if and only if it is nonsingular. Same - sized matrices $ H $ and $ hat{H} $ are matrix equivalent/ if there are nonsingular matrices $ P $ and $ Q $ such that $ hat{H}=PHQ $ . Matrix equivalent matrices represent the same map, with respect to appropriate pairs of bases. This is immediate from equation ( $ * $ ) above. $ X $ checks that matrix equivalence is an equivalence relation. Thus it partitions the set of matrices into matrix equivalence classes. We can get insight into the classes by comparing matrix equivalence with row equivalence (remember that matrices are row equivalent when they can be reduced to each other by row operations). In $ hat{H}=PHQ $ , the matrices $ P $ and $ Q $ are nonsingular and thus each is a product of elementary reduction matrices by Lemma IV.. Left - multiplication by the reduction matrices making up $ P $ performs row operations. Right - multiplication by the reduction matrices making up $ Q $ performs column operations. Hence, matrix equivalence is a generalization of row equivalenceDash two matrices are row equivalent if one can be converted to the other by a sequence of row reduction steps, while two matrices are matrix equivalent if one can be converted to the other by a sequence of row reduction steps followed by a sequence of column reduction steps. Consequently, if matrices are row equivalent then they are also matrix equivalent since we can take $ Q $ to be the identity matrix. The converse, however, does not hold: two matrices can be matrix equivalent but not row equivalent. These two are matrix equivalent $ [r] 1 &0  0 &0 qquad [r] 1 &1  0 &0 $ . because the second reduces to the first by the column operation of taking $ - 1 $ times the first column and adding to the second. They are not row equivalent because they have different reduced echelon forms (both are already in reduced form). We close this section by giving a set of representatives for the matrix equivalence classes. appendrefs{class representatives} Any $ nbym{m}{n} $ matrix of rank $ k $ is matrix equivalent to the $ nbym{m}{n} $ matrix that is all zeros except that the first $ k $ diagonal entries are ones. $ 1 &0 &ldots &0 &0 &ldots &0  0 &1 &ldots &0 &0 &ldots &0  &vdots  0 &0 &ldots &1 &0 &ldots &0  0 &0 &ldots &0 &0 &ldots &0  &vdots  0 &0 &ldots &0 &0 &ldots &0 $ . noindent This is a block partial - identity form. $ {c|c} I &Z  hline Z &Z $ . Gauss - Jordan reduce the given matrix and combine all the row reduction matrices to make $ P $ . Then use the leading entries to do column reduction and finish by swapping the columns to put the leading ones on the diagonal. Combine the column reduction matrices into $ Q $ . We illustrate the proof by finding $ P $ and $ Q $ for this matrix. $ [r] 1 &2 &1 & - 1  0 &0 &1 & - 1  2 &4 &2 & - 2 $ . First Gauss - Jordan row - reduce. $ [r] 1 & - 1 &0  0 &1 &0  0 &0 &1 [r] 1 &0 &0  0 &1 &0  - 2 &0 &1 [r] 1 &2 &1 & - 1  0 &0 &1 & - 1  2 &4 &2 & - 2 = [r] 1 &2 &0 &0  0 &0 &1 & - 1  0 &0 &0 &0 $ . Then column - reduce, which involves right - multiplication. $ [r] 1 &2 &0 &0  0 &0 &1 & - 1  0 &0 &0 &0 [r] 1 & - 2 &0 &0  0 &1 &0 &0  0 &0 &1 &0  0 &0 &0 &1 [r] 1 &0 &0 &0  0 &1 &0 &0  0 &0 &1 &1  0 &0 &0 &1 = [r] 1 &0 &0 &0  0 &0 &1 &0  0 &0 &0 &0 $ . Finish by swapping columns. $ [r] 1 &0 &0 &0  0 &0 &1 &0  0 &0 &0 &0 [r] 1 &0 &0 &0  0 &0 &1 &0  0 &1 &0 &0  0 &0 &0 &1 = [r] 1 &0 &0 &0  0 &1 &0 &0  0 &0 &0 &0 $ . Finally, combine the left - multipliers together as $ P $ and the right - multipliers together as $ Q $ to get $ PHQ $ . $ [r] 1 & - 1 &0  0 &1 &0  - 2 &0 &1 [r] 1 &2 &1 & - 1  0 &0 &1 & - 1  2 &4 &2 & - 2 [r] 1 &0 & - 2 &0  0 &0 &1 &0  0 &1 &0 &1  0 &0 &0 &1 = [r] 1 &0 &0 &0  0 &1 &0 &0  0 &0 &0 &0 $ . Matrix equivalence classes are characterized by rank: two same - sized matrices are matrix equivalent if and only if they have the same rank. Two same - sized matrices with the same rank are equivalent to the same block partial - identity matrix. The $ nbyn{2} $ matrices have only three possible ranks: zero, one, or two. Thus there are three matrix equivalence classes. Each class consists of all of the $ nbyn{2} $ matrices with the same rank. There is only one rank zero matrix. The other two classes have infinitely many members; we've shown only the canonical representative. One nice thing about the representative in $ X $ is that we can completely understand the linear map when it is expressed in this way: where the bases are $ B=sequence{vec{beta}_1, dots, vec{beta}_n} $ and $ D=sequence{vec{delta}_1, dots, vec{delta}_m} $ then the map's action is $ c_1vec{beta}_1+dots+c_kvec{beta}_k+c_{k+1}vec{beta}_{k+1}+dots +c_nvec{beta}_n ;mapsto; c_1vec{delta}_1+dots+c_kvec{delta}_k+zero+cdots+zero $ . where $ k $ is the rank. Thus we can view any linear map as a projection. $ colvec{c_1  vdots  c_k  c_{k+1}  vdots  c_n}_B quadlongmapstoquad colvec{c_1  vdots  c_k  0  vdots  0}_D $ .
The prior section shows how matrices represent linear maps. We now explore how this representation interacts with things that we already know. First we will see how the representation of a scalar product $ rcdot f $ of a linear map relates to the representation of $ f $ , and also how the representation of a sum $ f+g $ relates to the representations of the two summands. Later we will do the same comparison for the map operations of composition and inverse. Let $ map{f}{V}{W} $ be a linear function represented with respect to some bases by this matrix. $ rep{f}{B, D} = [r] 1 &0  1 &1 $ . Consider the map that is the scalar multiple $ map{5f}{V}{W} $ . We will relate the representation $ rep{5f}{B, D} $ with $ rep{f}{B, D} $ . Let $ f $ associate $ vec{v}mapsto vec{w} $ with these representations. $ rep{vec{v}}{B} =colvec{v_1  v_2} qquad rep{vec{w}}{D} =colvec{w_1  w_2} $ . Where the codomain's basis is $ D=sequence{vec{delta}_1, vec{delta}_2} $ , that representation gives that the output vector is $ vec{w}=w_1vec{delta}_1+w_2vec{delta}_2 $ . The action of the map $ 5f $ is $ vec{v}mapsto 5vec{w} $ and $ 5vec{w}=5cdot(w_1vec{delta}_1+w_2vec{delta}_2) =(5w_1)vec{delta}_1+(5w_2)vec{delta}_2 $ . So $ 5f $ associates the input vector $ vec{v} $ with the output vector having this representation. $ rep{5vec{w}}{D} =colvec{5w_1  5w_2} $ . Changing from the map $ f $ to the map $ 5f $ has the effect on the representation of the output vector of multiplying each entry by $ 5 $ . Because of that, $ rep{5f}{B, D} $ is this matrix. $ rep{5f}{B, D}cdotcolvec{v_1  v_2} = colvec{5v_1  5v_1+5v_2} qquad rep{5f}{B, D} = [r] 5 &0  5 &5 $ . Therefore, going from the matrix representing $ f $ to the one representing $ 5f $ means multiplying all the matrix entries by $ 5 $ . We can do a similar exploration for the sum of two maps. Suppose that two linear maps with the same domain and codomain $ map{f, g}{Re^2}{Re^2} $ are represented with respect to bases $ B $ and  $ D $ by these matrices. $ rep{f}{B, D}= 1 &3  2 &0 qquad rep{g}{B, D}= [r] - 2 & - 1  2 &4 $ . Recall the definition of sum: if $ f $  does $ vec{v}mapstovec{u} $ and $ g $  does $ vec{v}mapstovec{w} $ then $ f+g $ is the function whose action is $ vec{v}mapstovec{u}+vec{w} $ . Let these be the representations of the input and output vectors. $ rep{vec{v}}{B}=colvec{v_1  v_2} qquad rep{vec{u}}{D}=colvec{u_1  u_2} quad rep{vec{w}}{D}=colvec{w_1  w_2} $ . Where $ D=sequence{vec{delta}_1, vec{delta}_2} $ we have $ vec{u}+vec{w}=(u_1vec{delta}_1+u_2vec{delta}_2) +(w_1vec{delta}_1+w_2vec{delta}_2) =(u_1+w_1)vec{delta}_1+(u_2+w_2)vec{delta}_2 $ and so this is the representation of the vector sum. $ rep{vec{u}+vec{w}}{D}=colvec{u_1+w_1  u_2+w_2} $ . Thus, since these represent the actions of of the maps $ f $ and  $ g $ on the input  $ vec{v} $ $ 1 &3  2 &0 colvec{v_1  v_2} =colvec{v_1+3v_2  2v_1} qquad - 2 & - 1  2 &4 colvec{v_1  v_2} =colvec{ - 2v_1 - v_2  2v_1+4v_2} $ . adding the entries represents the action of the map $ f+g $ . $ rep{f+g}{B, D}cdotcolvec{v_1  v_2} = colvec{ - v_1+2v_2  4v_1+4v_2} $ . Therefore, we compute the matrix representing the function sum by adding the entries of the matrices representing the functions. $ rep{f+g}{B, D}= - 1 &2  4 &4 $ . The scalar multiple of a matrix is the result of entry - by - entry scalar multiplication. The sum/ of two same - sized matrices is their entry - by - entry sum. These operations extend the first chapter's operations of addition and scalar multiplication of vectors. We need a result that proves these matrix operations do what the examples suggest that they do. Let $ map{h, g}{V}{W} $ be linear maps represented with respect to bases $ B, D $ by the matrices $ H $ and $ G $ and let $ r $ be a scalar. Then with respect to $ B, D $ the map $ map{rcdot h}{V}{W} $ is represented by $ rH $ and the map $ map{h+g}{V}{W} $ is represented by $ H+G $ . Generalize the examples. This is $ X $ . These two operations on matrices are simple, but we did not define them in this way because they are simple. We defined them this way because they represent function addition and function scalar multiplication. That is, our program is to define matrix operations by referencing function operations. Simplicity is a bonus. We will see this again in the next subsection, where we will define the operation of multiplying matrices. Since we've just defined matrix scalar multiplication and matrix sum to be entry - by - entry operations, a naive thought is to define matrix multiplication to be the entry - by - entry product. In theory we could do whatever we please but we will instead be practical and combine the entries in the way that represents the function operation of composition. A special case of scalar multiplication is multiplication by zero. For any map $ 0cdot h $ is the zero homomorphism and for any matrix $ 0cdot H $ is the matrix with all entries zero. A zero matrix has all entries $ 0 $ . We write $ Z_{nbym{n}{m}} $ or simply $ Z $ (another common notation is $ 0_{nbym{n}{m}} $ or just $ 0 $ ). The zero map from any three - dimensional space to any two - dimensional space is represented by the $ nbym{2}{3} $ zero matrix $ Z=[r] 0 &0 &0  0 &0 &0 $ . no matter what domain and codomain bases we use. After representing addition and scalar multiplication of linear maps in the prior subsection, the natural next operation to consider is function composition. The composition of linear maps is linear. (textit{Note:} this argument has already appeared, as part of the proof of Theorem I..) Let $ map{h}{V}{W} $ and $ map{g}{W}{U} $ be linear. The calculation composed{g}{h}, bigl(c_1cdotvec{v}_1+c_2cdotvec{v}_2bigr) =gbigl(, h(c_1cdot vec{v}_1+c_2cdotvec{v}_2), bigr) =gbigl(, c_1cdot h(vec{v}_1)+c_2cdot h(vec{v}_2), bigr)  =c_1cdot gbigl(h(vec{v}_1))+c_2cdot g(h(vec{v}_2)bigr) =c_1cdot (composed{g}{h})(vec{v}_1) +c_2cdot (composed{g}{h})(vec{v}_2) shows that $ map{composed{g}{h}}{V}{U} $ preserves linear combinations, and so is linear. As we did with the operation of matrix addition and scalar multiplication, we will see how the representation of the composite relates to the representations of the compositors by first considering an example. Let $ map{h}{Re^4}{Re^2} $ and $ map{g}{Re^2}{Re^3} $ , fix bases $ BsubsetRe^4 $ , $ CsubsetRe^2 $ , $ DsubsetRe^3 $ , and let these be the representations. $ H=rep{h}{B, C} =[r] 4 &6 &8 &2  5 &7 &9 &3 _{B, C} quad G=rep{g}{C, D} =[r] 1 &1  0 &1  1 &0 _{C, D} $ . To represent the composition $ map{composed{g}{h}}{Re^4}{Re^3} $ we start with a $ vec{v} $ , represent $ h $ of $ vec{v} $ , and then represent $ g $ of that. The representation of $ h(vec{v}) $ is the product of $ h $ 's matrix and $ vec{v} $ 's vector. $ rep{, h(vec{v}), }{C} = [r] 4 &6 &8 &2  5 &7 &9 &3 _{B, C} colvec{v_1  v_2  v_3  v_4}_B = colvec{4v_1+6v_2+8v_3+2v_4  5v_1+7v_2+9v_3+3v_4}_C $ . The representation of $ g(, h(vec{v}), ) $ is the product of $ g $ 's matrix and $ h(vec{v}) $ 's vector. rep{, g(h(vec{v})), }{D}! =[r] 1 &1  0 &1  1 &0 _{C, D} colvec{4v_1+6v_2+8v_3+2v_4  5v_1+7v_2+9v_3+3v_4}_C  =colvec{1cdot(4v_1+6v_2+8v_3+2v_4)+1cdot(5v_1+7v_2+9v_3+3v_4)  0cdot(4v_1+6v_2+8v_3+2v_4)+1cdot(5v_1+7v_2+9v_3+3v_4)  1cdot(4v_1+6v_2+8v_3+2v_4)+0cdot(5v_1+7v_2+9v_3+3v_4)}_D Distributing and regrouping on the $ v $ 's gives $ = colvec{(1cdot4+1cdot5)v_1+ (1cdot6+1cdot7)v_2+ (1cdot8+1cdot9)v_3+ (1cdot2+1cdot3)v_4  (0cdot4+1cdot5)v_1+ (0cdot6+1cdot7)v_2+ (0cdot8+1cdot9)v_3+ (0cdot2+1cdot3)v_4  (1cdot4+0cdot5)v_1+ (1cdot6+0cdot7)v_2+ (1cdot8+0cdot9)v_3+ (1cdot2+0cdot3)v_4}_D $ . which is this matrix - vector product. $ = 1cdot4+1cdot5 &1cdot6+1cdot7 &1cdot8+1cdot9 &1cdot2+1cdot3  0cdot4+1cdot5 &0cdot6+1cdot7 &0cdot8+1cdot9 &0cdot2+1cdot3  1cdot4+0cdot5 &1cdot6+0cdot7 &1cdot8+0cdot9 &1cdot2+0cdot3 _{B, D} colvec{v_1  v_2  v_3  v_4}_B $ . The matrix representing $ composed{g}{h} $ has the rows of $ G $ combined with the columns of $ H $ . The matrix - multiplicative product of the $ nbym{m}{r} $ matrix  $ G $ and the $ nbym{r}{n} $ matrix  $ H $ is the $ nbym{m}{n} $ matrix  $ P $ , where $ p_{i, j} = g_{i, 1}h_{1, j}+g_{i, 2}h_{2, j}+dots+g_{i, r}h_{r, j} $ . so that the $ i, j $ - th entry of the product is the dot product of the $ i $ - th row of the first matrix with the $ j $ - th column of the second. $ GH= &vdots  g_{i, 1} &g_{i, 2} &cdots &g_{i, r}  &vdots &h_{1, j}  cdots &h_{2, j} &cdots  &vdots  &h_{r, j} = &vdots  cdots &p_{i, j} &cdots  &vdots $ . $ [r] 2 &0  4 &6  8 &2 [r] 1 &3  5 &7 = 2cdot 1+0cdot 5 &2cdot 3+0cdot 7  4cdot 1+6cdot 5 &4cdot 3+6cdot 7  8cdot 1+2cdot 5 &8cdot 3+2cdot 7 = [r] 2 &6  34 &54  18 &38 $ . Some products are not defined, such as the product of a $ nbym{2}{3} $ matrix with a $ nbyn{2} $ , because the number of columns in the first matrix must equal the number of rows in the second. But the product of two $ nbyn{n} $ matrices is always defined. Here are two $ nbyn{2} $ 's. $ [r] 1 &2  3 &4 [r] - 1 &0  2 & - 2 = 1cdot ( - 1)+2cdot 2 &1cdot 0+2cdot ( - 2)  3cdot ( - 1)+4cdot 2 &3cdot 0+4cdot ( - 2) = [r] 3 & - 4  5 & - 8 $ . The matrices from $ X $ combine in this way. [r] 1 &1  0 &1  1 &0 [r] 4 &6 &8 &2  5 &7 &9 &3  &= 1cdot4+1cdot5 &1cdot6+1cdot7 &1cdot8+1cdot9 &1cdot2+1cdot3  0cdot4+1cdot5 &0cdot6+1cdot7 &0cdot8+1cdot9 &0cdot2+1cdot3  1cdot4+0cdot5 &1cdot6+0cdot7 &1cdot8+0cdot9 &1cdot2+0cdot3  &=[r] 9 &13 &17 &5  5 &7 &9 &3  4 &6 &8 &2 A composition of linear maps is represented by the matrix product of the representatives. This argument generalizes $ X $ . Let $ map{h}{V}{W} $ and $ map{g}{W}{X} $ be represented by $ H $ and $ G $ with respect to bases $ Bsubset V $ , $ Csubset W $ , and $ Dsubset X $ , of sizes $ n $ , $ r $ , and $ m $ . For any $ vec{v}in V $ the $ k $ - th component of $ rep{, h(vec{v}), }{C} $ is $ h_{k, 1}v_1+cdots+h_{k, n}v_n $ . and so the $ i $ - th component of $ rep{, composed{g}{h}, (vec{v}), }{D} $ is this. g_{i, 1}cdot(h_{1, 1}v_1+dots+h_{1, n}v_n) +g_{i, 2}cdot(h_{2, 1}v_1+dots+h_{2, n}v_n)  +dots +g_{i, r}cdot(h_{r, 1}v_1+dots+h_{r, n}v_n) Distribute and regroup on the $ v $ 's. =(g_{i, 1} h_{1, 1}+g_{i, 2} h_{2, 1}+dots+g_{i, r}h_{r, 1})cdot v_1  +dots +(g_{i, 1} h_{1, n}+g_{i, 2} h_{2, n} +dots+g_{i, r} h_{r, n})cdot v_n Finish by recognizing that the coefficient of each $ v_j $ $ g_{i, 1}h_{1, j}+g_{i, 2}h_{2, j}+dots+g_{i, r}h_{r, j} $ . matches the definition of the $ i, j $ entry of the product $ GH $ . This arrow diagram pictures the relationship between maps and matrices (`wrt' abbreviates `with respect to'). Above the arrows, the maps show that the two ways of going from $ V $ to $ X $ , straight over via the composition or else in two steps by way of $ W $ , have the same effect $ vec{v}mapsunder{gcirc h}g(h(vec{v})) qquad vec{v}mapsunder{h}h(vec{v})mapsunder{g}g(h(vec{v})) $ . (this is just the definition of composition). Below the arrows, the matrices indicate that multiplying $ GH $ into the column vector $ rep{vec{v}}{B} $ has the same effect as multiplying the column vector first by $ H $ and then multiplying the result by $ G $ . $ rep{composed{g}{h}}{B, D} =GH qquad rep{g}{C, D};rep{h}{B, C}=GH $ . As mentioned in $ X $ , because the number of columns on the left does not equal the number of rows on the right, the product as here of a $ nbym{2}{3} $ matrix with a $ nbyn{2} $ matrix is not defined. $ [r] - 1 &2 &0  0 &10 &(i)1 [r] 0 &0  0 &2 $ . The definition requires that the sizes match because we want that the underlying function composition is possible. $ text{dimension $ n $ space} ;stackrel{h}{longrightarrow}; text{dimension $ r $ space} ;stackrel{g}{longrightarrow}; text{dimension $ m $ space} tag{ $ * $ } $ . Thus, matrix product combines the $ nbym{m}{r} $ matrix  $ G $ with the $ nbym{r}{n} $ matrix  $ F $ to yield the $ nbym{m}{n} $ result  $ GF $ . Briefly: $ nbym{m}{r}text{ times }nbym{r}{n}text{ equals }nbym{m}{n} $ . The order of the dimensions can be confusing. In ` $ nbym{m}{r}text{ times }nbym{r}{n}text{ equals }nbym{m}{n} $ ' the number written first is  $ m $ . But  $ m $ appears last in the map dimension description line ( $ * $ ) above, and the other dimensions also appear in reverse. The explanation is that while $ h $ is done first, followed by $ g $ , we write the composition as $ composed{g}{h} $ , with $ g $ on the left (arising from the notation $ g(h(vec{v})) $ ). That carries over to matrices, so that $ composed{g}{h} $ is represented by $ GH $ . We can get insight into matrix - matrix product operation by studying how the entries combine. For instance, an alternative way to understand why we require above that the sizes match is that the row of the left - hand matrix must have the same number of entries as the column of the right - hand matrix, or else some entry will be left without a matching entry from the other matrix. Another aspect of the combinatorics of matrix multiplication, in the sum defining the $ i, j $ entry, is brought out here by the boxing the equal subscripts. $ setlength{fboxsep}{.15em} p_{i, j} = g_{i, text{highlight{ $ 1 $ }}}h_{text{highlight{ $ 1 $ }}, j} +g_{i, text{highlight{ $ 2 $ }}}h_{text{highlight{ $ 2 $ }}, j} +dots+g_{i, text{highlight{ $ r $ }}}h_{text{highlight{ $ r $ }}, j} $ . The highlighted subscripts on the $ g $ 's are column indices while those on the $ h $ 's are for rows. That is, the summation takes place over the columns of $ G $ but over the rows of $ H $ Dash the definition treats left differently than right. So we may reasonably suspect that $ GH $ can be unequal to $ HG $ . Matrix multiplication is not commutative. $ [r] 1 &2  3 &4 [r] 5 &6  7 &8 = [r] 19 &22  43 &50 qquad [r] 5 &6  7 &8 [r] 1 &2  3 &4 = [r] 23 &34  31 &46 $ . Commutativity can fail more dramatically: $ [r] 5 &6  7 &8 [r] 1 &2 &0  3 &4 &0 = [r] 23 &34 &0  31 &46 &0 $ . while $ [r] 1 &2 &0  3 &4 &0 [r] 5 &6  7 &8 $ . isn't even defined. The fact that matrix multiplication is not commutative can seem odd at first, perhaps because most mathematical operations in prior courses are commutative. But matrix multiplication represents function composition and function composition is not commutative: if $ f(x)=2x $ and $ g(x)=x+1 $ then $ composed{g}{f}(x)=2x+1 $ while $ composed{f}{g}(x)=2(x+1)=2x+2 $ . Except for the lack of commutativity, matrix multiplication is algebraically well - behaved. The next result gives some nice properties and more are in $ X $ and $ X $ . If $ F $ , $ G $ , and $ H $ are matrices, and the matrix products are defined, then the product is associative $ (FG)H=F(GH) $ and distributes over matrix addition $ F(G+H)=FG+FH $ and $ (G+H)F=GF+HF $ . Associativity holds because matrix multiplication represents function composition, which is associative: the maps $ composed{(composed{f}{g})}{h} $ and $ composed{f}{(composed{g}{h})} $ are equal as both send $ vec{v} $ to $ f(g(h(vec{v}))) $ . Distributivity is similar. For instance, the first one goes $ composed{f}{(g+h)}, (vec{v}) =fbigl(, (g+h)(vec{v}), bigr) =fbigl(, g(vec{v})+h(vec{v}), bigr) =f(g(vec{v}))+f(h(vec{v})) =composed{f}{g}(vec{v})+composed{f}{h}(vec{v}) $ (the third equality uses the linearity of $ f $ ). Right - distributivity goes the same way. We could instead prove that result by slogging through indices. For example, for associativity the $ i, j $ entry of $ (FG)H $ is $ MoveEqLeft (f_{i, 1}g_{1, 1}+f_{i, 2}g_{2, 1}+dots+f_{i, r}g_{r, 1})h_{1, j}  &+(f_{i, 1}g_{1, 2}+f_{i, 2}g_{2, 2}+dots+f_{i, r}g_{r, 2})h_{2, j}  &vdotswithin{+}  &+(f_{i, 1}g_{1, s}+f_{i, 2}g_{2, s}+dots+f_{i, r}g_{r, s})h_{s, j} $ . where $ F $ , $ G $ , and $ H $ are $ nbym{m}{r} $ , $ nbym{r}{s} $ , and $ nbym{s}{n} $ matrices. Distribute $ MoveEqLeft f_{i, 1}g_{1, 1}h_{1, j}+f_{i, 2}g_{2, 1}h_{1, j}+ dots+f_{i, r}g_{r, 1}h_{1, j}  &+f_{i, 1}g_{1, 2}h_{2, j}+f_{i, 2}g_{2, 2}h_{2, j}+ dots+f_{i, r}g_{r, 2}h_{2, j}  &vdotswithin{+}  &+f_{i, 1}g_{1, s}h_{s, j}+f_{i, 2}g_{2, s}h_{s, j}+dots +f_{i, r}g_{r, s}h_{s, j} $ . and regroup around the $ f $ 's $ MoveEqLeft f_{i, 1}(g_{1, 1}h_{1, j}+g_{1, 2}h_{2, j}+dots+g_{1, s}h_{s, j})  &+f_{i, 2}(g_{2, 1}h_{1, j}+g_{2, 2}h_{2, j}+dots+g_{2, s}h_{s, j})  &vdotswithin{+}  &+f_{i, r}(g_{r, 1}h_{1, j}+g_{r, 2}h_{2, j}+dots+g_{r, s}h_{s, j}) $ . to get the $ i, j $ entry of $ F(GH) $ . Contrast the two proofs. The index - heavy argument is hard to understand in that while the calculations are easy to check, the arithmetic seems unconnected to any idea. The argument in the proof is shorter and also says why this property ``really'' holds. This illustrates the comments made at the start of the chapter on vector spacesDash at least sometimes an argument from higher - level constructs is clearer. We have now seen how to represent the composition of linear maps. The next subsection will continue to explore this operation. We can consider matrix multiplication as a mechanical process, putting aside for the moment any implications about the underlying maps. The striking thing about this operation is the way that rows and columns combine. The $ i, j $ entry of the matrix product is the dot product of row  $ i $ of the left matrix with column  $ j $ of the right one. For instance, here a second row and a third column combine to make a $ 2, 3 $  entry. $ setlength{fboxsep}{(i)5pt} {@{}cc@{}} 1 &1  text{highlight{ $ {@{}cc@{}} 0 &1 $ }}  {@{}cc@{}} 1 &0 {@{}c@{}} 4  5 &{@{}c@{}} 6  7 &text{highlight{ $ {@{}c@{}} 8  9 $ }} &{@{}c@{}} 2  3 = [r] 9 &13 &17 &5  5 &7 &text{highlight{ $ 9 $ }} &3  4 &6 &8 &2 $ . We can view this as the left matrix acting by multiplying its rows into the columns of the right matrix. Or, it is the right matrix using its columns to act on the rows of the left matrix. Below, we will examine actions from the left and from the right for some simple matrices. Simplest is the zero matrix. Multiplying by a zero matrix from the left or from the right results in a zero matrix. $ [r] 0 &0  0 &0 [r] 1 &3 &2  - 1 &1 & - 1 = [r] 0 &0 &0  0 &0 &0 qquad [r] 2 &3  1 &4 [r] 0 &0  0 &0 = [r] 0 &0  0 &0 $ . The next easiest matrices are the ones with a single nonzero entry. A matrix with all $ 0 $ 's except for a $ 1 $ in the $ i, j $ entry is an $ i, j $ unit matrix (or matrix unit). This is the $ 1, 2, $ unit matrix with three rows and two columns, multiplying from the left. $ [r] 0 &1  0 &0  0 &0 [r] 5 &6  7 &8 = [r] 7 &8  0 &0  0 &0 $ . Acting from the left, an $ i, j $ unit matrix copies row  $ j $ of the multiplicand into row  $ i $ of the result. From the right an $ i, j $ unit matrix picks out column  $ i $ of the multiplicand and copies it into column  $ j $ of the result. $ [r] 1 &2 &3  4 &5 &6  7 &8 &9 [r] 0 &1  0 &0  0 &0 = [r] 0 &1  0 &4  0 &7 $ . Rescaling unit matrices simply rescales the result. This is the action from the left of the matrix that is twice the one in the prior example. $ [r] 0 &2  0 &0  0 &0 [r] 5 &6  7 &8 = [r] 14 &16  0 &0  0 &0 $ . Next in complication are matrices with two nonzero entries. There are two cases. If a left - multiplier has entries in different rows then their actions don't interact. $ [r] 1 &0 &0  0 &0 &2  0 &0 &0 [r] 1 &2 &3  4 &5 &6  7 &8 &9 &=( [r] 1 &0 &0  0 &0 &0  0 &0 &0 + [r] 0 &0 &0  0 &0 &2  0 &0 &0 ) [r] 1 &2 &3  4 &5 &6  7 &8 &9  &=[r] 1 &2 &3  0 &0 &0  0 &0 &0 + [r] 0 &0 &0  14 &16 &18  0 &0 &0  &=[r] 1 &2 &3  14 &16 &18  0 &0 &0 $ . But if the left - multiplier's nonzero entries are in the same row then that row of the result is a combination. $ [r] 1 &0 &2  0 &0 &0  0 &0 &0 [r] 1 &2 &3  4 &5 &6  7 &8 &9 &=( [r] 1 &0 &0  0 &0 &0  0 &0 &0 + [r] 0 &0 &2  0 &0 &0  0 &0 &0 ) [r] 1 &2 &3  4 &5 &6  7 &8 &9  &=[r] 1 &2 &3  0 &0 &0  0 &0 &0 + [r] 14 &16 &18  0 &0 &0  0 &0 &0  &=[r] 15 &18 &21  0 &0 &0  0 &0 &0 $ . parnoindent Right - multiplication acts in the same way, but with columns. In a product of two matrices $ G $ and $ H $ , the columns of $ GH $ are formed by taking $ G $ times the columns of $ H $ $ Gcdot {c@{hspace{1em}}c@{hspace{1em}}c} vdots & &vdots  vec{h}_1 &cdots &vec{h}_n  vdots & &vdots ={c@{hspace{1em}}c@{hspace{1em}}c} vdots & &vdots  Gcdot vec{h}_1 &cdots &Gcdotvec{h}_n  vdots & &vdots $ . and the rows of $ GH $ are formed by taking the rows of $ G $ times $ H $ $ {c} cdots; vec{g}_1 ;cdots rule[ - (i)25ex]{0pt}{2ex}  vdots  $ .5ex] rule{0pt}{(ii)5ex}cdots; vec{g}_r ;cdots cdot H ={c} cdots; vec{g}_1cdot H ;cdotsrule[ - (i)25ex]{0pt}{2ex}  vdotsrule[ - 1ex]{0pt}{2ex}  $ .5ex] rule{0pt}{(ii)5ex}cdots; vec{g}_rcdot H ;cdots $ . We will check that in a product of $ nbyn{2} $ matrices, the rows of the product equal the product of the rows of  $ G $ with the entire matrix  $ H $ ; other cases worh the same way. $ g_{1, 1} &g_{1, 2}  g_{2, 1} &g_{2, 2} h_{1, 1} &h_{1, 2}  h_{2, 1} &h_{2, 2} &= rowvec{g_{1, 1} &g_{1, 2}}H  rowvec{g_{2, 1} &g_{2, 2}}H  &= rowvec{g_{1, 1}h_{1, 1}+g_{1, 2}h_{2, 1} &g_{1, 1}h_{1, 2}+g_{1, 2}h_{2, 2}}  rowvec{g_{2, 1}h_{1, 1}+g_{2, 2}h_{2, 1} &g_{2, 1}h_{1, 2}+g_{2, 2}h_{2, 2}} $ . (We ignore the extra parentheses.) Consider the columns of the product of two $ nbyn{2} $  matrices. $ g_{1, 1} &g_{1, 2}  g_{2, 1} &g_{2, 2} h_{1, 1} &h_{1, 2}  h_{2, 1} &h_{2, 2} = g_{1, 1}h_{1, 1}+g_{1, 2}h_{2, 1} &g_{1, 1}h_{1, 2}+g_{1, 2}h_{2, 2}  g_{2, 1}h_{1, 1}+g_{2, 2}h_{2, 1} &g_{2, 1}h_{1, 2}+g_{2, 2}h_{2, 2} $ . Each column is the result of multiplying $ G $ by the corresponding column of $ H $ . $ Gcolvec{h_{1, 1}  h_{2, 1}} = colvec{g_{1, 1}h_{1, 1}+g_{1, 2}h_{2, 1}  g_{2, 1}h_{1, 1}+g_{2, 2}h_{2, 1}} quad Gcolvec{h_{1, 2}  h_{2, 2}} =colvec{g_{1, 1}h_{1, 2}+g_{1, 2}h_{2, 2}  g_{2, 1}h_{1, 2}+g_{2, 2}h_{2, 2}} $ . An application of those observations is that there is a matrix that just copies out the rows and columns. The main diagonal (or principal diagonal or simply diagonal) of a square matrix goes from the upper left to the lower right. An identity matrix/ is square and every entry is $ 0 $ except for $ 1 $ 's in the main diagonal. $ I_{nbyn{n}}= 1 &0 &ldots &0  0 &1 &ldots &0  &vdots  0 &0 &ldots &1 $ . Here is the $ nbyn{2} $ identity matrix leaving its multiplicand unchanged when it acts from the right. $ [r] 1 & - 2  0 & - 2  1 & - 1  4 &3 [r] 1 &0  0 &1  = [r] 1 & - 2  0 & - 2  1 & - 1  4 &3 $ . Here the $ nbyn{3} $ identity leaves its multiplicand unchanged both from the left $ [r] 1 &0 &0  0 &1 &0  0 &0 &1 [r] 2 &3 &6  1 &3 &8  - 7 &1 &0 = [r] 2 &3 &6  1 &3 &8  - 7 &1 &0 $ . and from the right. $ [r] 2 &3 &6  1 &3 &8  - 7 &1 &0 [r] 1 &0 &0  0 &1 &0  0 &0 &1 = [r] 2 &3 &6  1 &3 &8  - 7 &1 &0 $ . noindent In short, an identity matrix is the identity element of the set of $ nbyn{n} $ matrices with respect to the operation of matrix multiplication. We can generalize the identity matrix by relaxing the ones to arbitrary reals. The resulting matrix rescales whole rows or columns. A diagonal matrix/ is square and has $ 0 $ 's off the main diagonal. $ a_{1, 1} &0 &ldots &0  0 &a_{2, 2} &ldots &0  &vdots  0 &0 &ldots &a_{n, n} $ . From the left, the action of multiplication by a diagonal matrix is to rescales the rows. $ [r] 2 &0  0 & - 1 [r] 2 &1 &4 & - 1  - 1 &3 &4 &4 = [r] 4 &2 &8 & - 2  1 & - 3 & - 4 & - 4 $ . From the right such a matrix rescales the columns. $ [r] 1 &2 &1  2 &2 &2 [r] 3 &0 &0  0 &2 &0  0 &0 & - 2 = [r] 3 &4 & - 2  6 &4 & - 4 $ . We can also generalize identity matrices by putting a single one in each row and column in ways other than putting them down the diagonal. A permutation matrix is square and is all $ 0 $ 's except for a single  $ 1 $ in each row and column. From the left these matrices permute rows. $ [r] 0 &0 &1  1 &0 &0  0 &1 &0 [r] 1 &2 &3  4 &5 &6  7 &8 &9 = [r] 7 &8 &9  1 &2 &3  4 &5 &6 $ . From the right they permute columns. $ [r] 1 &2 &3  4 &5 &6  7 &8 &9 [r] 0 &0 &1  1 &0 &0  0 &1 &0 = [r] 2 &3 &1  5 &6 &4  8 &9 &7 $ . We finish this subsection by applying these observations to get matrices that perform Gauss's Method and Gauss - Jordan reduction. We have already seen how to produce a matrix that rescales rows, and a row swapper. Multiplying by this matrix rescales the second row by three. $ [r] 1 &0 &0  0 &3 &0  0 &0 &1 [r] 0 &2 &1 &1  0 &1/3 &1 & - 1  1 &0 &2 &0 = [r] 0 &2 &1 &1  0 &1 &3 & - 3  1 &0 &2 &0 $ . This multiplication swaps the first and third rows. $ [r] 0 &0 &1  0 &1 &0  1 &0 &0 [r] 0 &2 &1 &1  0 &1 &3 & - 3  1 &0 &2 &0 = [r] 1 &0 &2 &0  0 &1 &3 & - 3  0 &2 &1 &1 $ . To see how to perform a row combination, we observe something about those two examples. The matrix that rescales the second row by a factor of three arises in this way from the identity. $ [r] 1 &0 &0  0 &1 &0  0 &0 &1 grstep{3rho_2} [r] 1 &0 &0  0 &3 &0  0 &0 &1 $ . Similarly, the matrix that swaps first and third rows arises in this way. $ [r] 1 &0 &0  0 &1 &0  0 &0 &1 grstep{rho_1leftrightarrowrho_3} [r] 0 &0 &1  0 &1 &0  1 &0 &0 $ . The $ nbyn{3} $ matrix that arises as $ [r] 1 &0 &0  0 &1 &0  0 &0 &1 grstep{ - 2rho_2+rho_3} [r] 1 &0 &0  0 &1 &0  0 & - 2 &1 $ . will, when it acts from the left, perform the combination operation $ - 2rho_2+rho_3 $ . $ [r] 1 &0 &0  0 &1 &0  0 & - 2 &1 [r] 1 &0 &2 &0  0 &1 &3 & - 3  0 &2 &1 &1 = [r] 1 &0 &2 &0  0 &1 &3 & - 3  0 &0 & - 5 &7 $ . The elementary reduction matrices (or just elementary matrices) result from applying a single Gaussian operation to an identity matrix. $ Igrstep{krho_i}M_i(k) $ for $ kneq 0 $ $ Igrstep{rho_ileftrightarrowrho_j}P_{i, j} $ for $ ineq j $ $ Igrstep{krho_i+rho_j}C_{i, j}(k) $ for $ ineq j $ Matrix multiplication can do Gaussian reduction. If $ Hgrstep{krho_i}G $ then $ M_i(k)H=G $ . If $ Hgrstep{rho_ileftrightarrowrho_j}G $ then $ P_{i, j}H=G $ . If $ Hgrstep{krho_i+rho_j}G $ then $ C_{i, j}(k)H=G $ . Clear. This is the first system, from the first chapter, on which we performed Gauss's Method. $ {3} & & & &3x_3 &= &9  x_1 &+ &5x_2 & - &2x_3 &= &2  (1/3)x_1 &+ &2x_2 & & &= &3 $ . We can reduce it with matrix multiplication. Swap the first and third rows, $ [r] 0 &0 &1  0 &1 &0  1 &0 &0 [r]{3} 0 &0 &3 &9  1 &5 & - 2 &2  1/3 &2 &0 &3 = [r]{3} 1/3 &2 &0 &3  1 &5 & - 2 &2  0 &0 &3 &9 $ . triple the first row, $ [r] 3 &0 &0  0 &1 &0  0 &0 &1 [r]{3} 1/3 &2 &0 &3  1 &5 & - 2 &2  0 &0 &3 &9 = [r]{3} 1 &6 &0 &9  1 &5 & - 2 &2  0 &0 &3 &9 $ . and then add $ - 1 $ times the first row to the second. $ [r] 1 &0 &0  - 1 &1 &0  0 &0 &1 [r]{3} 1 &6 &0 &9  1 &5 & - 2 &2  0 &0 &3 &9 = [r]{3} 1 &6 &0 &9  0 & - 1 & - 2 & - 7  0 &0 &3 &9 $ . Now back substitution will give the solution. Gauss - Jordan reduction works the same way. For the matrix ending the prior example, first turn the leading entries to ones, $ [r] 1 &0 &0  0 & - 1 &0  0 &0 &1/3 [r]{3} 1 &6 &0 &9  0 & - 1 & - 2 & - 7  0 &0 &3 &9 = [r]{3} 1 &6 &0 &9  0 &1 &2 &7  0 &0 &1 &3 $ . then clear the third column, and then the second column. $ [r] 1 & - 6 &0  0 &1 &0  0 &0 &1 [r] 1 &0 &0  0 &1 & - 2  0 &0 &1 [r]{3} 1 &6 &0 &9  0 &1 &2 &7  0 &0 &1 &3 = [r]{3} 1 &0 &0 &3  0 &1 &0 &1  0 &0 &1 &3 $ . For any matrix $ H $ there are elementary reduction matrices $ R_1 $ , ldots, $ R_r $ such that $ R_rcdot R_{r - 1}cdots R_1cdot H $ is in reduced echelon form. Until now we have taken the point of view that our primary objects of study are vector spaces and the maps between them, and we seemed to have adopted matrices only for computational convenience. This subsection shows that this isn't the entire story. Understanding matrix operations by understanding the mechanics of how the entries combine is also useful. In the rest of this book we shall continue to focus on maps as the primary objects but we will be pragmaticDash if the matrix point of view gives some clearer idea then we will go with it. We finish this section by considering how to represent the inverse of a linear map. We first recall some things about inverses. appendrefs{function inverses}spacefactor=1000 Where $ map{pi}{Re^3}{Re^2} $ is the projection map and $ map{iota}{Re^2}{Re^3} $ is the embedding $ colvec{x  y  z} mapsunder{pi} colvec{x  y} qquad colvec{x  y} mapsunder{iota} colvec{x  y  0} $ . then the composition $ composed{pi}{iota} $ is the identity map $ composed{pi}{iota}=identity $ on $ Re^2 $ . $ colvec{x  y} mapsunder{iota} colvec{x  y  0} mapsunder{pi} colvec{x  y} $ . We say that $ iota $ is a right inverse of $ pi $ or, what is the same thing, that $ pi $ is a left inverse of $ iota $ . However, composition in the other order $ composed{iota}{pi} $ doesn't give the identity mapDash here is a vector that is not sent to itself under $ composed{iota}{pi} $ . $ colvec[r]{0  0  1} mapsunder{pi} colvec[r]{0  0} mapsunder{iota} colvec[r]{0  0  0} $ . In fact, $ pi $ has no left inverse at all. For, if $ f $ were to be a left inverse of $ pi $ then we would have $ colvec{x  y  z} mapsunder{pi} colvec{x  y} mapsunder{f} colvec{x  y  z} $ . for all of the infinitely many $ z $ 's. But a function  $ f $ cannot send a single argument $ binom{x}{y} $ to more than one value. So a function can have a right inverse but no left inverse, or a left inverse but no right inverse. A function can also fail to have an inverse on either side; one example is the zero transformation on $ Re^2 $ . Some functions have a two - sided inverse, another function that is the inverse both from the left and from the right. For instance, the transformation given by $ vec{v}mapsto 2cdot vec{v} $ has the two - sided inverse $ vec{v}mapsto (1/2)cdotvec{v} $ . The appendix shows that a function has a two - sided inverse if and only if it is both one - to - one and onto. The appendix also shows that if a function $ f $ has a two - sided inverse then it is unique, so we call it `the' inverse and write $ f^{ - 1} $ . In addition, recall that we have shown in Theorem II. that if a linear map has a two - sided inverse then that inverse is also linear. Thus, our goal in this subsection is, where a linear $ h $ has an inverse, to find the relationship between $ rep{h}{B, D} $ and $ rep{h^{ - 1}}{D, B} $ . A matrix $ G $ is a left inverse matrix of the matrix $ H $ if $ GH $ is the identity matrix. It is a right inverse if $ HG $ is the identity. A matrix $ H $ with a two - sided inverse is an invertible matrix. That two - sided inverse is denoted $ H^{ - 1} $ . Because of the correspondence between linear maps and matrices, statements about map inverses translate into statements about matrix inverses. If a matrix has both a left inverse and a right inverse then the two are equal. A matrix is invertible if and only if it is nonsingular. textit{(For both results.)} Given a matrix $ H $ , fix spaces of appropriate dimension for the domain and codomain and fix bases for these spaces. With respect to these bases, $ H $ represents a map $ h $ . The statements are true about the map and therefore they are true about the matrix. A product of invertible matrices is invertible: if $ G $ and $ H $ are invertible and $ GH $ is defined then $ GH $ is invertible and $ (GH)^{ - 1}=H^{ - 1}G^{ - 1} $ . Because the two matrices are invertible they are square, and because their product is defined they must both be $ nbyn{n} $ . Fix spaces and basesDash say, $ Re^n $ with the standard basesDash to get maps $ map{g, h}{Re^n}{Re^n} $ that are associated with the matrices, $ G=rep{g}{stdbasis_n, stdbasis_n} $ and $ H=rep{h}{stdbasis_n, stdbasis_n} $ . Consider $ h^{ - 1}g^{ - 1} $ . By the prior paragraph this composition is defined. This map is a two - sided inverse of $ gh $ since $ (h^{ - 1}g^{ - 1})(gh) = h^{ - 1}(identity)h =h^{ - 1}h =identity $ and $ (gh)(h^{ - 1}g^{ - 1}) = g(identity)g^{ - 1} =gg^{ - 1} =identity $ . The matrices representing the maps reflect this equality. This is the arrow diagram giving the relationship between map inverses and matrix inverses. It is a special case of the diagram relating function composition to matrix multiplication. smallskip Beyond its place in our program of seeing how to represent map operations, another reason for our interest in inverses comes from linear systems. A linear system is equivalent to a matrix equation, as here. $ {2} x_1 &+ &x_2 &= &3  2x_1 & - &x_2 &= &2 quadLongleftrightarrowquad [r] 1 &1  2 & - 1 colvec{x_1  x_2} = colvec[r]{3  2} $ . By fixing spaces and bases (for instance, $ Re^2, Re^2 $ with the standard bases), we take the matrix $ H $ to represent a map $ h $ . The matrix equation then becomes this linear map equation. $ h(vec{x})=vec{d} $ . If we had a left inverse map  $ g $ then we could apply it to both sides $ composed{g}{h}(vec{x})=g(vec{d}) $ to get $ vec{x}=g(vec{d}) $ . Restating in terms of the matrices, we want to multiply by the inverse matrix $ rep{g}{C, B}cdotrep{vec{d}}{C} $ to get $ rep{vec{x}}{B} $ . We can find a left inverse for the matrix just given $ m &n  p &q [r] 1 &1  2 & - 1 = [r] 1 &0  0 &1 $ . by using Gauss's Method to solve the resulting linear system. $ {4} m &+ &2n & & & & &= &1  m & - &n & & & & &= &0  & & & &p &+ &2q &= &0  & & & &p & - &q &= &1 $ . Answer: $ m=1/3 $ , $ n=1/3 $ , $ p=2/3 $ , and $ q= - 1/3 $ . (This matrix is actually the two - sided inverse of $ H $ ; the check is easy.) With it, we can solve the system from the prior example. $ colvec{x  y} =[r] 1/3 &1/3  2/3 & - 1/3 colvec[r]{3  2} =colvec[r]{5/3  4/3} $ . Why solve systems with inverse matrices when we have Gauss's Method? Beyond the conceptual appeal of representing the map inverse operation, solving linear systems this way has two advantages. First, once we have done the work of finding an inverse then solving a system with the same coefficients but different constants is fast: if we change the constants on the right of the system above then we get a related problem $ [r] 1 &1  2 & - 1 colvec{x  y} = colvec[r]{5  1} $ . that our inverse method solves quickly. $ colvec{x  y} = [r] 1/3 &1/3  2/3 & - 1/3 colvec[r]{5  1} = colvec[r]{2  3} $ . Another advantage of inverses is that we can explore a system's sensitivity to changes in the constants. For example, tweaking the $ 3 $ on the right of the prior example's system to $ [r] 1 &1  2 & - 1 colvec{x_1  x_2} = colvec[l]{(iii)01  2} $ . and solving with the inverse $ [r] 1/3 &1/3  2/3 & - 1/3 colvec[l]{(iii)01  2} = colvec{(1/3)((iii)01)+(1/3)(2)  (2/3)((iii)01) - (1/3)(2)} $ . shows that the first component of the solution changes by $ 1/3 $ of the tweak, while the second component moves by $ 2/3 $ of the tweak. This is sensitivity analysis. We could use it to decide how accurately we must specify the data in a linear model to ensure that the solution has a desired accuracy. A matrix $ H $ is invertible if and only if it can be written as the product of elementary reduction matrices. We can compute the inverse by applying to the identity matrix the same row steps, in the same order, that Gauss - Jordan reduce $ H $ . The matrix $ H $ is invertible if and only if it is nonsingular and thus Gauss - Jordan reduces to the identity. By $ X $ we can do this reduction with elementary matrices. $ R_rcdot R_{r - 1}dots R_1cdot H=I tag{ $ * $ } $ . For the first sentence of the result, note that elementary matrices are invertible because elementary row operations are reversible, and that their inverses are also elementary. Apply $ R_r^{ - 1} $ from the left to both sides of ( $ * $ ). Then apply $ R_{r - 1}^{ - 1} $ , etc. The result gives $ H $ as the product of elementary matrices $ H=R_1^{ - 1}cdots R_r^{ - 1}cdot I $ . (The $ I $ there covers the case $ r=0 $ .) For the second sentence, group ( $ * $ ) as $ (R_rcdot R_{r - 1}dots R_1)cdot H=I $ and recognize what's in the parentheses as the inverse $ H^{ - 1}=R_rcdot R_{r - 1}dots R_1cdot I $ . Restated: applying $ R_1 $ to the identity, followed by $ R_2 $ , etc., yields the inverse of $ H $ . To find the inverse of $ [r] 1 &1  2 & - 1 $ . do Gauss - Jordan reduction, meanwhile performing the same operations on the identity. For clerical convenience we write the matrix and the identity side - by - side and do the reduction steps together. $ {rr|rr} 1 &1 &1 &0  2 & - 1 &0 &1 &grstep{ - 2rho_1+rho_2} {rr|rr} 1 &1 &1 &0  0 & - 3 & - 2 &1  &grstep{ - 1/3rho_2} {rr|rr} 1 &1 &1 &0  0 &1 &2/3 & - 1/3  &grstep{ - rho_2+rho_1} {rr|rr} 1 &0 &1/3 &1/3  0 &1 &2/3 & - 1/3 $ . This calculation has found the inverse. $ [r] 1 &1  2 & - 1 ^{ - 1} = [r] 1/3 &1/3  2/3 & - 1/3 $ . This one happens to start with a row swap. $ {rrr|rrr} 0 &3 & - 1 &1 &0 &0  1 &0 &1 &0 &1 &0  1 & - 1 &0 &0 &0 &1 &grstep{rho_1leftrightarrowrho_2} {rrr|rrr} 1 &0 &1 &0 &1 &0  0 &3 & - 1 &1 &0 &0  1 & - 1 &0 &0 &0 &1  &grstep{ - rho_1+rho_3} {rrr|rrr} 1 &0 &1 &0 &1 &0  0 &3 & - 1 &1 &0 &0  0 & - 1 & - 1 &0 & - 1 &1  &quadvdotswithin{longrightarrow}  &;grstep{} {rrr|rrr} 1 &0 &0 &1/4 &1/4 &3/4  0 &1 &0 &1/4 &1/4 & - 1/4  0 &0 &1 & - 1/4 &3/4 & - 3/4 $ . This algorithm detects a non - invertible matrix when the left half won't reduce to the identity. $ {rr|rr} 1 &1 &1 &0  2 &2 &0 &1 grstep{ - 2rho_1+rho_2} {rr|rr} 1 &1 &1 &0  0 &0 & - 2 &1 $ . With this procedure we can give a formula for the inverse of a general $ nbyn{2} $ matrix, which is worth memorizing. The inverse for a $ nbyn{2} $ matrix exists and equals $ a &b  c &d ^{ - 1} = frac{1}{ad - bc} d & - b  - c &a $ . if and only if $ ad - bcneq 0 $ . This computation is $ X $ . We have seen in this subsection, as in the subsection on Mechanics of Matrix Multiplication, how to exploit the correspondence between linear maps and matrices. We can fruitfully study both maps and matrices, translating back and forth to use whichever is handiest. Over the course of this entire section we have developed an algebra system for matrices. We can compare it with the familiar algebra of real numbers. Matrix addition and subtraction work in much the same way as the real number operations except that they only combine same - sized matrices. Scalar multiplication is in some ways an extension of real number multiplication. We also have a matrix multiplication operation and its inverse that are somewhat like the familiar real number operations (associativity, and distributivity over addition, for example), but there are differences (failure of commutativity). This section provides an example that algebra systems other than the usual real number one can be interesting and useful.
In the first section we had to do a bit of work to show that there are only three types of solution setsDash singleton, empty, and infinite. But this is easy to see geometrically in the case of systems with two equations and two unknowns. Draw each two - unknowns equation as a line in the plane and then the two lines could have a unique intersection, be parallel, or be the same line. These pictures aren't a short way to prove the results from the prior section, because those results apply to linear systems with any number of variables. But they do provide a visual insight, another way of seeing those results. This section develops what we need to express our results geometrically. In particular, while the two - dimensional case is familiar, to extend to systems with more than two unknowns we shall need some higher - dimensional geometry. ``Higher - dimensional geometry'' sounds exotic. It is exoticDash interesting and eye - opening. But it isn't distant or unreachable. We begin by defining one - dimensional space to be $ Re $ . To see that the definition is reasonable, picture a one - dimensional space and pick a point to label $ 0 $ and another to label  $ 1 $ . Now, with a scale and a direction, we have a correspondence with $ Re $ . For instance, to find the point matching $ +(ii)17 $ , start at $ 0 $ and head in the direction of $ 1 $ , and go $ (ii)17 $ times as far. The basic idea here, combining magnitude with direction, is the key to extending to higher dimensions. An object in an  $ Re^n $ that is comprised of a magnitude and a direction is a vector/ (we use the same word as in the prior section because we shall show below how to describe such an object with a column vector). We can draw a vector as having some length and pointing in some direction. There is a subtlety involved in the definition of a vector as consisting of a magnitude and a directionDash these are equal, even though they start in different places They are equal because they have equal lengths and equal directions. Again: those vectors are not just alike, they are equal. How can things that are in different places be equal? Think of a vector as representing a displacement (the word `vector' is Latin for ``carrier'' or ``traveler''). These two squares undergo displacements that are equal despite that they start in different places. When we want to emphasize this property vectors have of not being anchored we refer to them as free vectors. Thus, these free vectors are equal, as each is a displacement of one over and two up. More generally, vectors in the plane are the same if and only if they have the same change in first components and the same change in second components: the vector extending from $ (a_1, a_2) $ to $ (b_1, b_2) $ equals the vector from $ (c_1, c_2) $ to $ (d_1, d_2) $ if and only if $ b_1 - a_1=d_1 - c_1 $ and $ b_2 - a_2=d_2 - c_2 $ . Saying `the vector that, were it to start at $ (a_1, a_2) $ , would extend to $ (b_1, b_2) $ ' would be unwieldy. We instead describe that vector as $ colvec{b_1 - a_1  b_2 - a_2} $ . so that we represent the `one over and two up' arrows shown above in this way. $ colvec[r]{1  2} $ . We often draw the arrow as starting at the origin, and we then say it is in the canonical position (or natural position or standard position). When $ vec{v}=colvec{v_1  v_2} $ . is in canonical position then it extends from the origin to the endpoint $ (v_1, v_2) $ . We will typically say ``the point $ colvec[r]{1  2}text{''} $ . rather than ``the endpoint of the canonical position of'' that vector. Thus, we will call each of these $ Re^2 $ . $ set{(x_1, x_2)suchthat x_1, x_2inRe} qquad set{colvec{x_1  x_2}suchthat x_1, x_2inRe} $ . In the prior section we defined vectors and vector operations with an algebraic motivation; $ rcdotcolvec{v_1  v_2} = colvec{rv_1  rv_2} qquad colvec{v_1  v_2} + colvec{w_1  w_2} = colvec{v_1+w_1  v_2+w_2} $ . we can now understand those operations geometrically. For instance, if $ vec{v} $ represents a displacement then $ 3vec{v}, $ represents a displacement in the same direction but three times as far and $ - 1vec{v}, $ represents a displacement of the same distance as $ vec{v}, $ but in the opposite direction. And, where $ vec{v} $ and $ vec{w} $ represent displacements, $ vec{v}+vec{w}/ $ represents those displacements combined. The long arrow is the combined displacement in this sense: imagine that you are walking on a ship's deck. Suppose that in one minute the ship's motion gives it a displacement relative to the sea of $ vec{v} $ , and in the same minute your walking gives you a displacement relative to the ship's deck of $ vec{w} $ . Then $ vec{v}+vec{w}/ $ is your displacement relative to the sea. Another way to understand the vector sum is with the parallelogram rule. Draw the parallelogram formed by the vectors $ vec{v} $ and $ vec{w} $ . Then the sum $ vec{v}+vec{w} $ extends along the diagonal to the far corner. The above drawings show how vectors and vector operations behave in $ Re^2 $ . We can extend to $ Re^3 $ , or to even higher - dimensional spaces where we have no pictures, with the obvious generalization: the free vector that, if it starts at $ (a_1, ldots, a_n) $ , ends at $ (b_1, ldots, b_n) $ , is represented by this column. $ colvec{b_1 - a_1  vdotswithin{b_1 - a_1}  b_n - a_n} $ . Vectors are equal if they have the same representation. We aren't too careful about distinguishing between a point and the vector whose canonical representation ends at that point. $ Re^n= set{colvec{v_1  vdotswithin{v_1}  v_n}suchthat v_1, ldots, v_ninRe} $ . And, we do addition and scalar multiplication component - wise. Having considered points, we next turn to lines. In $ Re^2 $ , the line through $ (1, 2) $ and $ (3, 1) $ is comprised of (the endpoints of) the vectors in this set. In the description the vector that is associated with the parameter  $ t $ $ colvec[r]{2  - 1}=colvec[r]{3  1} - colvec[r]{1  2} $ . is the one shown in the picture as having its whole body in the lineDash it is a direction vector for the line. Note that points on the line to the left of $ x=1 $ are described using negative values of $ t $ . In $ Re^3 $ , the line through $ (1, 2, 1) $ and $ (0, 3, 2) $ is the set of (endpoints of) vectors of this form and lines in even higher - dimensional spaces work in the same way. In $ Re^3 $ , a line uses one parameter so that a particle on that line would be free to move back and forth in one dimension. A plane involves two parameters. For example, the plane through the points $ (1, 0, 5) $ , $ (2, 1, - 3) $ , and $ ( - 2, 4, 0.5) $ consists of (endpoints of) the vectors in this set. $ set{ colvec[r]{1  0  5} +tcolvec[r]{1  1  - 8} +scolvec[r]{ - 3  4  - (iv)5} suchthat t, sinRe } $ . The column vectors associated with the parameters come from these calculations. $ colvec[r]{1  1  - 8} = colvec[r]{2  1  - 3} - colvec[r]{1  0  5} qquad colvec[r]{ - 3  4  - (iv)5} = colvec[r]{ - 2  4  0.5} - colvec[r]{1  0  5} $ . As with the line, note that we describe some points in this plane with negative $ t $ 's or negative $ s $ 's or both. Calculus books often describe a plane by using a single linear equation. $ vcenteredhbox{includegraphics{gr/mp/ch(i)18}} $ . To translate from this to the vector description, think of this as a one - equation linear system and parametrize: $ x=2 - y/2 - z/2 $ . $ vcenteredhbox{includegraphics{gr/mp/ch(i)19}} $ . Shown in grey are the vectors associated with $ y $ and  $ z $ , offset from the origin by  $ 2 $ units along the $ x $ - axis, so that their entire body lies in the plane. Thus the vector sum of the two, shown in black, has its entire body in the plane along with the rest of the parallelogram. Generalizing, a set of the form $ set{vec{p}+t_1vec{v}_1+t_2vec{v}_2+cdots+t_kvec{v}_k suchthat t_1, ldots , t_kinRe} $ where $ vec{v}_1, ldots, vec{v}_kinRe^n $ and $ kleq n $ is a $ k $ - dimensional linear surface (or $ k $ - flat). For example, in $ Re^4 $ $ set{colvec[r]{2  pi  3  - 0.5} +tcolvec[r]{1  0  0  0} suchthat tinRe} $ . is a line, $ set{ colvec[r]{0  0  0  0} +tcolvec[r]{1  1  0  - 1} +scolvec[r]{2  0  1  0} suchthat t, sinRe} $ . is a plane, and $ set{ colvec[r]{3  1  - 2  0.5} +rcolvec[r]{0  0  0  - 1} +scolvec[r]{1  0  1  0} +tcolvec[r]{2  0  1  0} suchthat r, s, tinRe} $ . is a three - dimensional linear surface. Again, the intuition is that a line permits motion in one direction, a plane permits motion in combinations of two directions, etc. When the dimension of the linear surface is one less than the dimension of the space, that is, when in $ Re^n $ we have an $ (n - 1) $ - flat, the surface is called a hyperplane. A description of a linear surface can be misleading about the dimension. For example, this $ L=set{ colvec[r]{1  0  - 1  - 2} +tcolvec[r]{1  1  0  - 1} +scolvec[r]{2  2  0  - 2} suchthat t, sinRe} $ . is a degenerate plane because it is actually a line, since the vectors are multiples of each other and we can omit one. $ L=set{ colvec[r]{1  0  - 1  - 2} +rcolvec[r]{1  1  0  - 1} suchthat rinRe} $ . We shall see in the Linear Independence section of Chapter Two what relationships among vectors causes the linear surface they generate to be degenerate. We now can restate in geometric terms our conclusions from earlier. First, the solution set of a linear system with $ n $ unknowns is a linear surface in $ Re^n $ . Specifically, it is a $ k $ - dimensional linear surface, where $ k $ is the number of free variables in an echelon form version of the system. For instance, in the single equation case the solution set is an $ n - 1 $ - dimensional hyperplane in $ Re^n $ , where $ ngeq 1 $ . Second, the solution set of a homogeneous linear system is a linear surface passing through the origin. Finally, we can view the general solution set of any linear system as being the solution set of its associated homogeneous system offset from the origin by a vector, namely by any particular solution. We've translated the first section's results about solution sets into geometric terms, to better understand those sets. But we must be careful not to be misled by our own termsDash labeling subsets of $ Re^k $ of the forms $ set{vec{p}+tvec{v}suchthat tinRe} $ and $ set{vec{p}+tvec{v}+svec{w}suchthat t, sinRe} $ as `lines' and `planes' doesn't make them act like the lines and planes of our past experience. Rather, we must ensure that the names suit the sets. While we can't prove that the sets satisfy our intuitionDash we can't prove anything about intuitionDash in this subsection we'll observe that a result familiar from $ Re^2 $ and $ Re^3 $ , when generalized to arbitrary $ Re^n $ , supports the idea that a line is straight and a plane is flat. Specifically, we'll see how to do Euclidean geometry in a `plane' by giving a definition of the angle between two $ Re^n $ vectors, in the plane that they generate. The length of a vector $ vec{v}inRe^n $ is the square root of the sum of the squares of its components. $ absval{vec{v}, }=sqrt{v_1^2+cdots+v_n^2} $ . This is a natural generalization of the Pythagorean Theorem. A classic motivating discussion is in . For any nonzero $ vec{v} $ , the vector $ vec{v}/absval{vec{v}} $ has length one. We say that the second normalizes $ vec{v} $ to length one. We can use that to get a formula for the angle between two vectors. Consider two vectors in $ Re^3 $ where neither is a multiple of the other (the special case of multiples will turn out below not to be an exception). They determine a two - dimensional planeDash for instance, put them in canonical position and take the plane formed by the origin and the endpoints. In that plane consider the triangle with sides $ vec{u} $ , $ vec{v} $ , and $ vec{u} - vec{v} $ . Apply the Law of Cosines: $ absval{vec{u} - vec{v}, }^2 = absval{vec{u}, }^2+absval{vec{v}, }^2 - 2, absval{vec{u}, }, absval{vec{v}, }costheta $ where $ theta $ is the angle between the vectors. The left side gives (u_1 - v_1)^2+(u_2 - v_2)^2+(u_3 - v_3)^2  =(u_1^2 - 2u_1v_1+v_1^2)+(u_2^2 - 2u_2v_2+v_2^2)+(u_3^2 - 2u_3v_3+v_3^2) while the right side gives this. $ (u_1^2+u_2^2+u_3^2)+(v_1^2+v_2^2+v_3^2) - 2, absval{vec{u}, }, absval{vec{v}, }costheta $ . Canceling squares $ u_1^2 $ , ldots, $ v_3^2 $ and dividing by $ 2 $ gives a formula for the angle. $ theta = arccos(, frac{u_1v_1+u_2v_2+u_3v_3}{ absval{vec{u}, }, absval{vec{v}, } }, ) $ . In higher dimensions we cannot draw pictures as above but we can instead make the argument analytically. First, the form of the numerator is clear; it comes from the middle terms of $ (u_i - v_i)^2 $ . The dot product (or inner product or scalar product) of two $ n $ - component real vectors is the linear combination of their components. $ vec{u}dotprodvec{v}=lincombo{u}{v} $ . noindent Note that the dot product of two vectors is a real number, not a vector, and that the dot product is only defined if the two vectors have the same number of components. Note also that dot product is related to length: $ vec{u}dotprodvec{u}=u_1u_1+cdots+u_nu_n=absval{vec{u}, }^2 $ . Some authors require that the first vector be a row vector and that the second vector be a column vector. We shall not be that strict and will allow the dot product operation between two column vectors. Still reasoning analytically but guided by the pictures, we use the next theorem to argue that the triangle formed by the line segments making the bodies of $ vec{u} $ , $ vec{v} $ , and $ vec{u}+vec{v} $ in $ Re^n $ lies in the planar subset of $ Re^n $ generated by $ vec{u} $ and $ vec{v} $ (see the figure below). [Triangle Inequality] For any $ vec{u}, vec{v}inRe^n $ , $ absval{vec{u}+vec{v}, }leqabsval{vec{u}, }+absval{vec{v}, } $ . with equality if and only if one of the vectors is a nonnegative scalar multiple of the other one. This is the source of the familiar saying, ``The shortest distance between two points is in a straight line.'' (We'll use some algebraic properties of dot product that we have not yet checked, for instance that $ vec{u}dotprod(vec{a}+vec{b}) =vec{u}dotprodvec{a}+vec{u}dotprodvec{b} $ and that $ vec{u}dotprodvec{v}=vec{v}dotprodvec{u} $ . See $ X $ .) Since all the numbers are positive, the inequality holds if and only if its square holds. $ absval{vec{u}+vec{v}, }^2 &leq(, absval{vec{u}, }+absval{vec{v}, }, )^2  (, vec{u}+vec{v}, )dotprod(, vec{u}+vec{v}, ) &leqabsval{vec{u}, }^2+2, absval{vec{u}, }, absval{vec{v}, } +absval{vec{v}, }^2  vec{u}dotprodvec{u}+vec{u}dotprodvec{v} +vec{v}dotprodvec{u}+vec{v}dotprodvec{v} &leqvec{u}dotprodvec{u}+2, absval{vec{u}, }, absval{vec{v}, } +vec{v}dotprodvec{v}  2, vec{u}dotprodvec{v} &leq 2, absval{vec{u}, }, absval{vec{v}, } $ . That, in turn, holds if and only if the relationship obtained by multiplying both sides by the nonnegative numbers $ absval{vec{u}, } $ and $ absval{vec{v}, } $ $ 2, (, absval{vec{v}, }, vec{u}, )dotprod(, absval{vec{u}, }, vec{v}, ) leq 2, absval{vec{u}, }^2, absval{vec{v}, }^2 $ . and rewriting $ 0 leq absval{vec{u}, }^2, absval{vec{v}, }^2 - 2, (, absval{vec{v}, }, vec{u}, )dotprod(, absval{vec{u}, }, vec{v}, ) +absval{vec{u}, }^2, absval{vec{v}, }^2 $ . is true. But factoring shows that it is true $ 0leq (, absval{vec{u}, }, vec{v} - absval{vec{v}, }, vec{u}, )dotprod (, absval{vec{u}, }, vec{v} - absval{vec{v}, }, vec{u}, ) $ . since it only says that the square of the length of the vector $ absval{vec{u}, }, vec{v} - absval{vec{v}, }, vec{u}, $ is not negative. As for equality, it holds when, and only when, $ absval{vec{u}, }, vec{v} - absval{vec{v}, }, vec{u} $ is $ zero $ . The check that $ absval{vec{u}, }, vec{v}=absval{vec{v}, }, vec{u}, $ if and only if one vector is a nonnegative real scalar multiple of the other is easy. This result supports the intuition that even in higher - dimensional spaces, lines are straight and planes are flat. We can easily check from the definition that linear surfaces have the property that for any two points in that surface, the line segment between them is contained in that surface. But if the linear surface were not flat then that would allow for a shortcut. Because the Triangle Inequality says that in any $ Re^n $ the shortest cut between two endpoints is simply the line segment connecting them, linear surfaces have no bends. Back to the definition of angle measure. The heart of the Triangle Inequality's proof is the $ vec{u}dotprodvec{v}leq absval{vec{u}, }, absval{vec{v}, } $ line. We might wonder if some pairs of vectors satisfy the inequality in this way: while $ vec{u}dotprodvec{v} $ is a large number, with absolute value bigger than the right - hand side, it is a negative large number. The next result says that does not happen. [Cauchy - Schwarz Inequality] For any $ vec{u}, vec{v}inRe^n $ , $ absval{, vec{u}dotprodvec{v}, } leq absval{, vec{u}, }, absval{vec{v}, } $ . with equality if and only if one vector is a scalar multiple of the other. The Triangle Inequality's proof shows that $ vec{u}dotprodvec{v}leq absval{vec{u}, }, absval{vec{v}, } $ so if $ vec{u}dotprodvec{v} $ is positive or zero then we are done. If $ vec{u}dotprodvec{v} $ is negative then this holds. $ absval{, vec{u}dotprodvec{v}, } = - (, vec{u}dotprodvec{v}, ) =( - vec{u}, )dotprodvec{v} leq absval{ - vec{u}, }, absval{vec{v}, } =absval{vec{u}, }, absval{vec{v}, } $ . The equality condition is $ X $ . The Cauchy - Schwarz inequality assures us that the next definition makes sense because the fraction has absolute value less than or equal to one. The angle between two nonzero vectors $ vec{u}, vec{v}inRe^n $ is $ theta = arccos(, frac{vec{u}dotprodvec{v}}{ absval{vec{u}, }, absval{vec{v}, } }, ) $ . (if either is the zero vector then we take the angle to be a right angle). Vectors from $ Re^n $ are orthogonal, that is, perpendicular, if and only if their dot product is zero. They are parallel if and only if their dot product equals the product of their lengths. These vectors are orthogonal. We've drawn the arrows away from canonical position but nevertheless the vectors are orthogonal. The $ Re^3 $ angle formula given at the start of this subsection is a special case of the definition. Between these two the angle is $ arccos(frac{(1)(0)+(1)(3)+(0)(2)}{sqrt{1^2+1^2+0^2}sqrt{0^2+3^2+2^2}}) =arccos(frac{3}{sqrt{2}sqrt{13}}) $ . approximately $ 0.94 $ X $ $ . Notice that these vectors are not orthogonal. Although the $ yz $ - plane may appear to be perpendicular to the $ xy $ - plane, in fact the two planes are that way only in the weak sense that there are vectors in each orthogonal to all vectors in the other. Not every vector in each is orthogonal to all vectors in the other.
Determinants are a font of interesting and amusing formulas. Here is one that is often used to compute determinants by hand. In the permutation expansion of $ deter{T} $ , every term has one and only one entry from each row and column of  $ T $ . $ deter{T}=!! sum_{text{permutations }phi}!!!! t_{1, phi(1)}t_{2, phi(2)}cdots t_{n, phi(n)} deter{P_{phi}} $ . Here is the formula's $ nbyn{3} $ instance. $ t_{1, 1} &t_{1, 2} &t_{1, 3}  t_{2, 1} &t_{2, 2} &t_{2, 3}  t_{3, 1} &t_{3, 2} &t_{3, 3} &= [t] &t_{1, 1}t_{2, 2}t_{3, 3}deter{P_{phi_1}} +t_{1, 1}t_{2, 3}t_{3, 2}deter{P_{phi_2}} +t_{1, 2}t_{2, 1}t_{3, 3}deter{P_{phi_3}}  &hspace*{0em} +t_{1, 2}t_{2, 3}t_{3, 1}deter{P_{phi_4}} +t_{1, 3}t_{2, 1}t_{3, 2}deter{P_{phi_5}} +t_{1, 3}t_{2, 2}t_{3, 1}deter{P_{phi_6}}  &= [t] &t_{1, 1}t_{2, 2}t_{3, 3} - t_{1, 1}t_{2, 3}t_{3, 2} - t_{1, 2}t_{2, 1}t_{3, 3}  &hspace*{0em} +t_{1, 2}t_{2, 3}t_{3, 1} +t_{1, 3}t_{2, 1}t_{3, 2} - t_{1, 3}t_{2, 2}t_{3, 1} $ . We can focus on any row or column; here we bring out the entries from $ T $ 's first row. $ &= t_{1, 1}cdotbig[t_{2, 2}t_{3, 3} - t_{2, 3}t_{3, 2}big]  &quad - t_{1, 2}cdotbig[t_{2, 1}t_{3, 3} - t_{2, 3}t_{3, 1}big]  &quad +t_{1, 3}cdotbig[t_{2, 1}t_{3, 2} - t_{2, 2}t_{3, 1}big] tag{ $ * $ } $ . Note that inside the first set of square brackets is the determinant $ t_{2, 2} &t_{2, 3}  t_{3, 2} &t_{3, 3} tag{ $ ** $ } $ . and similarly, inside the other brackets are other determinants. $ t_{2, 1} &t_{2, 3}  t_{3, 1} &t_{3, 3} qquad t_{2, 1} &t_{2, 2}  t_{3, 1} &t_{3, 2} tag{ $ *!*!* $ } $ . The explanation for these determinants lies with the permutations. If we fix some row and columnDash such as when we consider all terms containing  $ t_{1, 1} $ , or all terms containing  $ t_{1, 2} $ , or  $ t_{1, 3} $ Dash then what remains is for the permutation to pick one entry from each of the other rows and columns. The matrices below illustrate. On the left, the row and column containing  $ t_{1, 1} $ are shaded and what remains is the matrix from ( $ ** $ ) above. Similarly, in the other two what remains are the matrices from ( $ *!*!* $ ). definecolor{shadematrix}{gray}{0.8} $ left({>{columncolor{shadematrix}}ccc} rowcolor{shadematrix} t_{1, 1} &t_{1, 2} &t_{1, 3}  t_{2, 1} &t_{2, 2} &t_{2, 3}  t_{3, 1} &t_{3, 2} &t_{3, 3} right) quad left({c>{columncolor{shadematrix}}cc} rowcolor{shadematrix} t_{1, 1} &t_{1, 2} &t_{1, 3}  t_{2, 1} &t_{2, 2} &t_{2, 3}  t_{3, 1} &t_{3, 2} &t_{3, 3} right) quad left({cc>{columncolor{shadematrix}}c} rowcolor{shadematrix} t_{1, 1} &t_{1, 2} &t_{1, 3}  t_{2, 1} &t_{2, 2} &t_{2, 3}  t_{3, 1} &t_{3, 2} &t_{3, 3} right) $ . So that accounts for the appearance in equation ( $ * $ ) of the determinants of smaller - sized matrices. As to the minus sign on that equation's second line, consider this version of $ deter{T}=t_{1, 1}t_{2, 2}t_{3, 3}deter{P_{phi_1}}+cdots+t_{1, 3}t_{2, 2}t_{3, 1}deter{P_{phi_6}} $ . $ [t] &t_{1, 1}cdot bigg[t_{2, 2}t_{3, 3}[r] 1 &0 &0  0 &1 &0  0 &0 &1 +t_{2, 3}t_{3, 2}[r] 1 &0 &0  0 &0 &1  0 &1 &0 , bigg]  &hbox{}quadhbox{} +t_{1, 2}cdot bigg[t_{2, 1}t_{3, 3}[r] 0 &1 &0  1 &0 &0  0 &0 &1 +t_{2, 3}t_{3, 1}[r] 0 &1 &0  0 &0 &1  1 &0 &0 , bigg]  &hbox{}quadhbox{} +t_{1, 3}cdot bigg[t_{2, 1}t_{3, 2}[r] 0 &0 &1  1 &0 &0  0 &1 &0 +t_{2, 2}t_{3, 1}[r] 0 &0 &1  0 &1 &0  1 &0 &0 , bigg] $ . The matrices $ P_{phi_1} $ and  $ P_{phi_2} $ in the first line are all set up for the determinant ( $ ** $ ), since their first row is as in the identity matrix and their other two rows look like the rows for that determinant. The matrices $ P_{phi_3} $ and  $ P_{phi_4} $ in the second line will be set up in the same way with a single row swap, putting  $ rowvec{1 &0 &0} $ into the first row. Of course, that row swap puts a negative sign on that line. Finally, setting up the third line's matrices in the same way takes two swaps, which keeps the sign positive. In short, we have this. $ deter{T} =t_{1, 1}cdot t_{2, 2} &t_{2, 3}  t_{3, 2} &t_{3, 3} - t_{1, 2}cdot t_{2, 1} &t_{2, 3}  t_{3, 1} &t_{3, 3} +t_{1, 3}cdot t_{2, 1} &t_{2, 2}  t_{3, 1} &t_{3, 2} $ . The formula given in $ X $ , which generalizes this, is a recurrence since the determinant is expressed as a combination of determinants. This formula isn't circular because it gives the $ nbyn{n} $ determinant in terms of smaller - sized matrices. For any $ nbyn{n} $ matrix $ T $ , the $ nbyn{(n - 1)} $ matrix formed by deleting row  $ i $ and column  $ j $ of $ T $ is the $ i, j $ minor of $ T $ . The $ i, j $ cofactor $ T_{i, j} $ of $ T $ is $ ( - 1)^{i+j} $ times the determinant of the $ i, j $ minor of $ T $ . The $ 1, 2 $ cofactor of the matrix from equation ( $ * $ ) is the negative of the first determinant in ( $ *!*!* $ ). $ T_{1, 2}= - 1cdot t_{2, 1} &t_{2, 3}  t_{3, 1} &t_{3, 3} $ . Where $ T= [r] 1 &2 &3  4 &5 &6  7 &8 &9 $ . these are the $ 1, 2 $ and $ 2, 2 $ cofactors. $ T_{1, 2}= ( - 1)^{1+2}cdot[r] 4 &6  7 &9 =6 qquad T_{2, 2}= ( - 1)^{2+2}cdot[r] 1 &3  7 &9 = - 12 $ . [Laplace Expansion of Determinants] Where $ T $ is an $ nbyn{n} $ matrix, we can find the determinant by expanding by cofactors on any row  $ i $ or column  $ j $ . $ deter{T} &=t_{i, 1}cdot T_{i, 1}+t_{i, 2}cdot T_{i, 2}+cdots+t_{i, n}cdot T_{i, n}  &=t_{1, j}cdot T_{1, j}+t_{2, j}cdot T_{2, j}+cdots+t_{n, j}cdot T_{n, j} $ . $ X $ . We can compute the determinant $ deter{T}= [r] 1 &2 &3  4 &5 &6  7 &8 &9 $ . by expanding along the first row. $ deter{T} =1cdot(+1)[r] 5 &6  8 &9 +2cdot( - 1)[r] 4 &6  7 &9 +3cdot(+1)[r] 4 &5  7 &8 = - 3+12 - 9 =0 $ . We could also expand down the second column. $ deter{T} =2cdot( - 1)[r] 4 &6  7 &9 +5cdot(+1)[r] 1 &3  7 &9 +8cdot( - 1)[r] 1 &3  4 &6 =12 - 60+48 =0 $ . A row or column with many zeroes makes a this expansion easier. $ [r] 1 &5 &0  2 &1 &1  3 & - 1 &0 = 0cdot(+1)[r] 2 &1  3 & - 1 + 1cdot( - 1)[r] 1 &5  3 & - 1 + 0cdot(+1)[r] 1 &5  2 &1 =16 $ . We finish by applying Laplace's expansion to derive a new formula for the inverse of a matrix. With $ X $ , we can calculate the determinant of a matrix by taking linear combinations of entries from a row with their associated cofactors. $ t_{i, 1}cdot T_{i, 1}+t_{i, 2}cdot T_{i, 2}+dots+t_{i, n}cdot T_{i, n} =deter{T} $ . Recall that a matrix with two identical rows has a zero determinant. Thus, weighting the cofactors by entries from row  $ k $ with $ kneq i $ gives zero $ t_{i, 1}cdot T_{k, 1}+t_{i, 2}cdot T_{k, 2}+dots+t_{i, n}cdot T_{k, n}=0 $ . because it represents the expansion along the row  $ k $ of a matrix with row  $ i $ equal to row  $ k $ . This summarizes. $ generalmatrix{t}{n}{n} T_{1, 1} &T_{2, 1} &ldots &T_{n, 1}  T_{1, 2} &T_{2, 2} &ldots &T_{n, 2}  &vdots & &  T_{1, n} &T_{2, n} &ldots &T_{n, n} = |T| &0 &ldots &0  0 &|T| &ldots &0  &vdots & &  0 &0 &ldots &|T| $ . Note that the order of the subscripts in the matrix of cofactors is opposite to the order of subscripts in the other matrix; e.g., along the first row of the matrix of cofactors the subscripts are $ 1, 1 $ then $ 2, 1 $ , etc. The matrix adjoint (or adjugate) to the square matrix $ T $ (or the classical adjoint) is $ adj(T)= T_{1, 1} &T_{2, 1} &ldots &T_{n, 1}  T_{1, 2} &T_{2, 2} &ldots &T_{n, 2}  &vdots & &  T_{1, n} &T_{2, n} &ldots &T_{n, n} $ . where the row  $ i $ , column  $ j $ entry, $ T_{j, i} $ , is the $ j, i $ cofactor. Where $ T $ is a square matrix, $ Tcdot adj(T)=adj(T)cdot T=deter{T}cdot I $ . Thus if $ T $ has an inverse, if $ deter{T}neq 0 $ , then $ T^{ - 1}=(1/deter{T})cdotadj(T) $ . The discussion before the theorem statement makes it clear. If $ T=[r] 1 &0 &4  2 &1 & - 1  1 &0 &1 $ . then $ adj(T) $ is $ T_{1, 1} &T_{2, 1} &T_{3, 1}  T_{1, 2} &T_{2, 2} &T_{3, 2}  T_{1, 3} &T_{2, 3} &T_{3, 3} !!=!! [r] 1 & - 1  0 &1 & - [r] 0 &4  0 &1 &[r] 0 &4  1 & - 1 - [r] 2 & - 1  1 &1 &[r] 1 &4  1 &1 & - [r] 1 &4  2 & - 1 [r] 2 &1  1 &0 & - [r] 1 &0  1 &0 &[r] 1 &0  2 &1 !!=! [r] 1 &0 & - 4  - 3 & - 3 &9  - 1 &0 &1 $ . and taking the product with $ T $ gives the diagonal matrix $ deter{T}cdot I $ . $ [r] 1 &0 &4  2 &1 & - 1  1 &0 &1 [r] 1 &0 & - 4  - 3 & - 3 &9  - 1 &0 &1 =[r] - 3 &0 &0  0 & - 3 &0  0 &0 & - 3 $ . noindent The inverse of $ T $ is $ (1/ - 3)cdotadj(T) $ . $ T^{ - 1} =[r] braces make the - a negative sign? 1/hbox{ $ - 3 $ } &0/hbox{ $ - 3 $ } & - 4/hbox{ $ - 3 $ }  - 3/hbox{ $ - 3 $ } & - 3/hbox{ $ - 3 $ } &9/hbox{ $ - 3 $ }  - 1/hbox{ $ - 3 $ } &0/hbox{ $ - 3 $ } &1/hbox{ $ - 3 $ } =[r] - 1/3 &0 &4/3  1 &1 & - 3  1/3 &0 & - 1/3 $ . The formulas from this subsection are often used for by - hand calculation and are sometimes useful with special types of matrices. However, for generic matrices they are not the best choice because they require more arithmetic than, for instance, the Gauss - Jordan method.
The prior section develops the determinant algebraically, by considering formulas satisfying certain conditions. This section complements that with a geometric approach. Beyond its intuitive appeal, an advantage of this approach is that while we have so far only considered whether or not a determinant is zero, here we shall give a meaning to the value of the determinant. (The prior section treats the determinant as a function of the rows but this section focuses on columns.) This parallelogram picture is familiar from the construction of the sum of the two vectors. In $ Re^n $ the box (or parallelepiped) formed by $ sequence{vec{v}_1, dots, vec{v}_n} $ is the set $ set{t_1vec{v}_1+dots+t_nvec{v}_n suchthat t_1, ldots, t_nin closedinterval{0}{1}} $ . noindent Thus the parallelogram above is the box formed by $ sequence{binom{x_1}{y_1}, binom{x_2}{y_2}} $ . A three - space box is shown in $ X $ . We can find the area of the above box by drawing an enclosing rectangle and subtracting away areas not in the box. That the area equals the value of the determinant $ x_1 &x_2  y_1 &y_2 =x_1y_2 - x_2y_1 $ . is no coincidence. The definition of determinants contains four properties that we know lead to a unique function for each dimension  $ n $ . We shall argue that these properties make good postulates for a function that measures the size of boxes in $ n $ - space. For instance, such a function should have the property that multiplying one of the box - defining vectors by a scalar will multiply the size by that scalar. Shown here is $ k=(i)4 $ . On the right the rescaled region is in solid lines with the original region shaded for comparison. That is, we can reasonably expect that $ size (dots, kvec{v}, dots)=kcdotsize (dots, vec{v}, dots) $ . Of course, this condition is one of those in the definition of determinants. Another property of determinants that should apply to any function measuring the size of a box is that it is unaffected by row combinations. Here are before - combining and after - combining boxes (the scalar shown is $ k= - 0.35 $ ). The box formed by $ v $ and $ kvec{v}+vec{w} $ slants differently than the original one but the two have the same base and the same height, and hence the same area. So we expect that size is not affected by a shear operation $ size (dots, vec{v}, dots, vec{w}, dots) =size (dots, vec{v}, dots, kvec{v}+vec{w}, dots) $ . Again, this is a determinant condition. We expect that the box formed by unit vectors has unit size and we naturally extend that to any $ n $ - space $ size(vec{e}_1, dots, vec{e}_n)=1 $ . Condition (2) of the definition of determinant is redundant, as remarked following the definition. We know from the prior section that for each  $ n $ the determinant exists and is unique so we know that these postulates for size functions are consistent and that we do not need any more postulates. Therefore, we are justified in interpreting $ det(vec{v}_1, dots, vec{v}_n) $ as giving the size of the box formed by the vectors. Although condition (2) is redundant it raises an important point. Consider these two. Swapping the columns changes the sign. On the left, starting with $ vec{u} $ and following the arc inside the angle to $ vec{v} $ (that is, going counterclockwise), we get a positive size. On the right, starting at $ vec{v} $ and going to  $ vec{u} $ , and so following the clockwise arc, gives a negative size. The sign returned by the size function reflects the orientation or sense of the box. (We see the same thing if we picture the effect of scalar multiplication by a negative scalar.) The volume of a box is the absolute value of the determinant of a matrix with those vectors as columns. By the formula that takes the area of the base times the height, the volume of this parallelepiped is $ 12 $ . That agrees with the determinant. Taking the vectors in a different order changes the sign but not the magnitude. $ [r] 0 &2 & - 1  3 &0 &0  1 &2 &1 = - 12 $ . A transformation $ map{t}{Re^n}{Re^n} $ changes the size of all boxes by the same factor, namely, the size of the image of a box $ deter{t(S)} $ is $ deter{T} $ times the size of the box $ deter{S} $ , where $ T $ is the matrix representing $ t $ with respect to the standard basis. That is, the determinant of a product is the product of the determinants $ deter{TS}=deter{T}cdotdeter{S} $ . The two sentences say the same thing, first in map terms and then in matrix terms. This is because $ deter{t(S)}=deter{TS} $ , as both give the size of the box that is the image of the unit box $ stdbasis_n $ under the composition $ composed{t}{s} $ , where the maps are represented with respect to the standard basis. We will prove the second sentence. First consider the case that $ T $ is singular and thus does not have an inverse. Observe that if $ TS $ is invertible then there is an $ M $ such that $ (TS)M=I $ , so $ T(SM)=I $ , and so $ T $ is invertible. The contrapositive of that observation is that if $ T $ is not invertible then neither is $ TS $ Dash if $ deter{T}=0 $ then $ deter{TS}=0 $ . Now consider the case that $ T $ is nonsingular. Any nonsingular matrix factors into a product of elementary matrices $ T=E_1E_2cdots E_r $ . To finish this argument we will verify that $ deter{ES}=deter{E}cdotdeter{S} $ for all matrices  $ S $ and elementary matrices  $ E $ . The result will then follow because $ deter{TS}=deter{E_1cdots E_rS}=deter{E_1}cdotsdeter{E_r}cdotdeter{S} =deter{E_1cdots E_r}cdotdeter{S}=deter{T}cdotdeter{S} $ . There are three types of elementary matrix. We will cover the $ M_i(k) $ case; the $ P_{i, j} $ and $ C_{i, j}(k) $ checks are similar. The matrix $ M_i(k)S $ equals $ S $ except that row  $ i $ is multiplied by $ k $ . The third condition of determinant functions then gives that $ deter{M_i(k)S}=kcdotdeter{S} $ . But $ deter{M_i(k)}=k $ , again by the third condition because $ M_i(k) $ is derived from the identity by multiplication of row  $ i $ by $ k $ . Thus $ deter{ES}=deter{E}cdotdeter{S} $ holds for $ E=M_i(k) $ . Application of the map $ t $ represented with respect to the standard bases by $ [r] 1 &1  - 2 &0 $ . will double sizes of boxes, e.g., from this to this If a matrix is invertible then the determinant of its inverse is the inverse of its determinant $ deter{T^{ - 1}}=1/deter{T} $ . $ 1=deter{I}=deter{TT^{ - 1}}=deter{T}cdotdeter{T^{ - 1}} $
In the first chapter we highlighted the special case of linear systems with the same number of equations as unknowns, those of the form $ Tvec{x}=vec{b} $ where $ T $ is a square matrix. We noted that there are only two kinds of $ T $ 's. If $ T $ is associated with a unique solution for any $ vec{b} $ , such as for the homogeneous system $ Tvec{x}=zero $ , then $ T $ is associated with a unique solution for every such $ vec{b} $ . We call such a matrix nonsingular. The other kind of $ T $ , where every linear system for which it is the matrix of coefficients has either no solution or infinitely many solutions, we call singular. In our work since then this distinction has been a theme. For instance, we now know that an $ nbyn{n} $ matrix $ T $ is nonsingular if and only if each of these holds: any system $ Tvec{x}=vec{b} $ has a solution and that solution is unique; Gauss - Jordan reduction of $ T $ yields an identity matrix; the rows of $ T $ form a linearly independent set; the columns of $ T $ form a linearly independent set, a basis for $ Re^n $ ; any map that $ T $ represents is an isomorphism; an inverse matrix $ T^{ - 1} $ exists. So when we look at a square matrix, one of the first things that we ask is whether it is nonsingular. This chapter develops a formula that determines whether $ T $ is nonsingular. More precisely, we will develop a formula for $ nbyn{1} $  matrices, one for $ nbyn{2} $  matrices, etc. These are naturally related; that is, we will develop a family of formulas, a scheme that describes the formula for each size. Since we will restrict the discussion to square matrices, in this chapter we will often simply say `matrix' in place of `square matrix'. Determining nonsingularity is trivial for $ nbyn{1} $ matrices. $ a quadtext{is nonsingular iff}quad a neq 0 $ . Corollary Three.IV. gives the $ nbyn{2} $ formula. $ a &b  c &d quadtext{is nonsingular iff}quad ad - bc neq 0 $ . We can produce the $ nbyn{3} $ formula as we did the prior one, although the computation is intricate (see $ X $ ). $ a &b &c  d &e &f  g &h &i quadtext{is nonsingular iff}quad aei+bfg+cdh - hfa - idb - gec neq 0 $ . With these cases in mind, we posit a family of formulas: $ a $ , $ ad - bc $ , etc. For each $ n $ the formula defines a determinant function $ map{det_{nbyn{n}}}{matspace_{nbyn{n}}}{Re} $ such that an $ nbyn{n} $ matrix $ T $ is nonsingular if and only if $ det_{nbyn{n}}(T)neq 0 $ . (We usually omit the subscript $ nbyn{n} $ because the size of $ T $ describes which determinant function we mean.) textit{This subsection is an optional motivation and development of the general definition. The definition is in the next subsection.} Above, in each case the matrix is nonsingular if and only if some formula is nonzero. But the three formulas don't show an obvious pattern. We may spot that the $ nbyn{1} $ term $ a $ has one letter, that the $ nbyn{2} $ terms $ ad $ and $ bc $ have two letters, and that the $ nbyn{3} $ terms each have three letters. We may even spot that in those terms there is a letter from each row and column of the matrix, e.g., in the $ cdh $ term one letter comes from each row and from each column. $ & &c  d  &h $ . But these observations are perhaps more puzzling than enlightening. For instance, we might wonder why some terms are added but some are subtracted. A good strategy for solving problems is to explore which properties the solution must have, and then search for something with those properties. So we shall start by asking what properties we'd like the determinant formulas to have. At this point, our main way to decide whether a matrix is singular or not is to do Gaussian reduction and then check whether the diagonal of the echelon form matrix has any zeroes, that is, whether the product down the diagonal is zero. So we could guess that whatever determinant formula we find, the proof that it is right may involve applying Gauss's Method to the matrix to show that in the end the product down the diagonal is zero if and only if our formula gives zero. This suggests a plan: we will look for a family of determinant formulas that are unaffected by row operations and such that the determinant of an echelon form matrix is the product of its diagonal entries. In the rest of this subsection we will test this plan against the $ nbyn{2} $ and $ nbyn{3} $ formulas. In the end we will have to modify the ``unaffected by row operations'' part, but not by much. First we check whether the $ nbyn{2} $ and $ nbyn{3} $ formulas are unaffected by the row operation of combining: if $ T grstep{krho_i+rho_j} hat{T} $ . then is $ det(hat{T})=det(T) $ ? This check of the $ nbyn{2} $ determinant after the $ krho_1+rho_2 $ operation $ det( a &b  ka+c &kb+d  ) = a(kb+d) - (ka+c)b = ad - bc $ . shows that it is indeed unchanged, and the other $ nbyn{2} $ combination $ krho_2+rho_1 $ gives the same result. Likewise, the $ nbyn{3} $ combination $ krho_3+rho_2 $ leaves the determinant unchanged $ det( a &b &c  kg+d &kh+e &ki+f  g &h &i ) &=[t]{@{}l@{}} a(kh+e)i+b(ki+f)g+c(kg+d)h   hbox{} - h(ki+f)a - i(kg+d)b - g(kh+e)c  &=aei + bfg + cdh - hfa - idb - gec $ . as do the other $ nbyn{3} $ row combination operations. So there seems to be promise in the plan. Of course, perhaps if we had worked out the $ nbyn{4} $ determinant formula and tested it then we might have found that it is affected by row combinations. This is an exploration and we do not yet have all the facts. Nonetheless, so far, so good. Next we compare $ det(hat{T}) $ with $ det(T) $ for row swaps. Here we hit a snag: the $ nbyn{2} $ row swap $ rho_1leftrightarrowrho_2 $ does not yield $ ad - bc $ . $ det( c &d  a &b ) = bc - ad $ . And this $ rho_1leftrightarrowrho_3 $ swap inside of a $ nbyn{3} $ matrix $ det( g &h &i  d &e &f  a &b &c ) = gec + hfa + idb - bfg - cdh - aei $ . also does not give the same determinant as before the swap since again there is a sign change. Trying a different $ nbyn{3} $ swap $ rho_1leftrightarrowrho_2 $ $ det( d &e &f  a &b &c  g &h &i ) = dbi + ecg + fah - hcd - iae - gbf $ . also gives a change of sign. So row swaps appear in this experiment to change the sign of a determinant. This does not wreck our plan entirely. We hope to decide nonsingularity by considering only whether the formula gives zero, not by considering its sign. Therefore, instead of expecting determinant formulas to be entirely unaffected by row operations we modify our plan so that on a swap they will change sign. Obviously we finish by comparing $ det(hat{T}) $ with $ det(T) $ for the operation of multiplying a row by a scalar. This $ det( a &b  kc &kd ) = a(kd) - (kc)b =kcdot (ad - bc) $ . ends with the entire determinant multiplied by  $ k $ , and the other $ nbyn{2} $ case has the same result. This $ nbyn{3} $ case ends the same way $ det( a &b &c  d &e &f  kg &kh &ki ) &= [t]{@{}l@{}} ae(ki) + bf(kg) + cd(kh)  > - (kh)fa - (ki)db - (kg)ec  &= kcdot(aei + bfg + cdh - hfa - idb - gec) $ . as do the other two $ nbyn{3} $ cases. These make us suspect that multiplying a row by  $ k $ multiplies the determinant by  $ k $ . As before, this modifies our plan but does not wreck it. We are asking only that the zero - ness of the determinant formula be unchanged, not focusing on the its sign or magnitude. So in this exploration our plan got modified in some inessential ways and is now: we will look for $ nbyn{n} $ determinant functions that remain unchanged under the operation of row combination, that change sign on a row swap, that rescale on the rescaling of a row, and such that the determinant of an echelon form matrix is the product down the diagonal. In the next two subsections we will see that for each  $ n $ there is one and only one such function. Finally, for the next subsection note that factoring out scalars is a row - wise operation: here $ det( [r] 3 &3 &9  2 &1 &1  5 &11 & - 5 ) =3 cdot det( [r] 1 &1 &3  2 &1 &1  5 &11 & - 5 ) $ . the $ 3 $ comes only out of the top row only, leaving the other rows unchanged. Consequently in the definition of determinant we will write it as a function of the rows $ det (vec{rho}_1, vec{rho}_2, dotsvec{rho}_n) $ , rather than as $ det(T) $ or as a function of the entries $ det(t_{1, 1}, dots, t_{n, n}) $ . We want a formula to determine whether an $ nbyn{n} $ matrix is nonsingular. We will not begin by stating such a formula. Instead we will begin by considering, for each  $ n $ , the function that such a formula calculates. We will define this function by a list of properties. We will then prove that a function with these properties exists and is unique, and also describe how to compute it. (Because we will eventually prove this, from the start we will just say ` $ det(T) $ ' instead of `if there is a unique determinant function then $ det(T) $ '.) A $ nbyn{n} $ determinant/ is a function $ map{det}{matspace_{nbyn{n}}}{Re} $ such that $ det (vec{rho}_1, dots, kcdotvec{rho}_i + vec{rho}_j, dots, vec{rho}_n) =det (vec{rho}_1, dots, vec{rho}_j, dots, vec{rho}_n) $ for $ ine j $ $ det (vec{rho}_1, ldots, vec{rho}_j, dots, vec{rho}_i, dots, vec{rho}_n) = - det (vec{rho}_1, dots, vec{rho}_i, dots, vec{rho}_j, dots, vec{rho}_n) $ for $ ine j $ $ det (vec{rho}_1, dots, kvec{rho}_i, dots, vec{rho}_n) = kcdot det (vec{rho}_1, dots, vec{rho}_i, dots, vec{rho}_n) $ for any scalar  $ k $ $ det(I)=1 $ where $ I $ is an identity matrix (the $ vec{rho}, $ 's are the rows of the matrix). We often write $ deter{T} $ for $ det (T) $ . Condition (2) is redundant since $ Tgrstep{rho_i+rho_j} repeatedgrstep{ - rho_j+rho_i} repeatedgrstep{rho_i+rho_j} repeatedgrstep{ - rho_i} hat{T} $ . swaps rows $ i $ and  $ j $ . We have listed it for consistency with the Gauss's Method presentation in earlier chapters. Condition (3) does not have a $ kneq 0 $ restriction, although the Gauss's Method operation of multiplying a row by  $ k $ does have it. The next result shows that we do not need that restriction here. A matrix with two identical rows has a determinant of zero. A matrix with a zero row has a determinant of zero. A matrix is nonsingular if and only if its determinant is nonzero. The determinant of an echelon form matrix is the product down its diagonal. To verify the first sentence swap the two equal rows. The sign of the determinant changes but the matrix is the same and so its determinant is the same. Thus the determinant is zero. For the second sentence multiply the zero row by two. That doubles the determinant but it also leaves the row unchanged, and hence leaves the determinant unchanged. Thus the determinant must be zero. Do Gauss - Jordan reduction for the third sentence, $ T rightarrowcdotsrightarrowhat{T} $ . By the first three properties the determinant of $ T $ is zero if and only if the determinant of $ hat{T} $ is zero (although the two could differ in sign or magnitude). A nonsingular matrix $ T $ Gauss - Jordan reduces to an identity matrix and so has a nonzero determinant. A singular $ T $ reduces to a $ hat{T} $ with a zero row; by the second sentence of this lemma its determinant is zero. The fourth sentence has two cases. If the echelon form matrix is singular then it has a zero row. Thus it has a zero on its diagonal and the product down its diagonal is zero. By the third sentence of this result the determinant is zero and therefore this matrix's determinant equals the product down its diagonal. If the echelon form matrix is nonsingular then none of its diagonal entries is zero. This means that we can divide by those entries and use condition (3) to get $ 1 $ 's on the diagonal. $ t_{1, 1} &t_{1, 2} & &t_{1, n}  0 &t_{2, 2} & &t_{2, n}  & &ddots  0 & & &t_{n, n} = t_{1, 1}cdot t_{2, 2}cdots t_{n, n}cdot 1 &t_{1, 2}/t_{1, 1} & &t_{1, n}/t_{1, 1}  0 &1 & &t_{2, n}/t_{2, 2}  & &ddots  0 & & &1 $ . Then the Jordan half of Gauss - Jordan elimination leaves the identity matrix. $ = t_{1, 1}cdot t_{2, 2}cdots t_{n, n}cdot 1 &0 & &0  0 &1 & &0  & &ddots  0 & & &1 = t_{1, 1}cdot t_{2, 2}cdots t_{n, n}cdot 1 $ . So in this case also, the determinant is the product down the diagonal. That gives us a way to compute the value of a determinant function on a matrix: do Gaussian reduction, keeping track of any changes of sign caused by row swaps and any scalars that we factor out, and finish by multiplying down the diagonal of the echelon form result. This algorithm is as fast as Gauss's Method and so is practical on all of the matrices that we will see. Doing $ nbyn{2} $ determinants with Gauss's Method $ [r] 2 &4  - 1 &3 = [r] 2 &4  0 &5 =10 $ . doesn't give a big time savings because the $ nbyn{2} $ determinant formula is easy. However, a $ nbyn{3} $ determinant is often easier to calculate with Gauss's Method than with its formula. $ [r] 2 &2 &6  4 &4 &3  0 & - 3 &5 = [r] 2 &2 &6  0 &0 & - 9  0 & - 3 &5 = - [r] 2 &2 &6  0 & - 3 &5  0 &0 & - 9 = - 54 $ . Determinants bigger than $ nbyn{3} $ go quickly with the Gauss's Method procedure. $ [r] 1 &0 &1 &3  0 &1 &1 &4  0 &0 &0 &5  0 &1 &0 &1 = [r] 1 &0 &1 &3  0 &1 &1 &4  0 &0 &0 &5  0 &0 & - 1 & - 3 = - [r] 1 &0 &1 &3  0 &1 &1 &4  0 &0 & - 1 & - 3  0 &0 &0 &5 = - ( - 5)=5 $ . That example raises an important point. This chapter's introduction gives formulas for $ nbyn{2} $ and $ nbyn{3} $ determinants, so we know that they exist, but not for determinant functions on matrices that are $ nbyn{4} $ or larger. Instead, $ X $ gives properties that a determinant function should have and leads to computing determinants by Gauss's Method. However, for any matrix we can reduce it to echelon form by Gauss's Method in multiple ways. For example, given a reduction we could change it by inserting a first step that multiplies the top row by  $ 2 $ and then a second step that multiplies it by  $ 1/2 $ . So we have to worry that two different Gauss's Method reductions could lead to two different computed values for the determinant. That is, we must verify that $ X $ gives a well - defined function. The next two subsections do this, showing that there exists a well - defined function satisfying the definition. But first we show that if there is such a function then there is no more than one. The example above illustrates the idea: we got  $ 5 $ by following the properties of the definition. So while we have not yet proved that $ det_{nbyn{4}} $ exists, that there is a function with properties (1), - - , (4), if such a function satisfying them does exist then we know what value it gives on the above matrix. For each $ n $ , if there is an $ nbyn{n} $ determinant function then it is unique. Suppose that there are two functions $ map{det_1, det_2}{matspace_{nbyn{n}}}{Re} $ satisfying the properties of $ X $ and its consequence $ X $ . Given a square matrix  $ M $ , fix some way of performing Gauss's Method to bring the matrix to echelon form (it does not matter that there are multiple ways, just fix one of them). By using this fixed reduction as in the above examplesDash keeping track of row - scaling factors and how the sign alternates on row swaps, and then multiplying down the diagonal of the echelon form resultDash we can compute the value that these two functions must return on  $ M $ , and they must return the same value. Since they give the same output on every input, they are the same function. The `if there is an $ nbyn{n} $ determinant function' emphasizes that, although we can use Gauss's Method to compute the only value that a determinant function could possibly return, we haven't yet shown that such a function exists for all $ n $ . The rest of this section does that. The prior subsection defines a function to be a determinant if it satisfies four conditions and shows that there is at most one $ nbyn{n} $ determinant function for each $ n $ . What is left is to show that for each $ n $ such a function exists. But, we easily compute determinants: we use Gauss's Method, keeping track of the sign changes from row swaps, and end by multiplying down the diagonal. How could they not exist? The difficulty is to show that the computation gives a well - definedDash that is, uniqueDash result. Consider these two Gauss's Method reductions of the same matrix, the first without any row swap $ [r] 1 &2  3 &4 grstep{ - 3rho_1+rho_2} [r] 1 &2  0 & - 2 $ . and the second with one. $ [r] 1 &2  3 &4 grstep{rho_1leftrightarrowrho_2} [r] 3 &4  1 &2 grstep{ - (1/3)rho_1+rho_2} [r] 3 &4  0 &2/3 $ . Both yield the determinant $ - 2 $ since in the second one we note that the row swap changes the sign of the result we get by multiplying down the diagonal. The fact that we are able to proceed in two ways opens the possibility that the two give different answers. That is, the way that we have given to compute determinant values does not plainly eliminate the possibility that there might be, say, two reductions of some $ nbyn{7} $  matrix that lead to different determinant values. In that case we would not have a function, since the definition of a function is that for each input there must be exactly associated one output. The rest of this section shows that the definition $ X $ never leads to a conflict. To do this we will define an alternative way to find the value of a determinant. (This alternative is less useful in practice because it is slow. But it is very useful for theory.) The key idea is that condition (3) of $ X $ shows that the determinant function is not linear. With condition (3) scalars come out of each row separately, $ [r] 4 &2  - 2 &6 =2cdot[r] 2 &1  - 2 &6 =4cdot[r] 2 &1  - 1 &3 $ . not from the entire matrix at once. So, where $ A=[r] 2 &1  - 1 &3 $ . then $ det(2A) neq 2cdotdet(A) $ (instead, $ det(2A) = 4cdotdet(A) $ ). Since scalars come out a row at a time we might guess that determinants are linear a row at a time. Let $ V $ be a vector space. A map $ map{f}{V^n}{Re} $ is multilinear if $ f(vec{rho}_1, dots, vec{v}+vec{w}, ldots, vec{rho}_n) =f(vec{rho}_1, dots, vec{v}, dots, vec{rho}_n) +f(vec{rho}_1, dots, vec{w}, dots, vec{rho}_n) $ $ f(vec{rho}_1, dots, kvec{v}, dots, vec{rho}_n) =kcdot f(vec{rho}_1, dots, vec{v}, dots, vec{rho}_n) $ for $ vec{v}, vec{w}in V $ and $ kinRe $ . Determinants are multilinear. Property (2) here is just $ X $ 's condition (3) so we need only verify property (1). There are two cases. If the set of other rows $ set{vec{rho}_1, dots, vec{rho}_{i - 1}, vec{rho}_{i+1}, dots, vec{rho}_n} $ is linearly dependent then all three matrices are singular and so all three determinants are zero and the equality is trivial. Therefore assume that the set of other rows is linearly independent. We can make a basis by adding one more vector $ sequence{vec{rho}_1, dots, vec{rho}_{i - 1}, vec{beta}, vec{rho}_{i+1}, dots, vec{rho}_n} $ . Express $ vec{v} $ and $ vec{w} $ with respect to this basis $ vec{v} &=v_1vec{rho}_1+dots+v_{i - 1}vec{rho}_{i - 1}+v_ivec{beta} +v_{i+1}vec{rho}_{i+1}+dots+v_nvec{rho}_n  vec{w} &= w_1vec{rho}_1+dots+w_{i - 1}vec{rho}_{i - 1}+w_ivec{beta} +w_{i+1}vec{rho}_{i+1}+dots+w_nvec{rho}_n $ . and add. $ vec{v}+vec{w} = (v_1+w_1)vec{rho}_1+dots+(v_i+w_i)vec{beta} +dots+(v_n+w_n)vec{rho}_n $ . Consider the left side of (1) and expand $ vec{v}+vec{w} $ . $ det (vec{rho}_1, dots, , (v_1+w_1)vec{rho}_1+dots+(v_i+w_i)vec{beta} +dots+(v_n+w_n)vec{rho}_n, , dots, vec{rho}_n) tag{ $ * $ } $ . By the definition of determinant's condition (1), the value of ( $ * $ ) the determinant is unchanged by the operation of adding $ - (v_1+w_1)vec{rho}_1 $ to the $ i $ - th row $ vec{v}+vec{w} $ . The $ i $ - th row becomes this. $ vec{v}+vec{w} - (v_1+w_1)vec{rho}_1 = (v_2+w_2)vec{rho}_2+cdots+(v_i+w_i)vec{beta} +dots+(v_n+w_n)vec{rho}_n $ . Next add $ - (v_2+w_2)vec{rho}_2 $ , etc., to eliminate all of the terms from the other rows. Apply condition (3) from the definition of determinant. det (vec{rho}_1, dots, vec{v}+vec{w}, dots, vec{rho}_n)  &=det (vec{rho}_1, dots, (v_i+w_i)cdotvec{beta}, dots, vec{rho}_n)  &=(v_i+w_i)cdotdet (vec{rho}_1, dots, vec{beta}, dots, vec{rho}_n)  &=v_icdot det (vec{rho}_1, dots, vec{beta}, dots, vec{rho}_n) +w_icdot det (vec{rho}_1, dots, vec{beta}, dots, vec{rho}_n) Now this is a sum of two determinants. To finish, bring $ v_i $ and $ w_i $ back inside in front of the $ vec{beta} $ 's and use row combinations again, this time to reconstruct the expressions of $ vec{v} $ and $ vec{w} $ in terms of the basis. That is, start with the operations of adding $ v_1vec{rho}_1 $ to $ v_ivec{beta} $ and $ w_1vec{rho}_1 $ to $ w_ivec{rho}_1 $ , etc., to get the expansions of $ vec{v} $ and $ vec{w} $ . Multilinearity allows us to expand a determinant into a sum of determinants, each of which involves a simple matrix. Use property (1) of multilinearity to break up the first row $ [r] 2 &1  4 &3 = [r] 2 &0  4 &3 + [r] 0 &1  4 &3 $ . and then use (1) again to break each along the second row. $ =[r] 2 &0  4 &0 + [r] 2 &0  0 &3 + [r] 0 &1  4 &0 + [r] 0 &1  0 &3 $ . The result is four determinants. In each row of each of the four there is a single entry from the original matrix. In the same way, a $ nbyn{3} $ determinant separates into a sum of many simpler determinants. Splitting along the first row produces three determinants (we have highlighted the zero in the $ 1, 3 $ position to set it off visually from the zeroes that appear as part of the splitting). $ [r] 2 &1 & - 1  4 &3 &highlight{0}  2 &1 &5 = [r] 2 &0 &0  4 &3 &highlight{0}  2 &1 &5 + [r] 0 &1 &0  4 &3 &highlight{0}  2 &1 &5 + [r] 0 &0 & - 1  4 &3 &highlight{0}  2 &1 &5 $ . In turn, each of the above splits in three along the second row. Then each of the nine splits in three along the third row. The result is twenty seven determinants, such that each row contains a single entry from the starting matrix. $ = [r] 2 &0 &0  4 &0 &0  2 &0 &0 + [r] 2 &0 &0  4 &0 &0  0 &1 &0 + [r] 2 &0 &0  4 &0 &0  0 &0 &5 + [r] 2 &0 &0  0 &3 &0  2 &0 &0 +dots+ [r] 0 &0 & - 1  0 &0 &highlight{0}  0 &0 &5 $ . So multilinearity will expand an $ nbyn{n} $ determinant into a sum of $ n^n $ - many determinants, where each row of each determinant contains a single entry from the starting matrix. In this expansion, although there are lots of terms, most of them have a determinant of zero. In each of these examples from the prior expansion, two of the entries from the original matrix are in the same column. $ [r] 2 &0 &0  4 &0 &0  0 &1 &0 qquad [r] 0 &0 & - 1  0 &3 &0  0 &0 &5 qquad [r] 0 &1 &0  0 &0 &highlight{0}  0 &0 &5 $ . For instance, in the first matrix the $ 2 $ and the $ 4 $ both come from the first column of the original matrix. In the second matrix the $ - 1 $ and  $ 5 $ both come from the third column. And in the third matrix the $ 0 $ and  $ 5 $ both come from the third column. Any such matrix is singular because one row is a multiple of the other. Thus any such determinant is zero, by $ X $ . With that observation the above expansion of the $ nbyn{3} $ determinant into the sum of the twenty seven determinants simplifies to the sum of these six where the entries from the original matrix come one per row, and also one per column. $ [r] 2 &1 & - 1  4 &3 &highlight{0}  2 &1 &5 &=[r] 2 &0 &0  0 &3 &0  0 &0 &5 + [r] 2 &0 &0  0 &0 &highlight{0}  0 &1 &0  &quadhbox{}+[r] 0 &1 &0  4 &0 &0  0 &0 &5 + [r] 0 &1 &0  0 &0 &highlight{0}  2 &0 &0  &quadhbox{}+[r] 0 &0 & - 1  4 &0 &0  0 &1 &0 + [r] 0 &0 & - 1  0 &3 &0  2 &0 &0 $ . In that expansion we can bring out the scalars. $ &=(2)(3)(5)[r] 1 &0 &0  0 &1 &0  0 &0 &1 +(2)(highlight{0})(1)[r] 1 &0 &0  0 &0 &1  0 &1 &0  &quadhbox{}+(1)(4)(5)[r] 0 &1 &0  1 &0 &0  0 &0 &1 +(1)(highlight{0})(2)[r] 0 &1 &0  0 &0 &1  1 &0 &0  &quadhbox{}+( - 1)(4)(1)[r] 0 &0 &1  1 &0 &0  0 &1 &0 +( - 1)(3)(2)[r] 0 &0 &1  0 &1 &0  1 &0 &0 $ . To finish, evaluate those six determinants by row - swapping them to the identity matrix, keeping track of the sign changes. $ &=30cdot (+1)+0cdot ( - 1)  &quadhbox{}+20cdot ( - 1)+0cdot (+1)  &quad hbox{} - 4cdot (+1) - 6cdot ( - 1)=12 $ . That example captures this subsection's new calculation scheme. Multilinearity expands a determinant into many separate determinants, each with one entry from the original matrix per row. Most of these have one row that is a multiple of another so we omit them. We are left with the determinants that have one entry per row and column from the original matrix. Factoring out the scalars further reduces the determinants that we must compute to the one - entry - per - row - and - column matrices where all entries are $ 1 $ 's. Recall Definition Three.IV., that a permutation matrix is square, with entries $ 0 $ 's except for a single $ 1 $ in each row and column. We now introduce a notation for permutation matrices. An $ n $ - permutation is a function on the first  $ n $ positive integers $ map{phi}{set{1, ldots, n}}{set{1, ldots, n}} $ that is one - to - one and onto. In a permutation each number $ 1 $ , ldots, $ n $ appears as output for one and only one input. We can denote a permutation as a sequence $ phi=sequence{phi(1), phi(2), ldots, phi(n)} $ . The $ 2 $ - permutations are the functions $ map{phi_1}{set{1, 2}}{set{1, 2}} $ given by $ phi_1(1)=1 $ , $ phi_1(2)=2 $ , and $ map{phi_2}{set{1, 2}}{set{1, 2}} $ given by $ phi_2(1)=2 $ , $ phi_2(2)=1 $ . The sequence notation is shorter: $ phi_1=sequence{1, 2} $ and $ phi_2=sequence{2, 1} $ . In the sequence notation the $ 3 $ - permutations are $ phi_1=sequence{1, 2, 3} $ , $ phi_2=sequence{1, 3, 2} $ , $ phi_3=sequence{2, 1, 3} $ , $ phi_4=sequence{2, 3, 1} $ , $ phi_5=sequence{3, 1, 2} $ , and $ phi_6=sequence{3, 2, 1} $ . We denote the row vector that is all $ 0 $ 's except for a $ 1 $ in entry $ j $ with $ iota_j $ so that the four - wide $ iota_2 $ is  $ rowvec{0 &1 &0 &0} $ . Now our notation for permutation matrices is: with any $ phi=sequence{phi(1), ldots, phi(n)} $ associate the matrix whose rows are $ iota_{phi(1)} $ , ldots, $ iota_{phi(n)} $ . For instance, associated with the $ 4 $ - permutation $ phi=sequence{3, 2, 1, 4} $ is the matrix whose rows are the corresponding $ iota $ 's. $ P_{phi}= iota_{3}  iota_{2}  iota_{1}  iota_{4} = [r] 0 &0 &1 &0  0 &1 &0 &0  1 &0 &0 &0  0 &0 &0 &1 $ . These are the permutation matrices for the $ 2 $ - permutations listed in $ X $ . $ P_{phi_1} = iota_1  iota_2 =[r] 1 &0  0 &1 qquad P_{phi_2} = iota_2  iota_1 =[r] 0 &1  1 &0 $ . For instance, $ P_{phi_2} $ 's first row is $ iota_{phi_2(1)}=iota_2 $ and its second is $ iota_{phi_2(2)}=iota_1 $ . Consider the $ 3 $ - permutation $ phi_5=sequence{3, 1, 2} $ . The permutation matrix $ P_{phi_5} $ has rows $ iota_{phi_5(1)}=iota_3 $ , $ iota_{phi_5(2)}=iota_1 $ , and $ iota_{phi_5(3)}=iota_2 $ . $ P_{phi_5} =[r] 0 &0 &1  1 &0 &0  0 &1 &0 $ . The permutation expansion for determinants is $ t_{1, 1} &t_{1, 2} &ldots &t_{1, n}  t_{2, 1} &t_{2, 2} &ldots &t_{2, n}  &vdots  t_{n, 1} &t_{n, 2} &ldots &t_{n, n} = [t]{l} t_{1, phi_1(1)}t_{2, phi_1(2)}cdots t_{n, phi_1(n)}deter{P_{phi_1}} >hbox{}+t_{1, phi_2(1)}t_{2, phi_2(2)}cdots t_{n, phi_2(n)}deter{P_{phi_2}}  >vdotswithin{+}  >hbox{}+t_{1, phi_k(1)}t_{2, phi_k(2)}cdots t_{n, phi_k(n)}deter{P_{phi_k}} $ . where $ phi_1, ldots, phi_k $ are all of the $ n $ - permutations. We can restate the formula in summation notation $ deter{T}=!! sum_{text{permutations }phi}!!!! t_{1, phi(1)}t_{2, phi(2)}cdots t_{n, phi(n)} deter{P_{phi}} $ . read aloud as, ``the sum, over all permutations $ phi $ , of terms having the form $ t_{1, phi(1)}t_{2, phi(2)}cdots t_{n, phi(n)} deter{P_{phi}} $ .'' The familiar $ nbyn{2} $ determinant formula follows from the above $ t_{1, 1} &t_{1, 2}  t_{2, 1} &t_{2, 2} &= t_{1, 1}t_{2, 2}cdotdeter{P_{phi_1}} + t_{1, 2}t_{2, 1}cdotdeter{P_{phi_2}} &= t_{1, 1}t_{2, 2}cdot[r] 1 &0  0 &1 + t_{1, 2}t_{2, 1}cdot[r] 0 &1  1 &0  &=t_{1, 1}t_{2, 2} - t_{1, 2}t_{2, 1} $ . as does the $ nbyn{3} $ formula. $ t_{1, 1} &t_{1, 2} &t_{1, 3}  t_{2, 1} &t_{2, 2} &t_{2, 3}  t_{3, 1} &t_{3, 2} &t_{3, 3} &= [t] &t_{1, 1}t_{2, 2}t_{3, 3}deter{P_{phi_1}} +t_{1, 1}t_{2, 3}t_{3, 2}deter{P_{phi_2}} +t_{1, 2}t_{2, 1}t_{3, 3}deter{P_{phi_3}}  &hspace*{0em} +t_{1, 2}t_{2, 3}t_{3, 1}deter{P_{phi_4}} +t_{1, 3}t_{2, 1}t_{3, 2}deter{P_{phi_5}} +t_{1, 3}t_{2, 2}t_{3, 1}deter{P_{phi_6}}  &= [t] &t_{1, 1}t_{2, 2}t_{3, 3} - t_{1, 1}t_{2, 3}t_{3, 2} - t_{1, 2}t_{2, 1}t_{3, 3}  &hspace*{0em} +t_{1, 2}t_{2, 3}t_{3, 1} +t_{1, 3}t_{2, 1}t_{3, 2} - t_{1, 3}t_{2, 2}t_{3, 1} $ . Computing a determinant with the permutation expansion typically takes longer than with Gauss's Method. However, we will use it to prove that the determinant function exists. The proof is long so we will just state the result here and defer the proof to the following subsection. For each $ n $ there is an $ nbyn{n} $ determinant function. Also in the next subsection is the proof of the next result (they are together because the two proofs overlap). The determinant of a matrix equals the determinant of its transpose. Because of this theorem, while we have so far stated determinant results in terms of rows, all of the results also hold in terms of columns. A matrix with two equal columns is singular. Column swaps change the sign of a determinant. Determinants are multilinear in their columns. For the first statement, transposing the matrix results in a matrix with the same determinant, and with two equal rows, and hence a determinant of zero. Prove the other two in the same way. We finish this subsection with a summary: determinant functions exist, are unique, and we know how to compute them. As for what determinants are about, perhaps these lines help make it memorable. small Determinant none, *  Solution: lots or none. * Determinant some, *  Solution: just one. textit{This subsection contains proofs of two results from the prior subsection. It is optional. We will use the material developed here only in the Jordan Canonical Form subsection, which is also optional.} We wish to show that for any size  $ n $ , the determinant function on $ nbyn{n} $ matrices is well - defined. The prior subsection develops the permutation expansion formula. $ t_{1, 1} &t_{1, 2} &ldots &t_{1, n}  t_{2, 1} &t_{2, 2} &ldots &t_{2, n}  &vdots  t_{n, 1} &t_{n, 2} &ldots &t_{n, n} &= [t]{l} t_{1, phi_1(1)}t_{2, phi_1(2)}cdots t_{n, phi_1(n)}deter{P_{phi_1}} >hbox{}+t_{1, phi_2(1)}t_{2, phi_2(2)}cdots t_{n, phi_2(n)}deter{P_{phi_2}} >alignedvdots  >hbox{}+t_{1, phi_k(1)}t_{2, phi_k(2)}cdots t_{n, phi_k(n)}deter{P_{phi_k}} &=!!sum_{text{permutations }phi}!!!! t_{1, phi(1)}t_{2, phi(2)}cdots t_{n, phi(n)} deter{P_{phi}} $ . This reduces the problem of showing that the determinant is well - defined to only showing that the determinant is well - defined on the set of permutation matrices. A permutation matrix can be row - swapped to the identity matrix. So one way that we can calculate its determinant is by keeping track of the number of swaps. However, we still must show that the result is well - defined. Recall what the difficulty is: the determinant of $ P_{phi}= [r] 0 &1 &0 &0  1 &0 &0 &0  0 &0 &1 &0  0 &0 &0 &1 $ . could be computed with one swap $ P_{phi} grstep{rho_1leftrightarrowrho_2} [r] 1 &0 &0 &0  0 &1 &0 &0  0 &0 &1 &0  0 &0 &0 &1 $ . or with three. $ P_{phi} grstep{rho_3leftrightarrowrho_1} repeatedgrstep{rho_2leftrightarrowrho_3} repeatedgrstep{rho_1leftrightarrowrho_3} [r] 1 &0 &0 &0  0 &1 &0 &0  0 &0 &1 &0  0 &0 &0 &1 $ . Both reductions have an odd number of swaps so in this case we figure that $ deter{P_{phi}}= - 1 $ but if there were some way to do it with an even number of swaps then we would have the determinant giving two different outputs from a single input. Below, $ X $ proves that this cannot happenDash there is no permutation matrix that can be row - swapped to an identity matrix in two ways, one with an even number of swaps and the other with an odd number of swaps. In a permutation $ phi=sequence{ldots, k, ldots, j, ldots} $ , elements such that $ k>j $ are in an inversion of their natural order. Similarly, in a permutation matrix two rows $ P_{phi}= vdots  iota_{k}  vdots  iota_{j}  vdots $ . such that $ k>j $ are in an inversion. This permutation matrix $ [r] 1 &0 &0 &0  0 &0 &1 &0  0 &1 &0 &0  0 &0 &0 &1 = iota_1  iota_3  iota_2  iota_4 $ . has a single inversion, that $ iota_3 $ precedes $ iota_2 $ . There are three inversions here: $ [r] 0 &0 &1  0 &1 &0  1 &0 &0 = iota_3  iota_2  iota_1 $ . $ iota_3 $ precedes $ iota_1 $ , $ iota_3 $ precedes $ iota_2 $ , and $ iota_2 $ precedes $ iota_1 $ . A row - swap in a permutation matrix changes the number of inversions from even to odd, or from odd to even. Consider a swap of rows $ j $ and $ k $ , where $ k>j $ . If the two rows are adjacent $ P_{phi}= vdots  iota_{phi(j)}  iota_{phi(k)}  vdots grstep{rho_kleftrightarrowrho_j} vdots  iota_{phi(k)}  iota_{phi(j)}  vdots $ . then since inversions involving rows not in this pair are not affected, the swap changes the total number of inversions by one, either removing or producing one inversion depending on whether $ phi(j)>phi(k) $ or not. Consequently, the total number of inversions changes from odd to even or from even to odd. If the rows are not adjacent then we can swap them via a sequence of adjacent swaps, first bringing row  $ k $ up $ vdots  iota_{phi(j)}  iota_{phi(j+1)}  iota_{phi(j+2)}  vdots  iota_{phi(k)}  vdots grstep{rho_kswaprho_{k - 1}} repeatedgrstep{rho_{k - 1}swaprho_{k - 2}} cdots grstep{rho_{j+1}swaprho_j} vdots  iota_{phi(k)}  iota_{phi(j)}  iota_{phi(j+1)}  vdots  iota_{phi(k - 1)}  vdots $ . and then bringing row  $ j $ down. $ grstep{rho_{j+1}swaprho_{j+2}} repeatedgrstep{rho_{j+2}swaprho_{j+3}} cdots grstep{rho_{k - 1}swaprho_k} vdots  iota_{phi(k)}  iota_{phi(j+1)}  iota_{phi(j+2)}  vdots  iota_{phi(j)}  vdots $ . Each of these adjacent swaps changes the number of inversions from odd to even or from even to odd. The total number of swaps $ (k - j)+(k - j - 1) $ is odd. Thus, in aggregate, the number of inversions changes from even to odd, or from odd to even. If a permutation matrix has an odd number of inversions then swapping it to the identity takes an odd number of swaps. If it has an even number of inversions then swapping to the identity takes an even number. The identity matrix has zero inversions. To change an odd number to zero requires an odd number of swaps, and to change an even number to zero requires an even number of swaps. The matrix in $ X $ can be brought to the identity with one swap $ rho_1leftrightarrowrho_3 $ . (So the number of swaps needn't be the same as the number of inversions, but the oddness or evenness of the two numbers is the same.) The signum } of a permutation $ sgn(phi) $ is $ - 1 $ if the number of inversions in $ phi $ is odd and is $ +1 $ if the number of inversions is even. Using the notation for the $ 3 $ - permutations from $ X $ we have $ P_{phi_1}= 1 &0 &0  0 &1 &0  0 &0 &1 qquad P_{phi_2}= 1 &0 &0  0 &0 &1  0 &1 &0 $ . so $ sgn(phi_1)=1 $ because there are no inversions, while $ sgn(phi_2)= - 1 $ because there is one. We still have not shown that the determinant function is well - defined because we have not considered row operations on permutation matrices other than row swaps. We will finesse this issue. Define a function $ map{d}{matspace_{nbyn{n}}}{Re} $ by altering the permutation expansion formula, replacing $ deter{P_phi} $ with $ sgn(phi) $ . $ d(T)= sum_{text{permutations }phi}t_{1, phi(1)}t_{2, phi(2)}cdots t_{n, phi(n)} cdotsgn(phi) $ . The advantage of this formula is that the number of inversions is clearly well - definedDash just count them. Therefore, we will be finished showing that an  $ nbyn{n} $ determinant function exists when we show that  $ d $ satisfies the conditions in the definition of a determinant. The function $ d $ above is a determinant. Hence determinant functions $ det_{nbyn{n}} $ exist for every $ n $ . We must check that it satisfies the four conditions from the definition of determinant, $ X $ . Condition (4) is easy: where $ I $ is the $ nbyn{n} $ identity, in $ d(I)= sum_{text{perm }phi} iota_{1, phi(1)}iota_{2, phi(2)}cdots iota_{n, phi(n)} sgn(phi) $ . all of the terms in the summation are zero except for the one where the permutation  $ phi $ is the identity, which gives the product down the diagonal, which is one. For condition (3) suppose that $ Tsmash[b]{grstep{krho_i}}hat{T} $ and consider $ d(hat{T}) $ . sum_{text{perm }phi}!! hat{t}_{1, phi(1)} cdotshat{t}_{i, phi(i)}cdotshat{t}_{n, phi(n)} sgn(phi)  =sum_{phi} t_{1, phi(1)}cdots kt_{i, phi(i)}cdots t_{n, phi(n)} sgn(phi) Factor out  $ k $ to get the desired equality. $ =kcdotsum_{phi} t_{1, phi(1)}cdots t_{i, phi(i)}cdots t_{n, phi(n)} sgn(phi) =kcdot d(T) $ . For (2) suppose that $ Tsmash[b]{grstep{rho_iswaprho_j}}hat{T} $ . We must show that $ d(hat{T}) $ is the negative of $ d(T) $ . $ d(hat{T})= sum_{text{perm }phi}!! hat{t}_{1, phi(1)} cdotshat{t}_{i, phi(i)} cdotshat{t}_{j, phi(j)} cdots hat{t}_{n, phi(n)} sgn(phi) tag{ $ * $ } $ . We will show that each term in ( $ * $ ) is associated with a term in $ d(T) $ , and that the two terms are negatives of each other. Consider the matrix from the multilinear expansion of $ d(hat{T}) $ giving the term $ hat{t}_{1, phi(1)} cdotshat{t}_{i, phi(i)} cdotshat{t}_{j, phi(j)} cdots hat{t}_{n, phi(n)} sgn(phi) $ . $  & &vdots & &  &hat{t}_{i, phi(i)} & & &  & &vdots & &  & & &hat{t}_{j, phi(j)} &  & &vdots & $ . It is the result of the $ rho_iswaprho_j $ operation performed on this matrix. $  & &vdots & &  & & &t_{i, phi(j)} &  & &vdots & &  &t_{j, phi(i)} & & &  & &vdots & & $ . That is, the term with hatted $ t $ 's is associated with this term from the $ d(T) $ expansion: $ t_{1, sigma(1)} cdots t_{j, sigma(j)} cdots t_{i, sigma(i)} cdots t_{n, sigma(n)} sgn(sigma) $ , where the permutation $ sigma $ equals $ phi $ but with the $ i $ - th and $ j $ - th numbers interchanged, $ sigma(i)=phi(j) $ and $ sigma(j)=phi(i) $ . The two terms have the same multiplicands $ hat{t}_{1, phi(1)}=t_{1, sigma(1)} $ , ldots, including the entries from the swapped rows $ hat{t}_{i, phi(i)}=t_{j, phi(i)}=t_{j, sigma(j)} $ and $ hat{t}_{j, phi(j)}=t_{i, phi(j)}=t_{i, sigma(i)} $ . But the two terms are negatives of each other since $ sgn(phi)= - sgn(sigma) $ by $ X $ . Now, any permutation $ phi $ can be derived from some other permutation $ sigma $ by such a swap, in one and only one way. Therefore the summation in ( $ * $ ) is in fact a sum over all permutations, taken once and only once. $ d(hat{T}) &= sum_{text{perm }phi}!! hat{t}_{1, phi(1)} cdotshat{t}_{i, phi(i)} cdotshat{t}_{j, phi(j)} cdots hat{t}_{n, phi(n)} sgn(phi)  &=sum_{text{perm }sigma}!! t_{1, sigma(1)} cdots t_{j, sigma(j)} cdots t_{i, sigma(i)} cdots t_{n, sigma(n)} cdotbigl( - sgn(sigma)bigr) $ . Thus $ d(hat{T})= - d(T) $ . Finally, for condition (1) suppose that $ Tsmash[b]{grstep{krho_i+rho_j}}hat{T} $ . $ d(hat{T}) &=sum_{text{perm }phi} hat{t}_{1, phi(1)}cdotshat{t}_{i, phi(i)} cdotshat{t}_{j, phi(j)}cdotshat{t}_{n, phi(n)} sgn(phi)  &=sum_{phi} t_{1, phi(1)}cdots t_{i, phi(i)} cdots (kt_{i, phi(j)}+t_{j, phi(j)})cdots t_{n, phi(n)} sgn(phi) $ . Distribute over the addition in $ kt_{i, phi(j)}+t_{j, phi(j)} $ . $ = [t] &smash[b]{sum_{smash{phi}}}big[t_{1, phi(1)}cdots t_{i, phi(i)} cdots kt_{i, phi(j)}cdots t_{n, phi(n)} sgn(phi)  [ - 1ex] &hbox{}qquadquadhbox{} +t_{1, phi(1)}cdots t_{i, phi(i)} cdots t_{j, phi(j)}cdots t_{n, phi(n)} sgn(phi)big] $ . Break it into two summations. $ =[t] &smash[b]{sum_{smash{phi}}} :t_{1, phi(1)}cdots t_{i, phi(i)} cdots kt_{i, phi(j)}cdots t_{n, phi(n)} sgn(phi)  [ - 1ex] &hbox{}qquadquadhbox{} +smash[b]{sum_{phi}}: t_{1, phi(1)}cdots t_{i, phi(i)} cdots t_{j, phi(j)}cdots t_{n, phi(n)} sgn(phi) $ . Recognize the second one. $ =[t] &kcdot smash[b]{sum_{smash{phi}}}: t_{1, phi(1)}cdots t_{i, phi(i)} cdots t_{i, phi(j)}cdots t_{n, phi(n)} sgn(phi)  [ - .75ex] &hbox{}qquadquadhbox{} +d(T) $ . Consider the terms $ t_{1, phi(1)}cdots t_{i, phi(i)} cdots t_{i, phi(j)}cdots t_{n, phi(n)} sgn(phi) $ . Notice the subscripts; the entry is $ t_{i, phi(j)} $ , not $ t_{j, phi(j)} $ . The sum of these terms is the determinant of a matrix  $ S $ that is equal to $ T $ except that row  $ j $ of $ S $ is a copy of row  $ i $ of $ T $ , that is, $ S $ has two equal rows. In the same way that we proved $ X $ we can see that $ d(S)=0 $ : a swap of $ S $ 's equal rows will change the sign of $ d(S) $ but since the matrix is unchanged by that swap the value of $ d(S) $ must also be unchanged, and so that value must be zero. We have now proved that determinant functions exist for each size  $ nbyn{n} $ . We already know that for each size there is at most one determinant. Therefore, for each size there is one and only one determinant function. We end this subsection by proving the other result remaining from the prior subsection. The determinant of a matrix equals the determinant of its transpose. The proof is best understood by doing the general $ nbyn{3} $ case. That the argument applies to the $ nbyn{n} $ case will be clear. Compare the permutation expansion of the matrix  $ T $ $ t_{1, 1} &t_{1, 2} &t_{1, 3}  t_{2, 1} &t_{2, 2} &t_{2, 3}  t_{3, 1} &t_{3, 2} &t_{3, 3} &= t_{1, 1}t_{2, 2}t_{3, 3} 1 &0 &0  0 &1 &0  0 &0 &1 +t_{1, 1}t_{2, 3}t_{3, 2} 1 &0 &0  0 &0 &1  0 &1 &0  &quad+ t_{1, 2}t_{2, 1}t_{3, 3} 0 &1 &0  1 &0 &0  0 &0 &1 +t_{1, 2}t_{2, 3}t_{3, 1} 0 &1 &0  0 &0 &1  1 &0 &0  &quad+ t_{1, 3}t_{2, 1}t_{3, 2} 0 &0 &1  1 &0 &0  0 &1 &0 +t_{1, 3}t_{2, 2}t_{3, 1} 0 &0 &1  0 &1 &0  1 &0 &0 $ . with the permutation expansion of its transpose. $ t_{1, 1} &t_{2, 1} &t_{3, 1}  t_{1, 2} &t_{2, 2} &t_{3, 2}  t_{1, 3} &t_{2, 3} &t_{3, 3} &= t_{1, 1}t_{2, 2}t_{3, 3} 1 &0 &0  0 &1 &0  0 &0 &1 +t_{1, 1}t_{3, 2}t_{2, 3} 1 &0 &0  0 &0 &1  0 &1 &0  &quad+ t_{2, 1}t_{1, 2}t_{3, 3} 0 &1 &0  1 &0 &0  0 &0 &1 +t_{2, 1}t_{3, 2}t_{1, 3} 0 &1 &0  0 &0 &1  1 &0 &0  &quad+ t_{3, 1}t_{1, 2}t_{2, 3} 0 &0 &1  1 &0 &0  0 &1 &0 +t_{3, 1}t_{2, 2}t_{1, 3} 0 &0 &1  0 &1 &0  1 &0 &0 $ . Compare first the six products of $ t $ 's. The ones in the expansion of  $ T $ are the same as the ones in the expansion of the transpose; for instance, $ t_{1, 2}t_{2, 3}t_{3, 1} $ is in the top and $ t_{3, 1}t_{1, 2}t_{2, 3} $ is in the bottom. That's perfectly sensibleDash the six in the top arise from all of the ways of picking one entry of  $ T $ from each row and column while the six in the bottom are all of the ways of picking one entry of  $ T $ from each column and row, so of course they are the same set. Next observe that in the two expansions, each $ t $ - product expression is not necessarily associated with the same permutation matrix. For instance, on the top $ t_{1, 2}t_{2, 3}t_{3, 1} $ is associated with the matrix for the map $ 1mapsto 2 $ , $ 2mapsto 3 $ , $ 3mapsto 1 $ . On the bottom $ t_{3, 1}t_{1, 2}t_{2, 3} $ is associated with the matrix for the map $ 1mapsto 3 $ , $ 2mapsto 1 $ , $ 3mapsto 2 $ . The second map is inverse to the first. This is also perfectly sensibleDash both the matrix transpose and the map inverse flip the $ 1, 2 $ to $ 2, 1 $ , flip the $ 2, 3 $ to $ 3, 2 $ , and flip $ 3, 1 $ to $ 1, 3 $ . We finish by noting that the determinant of $ P_{phi} $ equals the determinant of $ P_{phi^{ - 1}} $ , as $ X $ shows.
